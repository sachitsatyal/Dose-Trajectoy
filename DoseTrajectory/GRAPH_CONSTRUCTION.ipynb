{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74031df8",
   "metadata": {},
   "source": [
    "## Step 1: Load Required Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f28fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the dosage to load (modify for other dosages)\n",
    "dosage = \"T10\"\n",
    "base_dir = \"\"  # Ensure the base directory is correct\n",
    "deg_results_dir = os.path.join(base_dir, \"DEG_Results\")\n",
    "coexpression_dir = os.path.join(base_dir, \"CoExpression_Results\")\n",
    "final_enrichment_dir = os.path.join(deg_results_dir, \"Final_Enrichment\")\n",
    "\n",
    "# File paths - Ensure correct filenames based on the desired structure\n",
    "gene_file = os.path.join(deg_results_dir, f\"DEG_{dosage}\", f\"DEG_{dosage}_vs_Control.csv\")  # Corrected to dosage vs control\n",
    "pathway_file = os.path.join(final_enrichment_dir, f\"Polygenic_Enrichment_DEG_{dosage}.csv\")  # Corrected to dosage vs control enrichment\n",
    "coexpression_file = os.path.join(coexpression_dir, f\"CoExpression_{dosage}.csv\")\n",
    "\n",
    "# Load gene differential expression data (Dosage vs Control)\n",
    "if os.path.exists(gene_file):\n",
    "    df_gene = pd.read_csv(gene_file)\n",
    "    print(f\"‚úÖ Loaded {df_gene.shape[0]} genes from {gene_file}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Gene file not found: {gene_file}\")\n",
    "    df_gene = None  # Handle missing data case\n",
    "\n",
    "# Load pathway enrichment data (Dosage vs Control)\n",
    "if os.path.exists(pathway_file):\n",
    "    df_pathway = pd.read_csv(pathway_file)\n",
    "    print(f\"‚úÖ Loaded {df_pathway.shape[0]} pathways from {pathway_file}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Pathway enrichment file not found: {pathway_file}\")\n",
    "    df_pathway = None\n",
    "\n",
    "# Load gene-gene coexpression data\n",
    "if os.path.exists(coexpression_file):\n",
    "    df_coexpression = pd.read_csv(coexpression_file)\n",
    "    print(f\"‚úÖ Loaded {df_coexpression.shape[0]} co-expression pairs from {coexpression_file}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Co-expression file not found: {coexpression_file}\")\n",
    "    df_coexpression = None\n",
    "\n",
    "# Display sample data for verification\n",
    "if df_gene is not None:\n",
    "    print(\"\\nüîπ Sample Gene Data:\")\n",
    "    print(df_gene.head())\n",
    "\n",
    "if df_pathway is not None:\n",
    "    print(\"\\nüîπ Sample Pathway Data:\")\n",
    "    print(df_pathway.head())\n",
    "\n",
    "if df_coexpression is not None:\n",
    "    print(\"\\nüîπ Sample Coexpression Data:\")\n",
    "    print(df_coexpression.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f22175d",
   "metadata": {},
   "source": [
    "## Step 2: Generate Node Features for Genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b4c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define directories\n",
    "deg_dir = \"DEG_Results\"\n",
    "coexp_dir = \"CoExpression_Results\"\n",
    "output_dir = \"Graph_Results/Gene_Features\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load globally relevant genes\n",
    "global_gene_file = os.path.join(deg_dir, \"Filtered_State_Global_DEGs.csv\")\n",
    "df_global_genes = pd.read_csv(global_gene_file)\n",
    "global_genes = set(df_global_genes[\"Gene\"].str.upper())  # Ensure case consistency\n",
    "\n",
    "# Get all dosage levels available\n",
    "dosages = [d for d in os.listdir(deg_dir) if d.startswith(\"DEG_\")]\n",
    "\n",
    "# Choose a dosage to print sample results for verification\n",
    "verification_dosage = \"DEG_T10\"\n",
    "\n",
    "print(\"\\nüîπ **Starting Node Feature Generation with Overlap Calculation** üîπ\\n\")\n",
    "\n",
    "for dosage in dosages:\n",
    "    dosage_level = dosage.split(\"_\")[-1]  # Extract dosage (e.g., T10, T20)\n",
    "    \n",
    "    # Define correct file paths (Ensuring control vs. dosage format)\n",
    "    deg_file = os.path.join(deg_dir, dosage, f\"DEG_{dosage_level}_vs_Control.csv\")\n",
    "    coexpression_file = os.path.join(coexp_dir, f\"CoExpression_{dosage_level}.csv\")\n",
    "    pathway_file = os.path.join(deg_dir, \"Final_Enrichment\", f\"Polygenic_Enrichment_DEG_{dosage_level}.csv\")\n",
    "\n",
    "    # Ensure DEG file exists before proceeding\n",
    "    if not os.path.exists(deg_file):\n",
    "        print(f\"‚ö†Ô∏è Skipping {dosage_level} - Missing DEG file!\")\n",
    "        continue\n",
    "\n",
    "    # Load DEG data (Control vs Dosage)\n",
    "    df_gene = pd.read_csv(deg_file)\n",
    "    df_gene[\"Gene\"] = df_gene[\"Gene\"].str.upper()  # Ensure case consistency\n",
    "\n",
    "    # Compute Degree in Pathways (number of pathways a gene appears in)\n",
    "    if os.path.exists(pathway_file):\n",
    "        df_pathway = pd.read_csv(pathway_file)\n",
    "        gene_pathway_counts = df_pathway[\"Genes\"].str.split(\";\").explode().str.upper().value_counts()\n",
    "        df_gene[\"Degree in Pathways\"] = df_gene[\"Gene\"].map(gene_pathway_counts).fillna(0).astype(int)\n",
    "    else:\n",
    "        df_gene[\"Degree in Pathways\"] = 0\n",
    "\n",
    "    # Load Co-expression Data\n",
    "    if os.path.exists(coexpression_file):\n",
    "        df_coexpression = pd.read_csv(coexpression_file)\n",
    "        coexpressed_genes = set(df_coexpression[\"Gene1\"].str.upper()).union(set(df_coexpression[\"Gene2\"].str.upper()))\n",
    "    else:\n",
    "        coexpressed_genes = set()\n",
    "\n",
    "    # **Compute Overlap Before Filtering**\n",
    "    original_deg_genes = set(df_gene[\"Gene\"])\n",
    "    retained_genes = df_gene[\n",
    "        (df_gene[\"Degree in Pathways\"] > 0) | \n",
    "        (df_gene[\"Gene\"].isin(global_genes)) | \n",
    "        (df_gene[\"Gene\"].isin(coexpressed_genes))\n",
    "    ][\"Gene\"].unique()\n",
    "    \n",
    "    # Compute how many genes remain\n",
    "    overlap_count = len(set(retained_genes) & original_deg_genes)\n",
    "    \n",
    "    # Print overlap statistics\n",
    "    print(f\"üìå Dosage: {dosage_level} ‚Üí Original DEG Genes: {len(original_deg_genes)}, Retained Genes: {len(retained_genes)}, Overlapping: {overlap_count}\")\n",
    "\n",
    "    # Apply filtering\n",
    "    df_gene = df_gene[df_gene[\"Gene\"].isin(retained_genes)]\n",
    "\n",
    "    # Save the final filtered gene features for this dosage\n",
    "    output_file = os.path.join(output_dir, f\"Filtered_Gene_Features_{dosage_level}.csv\")\n",
    "    df_gene.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"‚úÖ Processed {dosage_level}: Saved filtered gene features!\")\n",
    "\n",
    "    # Print sample results for verification\n",
    "    if dosage == verification_dosage:\n",
    "        print(\"\\nüîç Sample Verification for\", dosage_level)\n",
    "        print(df_gene.head())\n",
    "\n",
    "print(\"\\nüéØ **All dosages processed! Filtered gene node features saved in Graph_Results/**\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcae7a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define directory where filtered gene features are stored\n",
    "graph_results_dir = \"Graph_Results/Gene_Features\"\n",
    "\n",
    "# Get all processed dosage files\n",
    "gene_feature_files = [f for f in os.listdir(graph_results_dir) if f.startswith(\"Filtered_Gene_Features_\")]\n",
    "\n",
    "# Iterate through each file and compute the number of genes with Degree 0\n",
    "for file in gene_feature_files:\n",
    "    dosage_level = file.split(\"_\")[-1].replace(\".csv\", \"\")  # Extract dosage level\n",
    "    file_path = os.path.join(graph_results_dir, file)\n",
    "    \n",
    "    # Load gene feature data\n",
    "    df_gene = pd.read_csv(file_path)\n",
    "    \n",
    "    # Ensure 'Degree in Pathways' column is treated as numeric\n",
    "    df_gene[\"Degree in Pathways\"] = pd.to_numeric(df_gene[\"Degree in Pathways\"], errors='coerce')\n",
    "\n",
    "    # Count total genes and genes with Degree 0\n",
    "    total_genes = df_gene.shape[0]\n",
    "    genes_with_degree_0 = df_gene[df_gene[\"Degree in Pathways\"] == 0].shape[0]\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"üìå Dosage: {dosage_level} ‚Üí Total Genes: {total_genes}, Genes with Degree 0: {genes_with_degree_0}\")\n",
    "\n",
    "    # Display sample genes with Degree 0 (for verification)\n",
    "    sample_genes = df_gene[df_gene[\"Degree in Pathways\"] == 0].head(5)\n",
    "    if not sample_genes.empty:\n",
    "        print(f\"üîç Sample genes with Degree 0 in {dosage_level}:\")\n",
    "        print(sample_genes[[\"Gene\", \"Degree in Pathways\"]])\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "print(\"‚úÖ Completed Degree 0 analysis for all dosages!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9932cd0",
   "metadata": {},
   "source": [
    "## Step 3: Generate Node Features for pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e3c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define directories\n",
    "deg_dir = \"DEG_Results/Final_Enrichment\"\n",
    "output_dir = \"Graph_Results/Pathway_Features\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Identify **only dosage** pathway files (excluding merged, state, and vs comparisons)\n",
    "pathway_files = [\n",
    "    f for f in os.listdir(deg_dir) \n",
    "    if f.startswith(\"Polygenic_Enrichment_DEG_\") and f.endswith(\".csv\") \n",
    "    and \"merged\" not in f and \"State\" not in f and \"vs\" not in f\n",
    "]\n",
    "\n",
    "# Process each dosage\n",
    "for file in pathway_files:\n",
    "    dosage = file.replace(\"Polygenic_Enrichment_DEG_\", \"\").replace(\".csv\", \"\")  # Extract dosage (e.g., T10, T20)\n",
    "\n",
    "    # Define pathway file\n",
    "    pathway_file = os.path.join(deg_dir, file)\n",
    "    \n",
    "    # Ensure file exists\n",
    "    if not os.path.exists(pathway_file):\n",
    "        print(f\"‚ö†Ô∏è Skipping {dosage} - Missing pathway file!\")\n",
    "        continue\n",
    "\n",
    "    # Load pathway data\n",
    "    df_pathway = pd.read_csv(pathway_file)\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    if not {\"Term\", \"Combined Score\", \"Genes\"}.issubset(df_pathway.columns):\n",
    "        print(f\"‚ö†Ô∏è Skipping {dosage} - Missing required columns!\")\n",
    "        continue\n",
    "\n",
    "    # Compute Pathway Size (Number of genes per pathway)\n",
    "    df_pathway[\"Pathway Size\"] = df_pathway[\"Genes\"].apply(lambda x: len(str(x).split(\";\")))\n",
    "\n",
    "    # Select required columns\n",
    "    df_pathway = df_pathway[[\"Term\", \"Combined Score\", \"Pathway Size\"]]\n",
    "\n",
    "    # Save the pathway features for this dosage\n",
    "    output_file = os.path.join(output_dir, f\"Pathway_Features_{dosage}.csv\")\n",
    "    df_pathway.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"‚úÖ Processed {dosage}: Saved pathway features!\")\n",
    "\n",
    "print(\"üéØ All dosages processed! Pathway features saved in `Graph_Results/Pathway_Features/`.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17401f9f",
   "metadata": {},
   "source": [
    "## Step 4: Generate Gene-Pathway and Gene-Gene edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaf56f2",
   "metadata": {},
   "source": [
    "### Gene Pathway Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f222f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define directories\n",
    "deg_dir = \"DEG_Results/Final_Enrichment\"\n",
    "gene_feature_dir = \"Graph_Results/Gene_Features\"\n",
    "output_dir = \"Graph_Results/Gene_Pathway_Edge\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Identify **only dosage** pathway files (excluding merged, state, and vs comparisons)\n",
    "pathway_files = [\n",
    "    f for f in os.listdir(deg_dir) \n",
    "    if f.startswith(\"Polygenic_Enrichment_DEG_\") and f.endswith(\".csv\") \n",
    "    and \"merged\" not in f and \"State\" not in f and \"vs\" not in f\n",
    "]\n",
    "\n",
    "# Process each dosage\n",
    "for file in pathway_files:\n",
    "    dosage = file.replace(\"Polygenic_Enrichment_DEG_\", \"\").replace(\".csv\", \"\")  # Extract dosage (e.g., T10, T20)\n",
    "\n",
    "    # Define pathway file\n",
    "    pathway_file = os.path.join(deg_dir, file)\n",
    "    gene_feature_file = os.path.join(gene_feature_dir, f\"Filtered_Gene_Features_{dosage}.csv\")\n",
    "\n",
    "    # Ensure files exist\n",
    "    if not os.path.exists(pathway_file) or not os.path.exists(gene_feature_file):\n",
    "        print(f\"‚ö†Ô∏è Skipping {dosage} - Missing pathway or gene feature file!\")\n",
    "        continue\n",
    "\n",
    "    # Load pathway data\n",
    "    df_pathway = pd.read_csv(pathway_file)\n",
    "    df_genes = pd.read_csv(gene_feature_file)\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    if not {\"Term\", \"Combined Score\", \"Genes\"}.issubset(df_pathway.columns):\n",
    "        print(f\"‚ö†Ô∏è Skipping {dosage} - Missing required columns in pathway file!\")\n",
    "        continue\n",
    "\n",
    "    # Convert gene names to uppercase for consistency\n",
    "    df_genes[\"Gene\"] = df_genes[\"Gene\"].str.upper()\n",
    "    valid_genes = set(df_genes[\"Gene\"])\n",
    "\n",
    "    # Expand gene-pathway relationships\n",
    "    gene_pathway_edges = []\n",
    "    \n",
    "    for _, row in df_pathway.iterrows():\n",
    "        pathway = row[\"Term\"]\n",
    "        combined_score = row[\"Combined Score\"]\n",
    "        \n",
    "        # Ensure genes are properly split and filtered\n",
    "        genes = str(row[\"Genes\"]).split(\";\")\n",
    "        genes = [gene.strip().upper() for gene in genes if gene.strip()]  # Clean and format genes\n",
    "        \n",
    "        # Remove genes that are NOT in `Gene_Features`\n",
    "        filtered_genes = [gene for gene in genes if gene in valid_genes]\n",
    "\n",
    "        # Skip pathways with no valid gene connections\n",
    "        if len(filtered_genes) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Compute edge weight\n",
    "        edge_weight = combined_score / len(filtered_genes) if len(filtered_genes) > 0 else 0\n",
    "\n",
    "        # Store edges\n",
    "        for gene in filtered_genes:\n",
    "            gene_pathway_edges.append([gene, pathway, edge_weight])\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_edges = pd.DataFrame(gene_pathway_edges, columns=[\"Gene\", \"Pathway\", \"Edge Weight\"])\n",
    "\n",
    "    # Save edge file if edges exist\n",
    "    if not df_edges.empty:\n",
    "        edge_file = os.path.join(output_dir, f\"Edges_Gene_Pathway_{dosage}.csv\")\n",
    "        df_edges.to_csv(edge_file, index=False)\n",
    "        print(f\"‚úÖ Processed {dosage}: Saved Gene ‚Üî Pathway edges!\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Skipping {dosage} - No valid gene-pathway edges!\")\n",
    "\n",
    "print(\"üéØ All dosages processed! Gene ‚Üî Pathway edge files saved in Graph_Results/Gene_Pathway_Edge/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f733448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define directories\n",
    "gene_feature_dir = \"Graph_Results/Gene_Features\"\n",
    "gene_pathway_edge_dir = \"Graph_Results/Gene_Pathway_Edge\"\n",
    "\n",
    "# Get all dosage levels from gene feature files\n",
    "dosages = [\n",
    "    f.replace(\"Filtered_Gene_Features_\", \"\").replace(\".csv\", \"\")\n",
    "    for f in os.listdir(gene_feature_dir) if f.startswith(\"Filtered_Gene_Features\")\n",
    "]\n",
    "\n",
    "# Storage for overlap results\n",
    "overlap_results = []\n",
    "\n",
    "# Process each dosage\n",
    "for dosage in dosages:\n",
    "    print(f\"\\nüîπ Analyzing Dosage: {dosage}\")\n",
    "\n",
    "    # Define file paths\n",
    "    gene_feature_file = os.path.join(gene_feature_dir, f\"Filtered_Gene_Features_{dosage}.csv\")\n",
    "    gene_pathway_edge_file = os.path.join(gene_pathway_edge_dir, f\"Edges_Gene_Pathway_{dosage}.csv\")\n",
    "\n",
    "    # Ensure files exist\n",
    "    if not os.path.exists(gene_feature_file) or not os.path.exists(gene_pathway_edge_file):\n",
    "        print(f\"‚ö†Ô∏è Skipping {dosage} - Missing required files!\")\n",
    "        continue\n",
    "\n",
    "    # Load data\n",
    "    df_genes = pd.read_csv(gene_feature_file)\n",
    "    df_gene_pathway_edges = pd.read_csv(gene_pathway_edge_file)\n",
    "\n",
    "    # Extract unique genes from both datasets\n",
    "    genes_in_features = set(df_genes[\"Gene\"].str.upper())  # Ensure consistency\n",
    "    genes_in_pathway_edges = set(df_gene_pathway_edges[\"Gene\"].str.upper())\n",
    "\n",
    "    # Compute overlap\n",
    "    overlapping_genes = genes_in_features.intersection(genes_in_pathway_edges)\n",
    "\n",
    "    # Store results\n",
    "    overlap_results.append({\n",
    "        \"Dosage\": dosage,\n",
    "        \"Total Genes (Gene Features)\": len(genes_in_features),\n",
    "        \"Total Genes (Pathway Edges)\": len(genes_in_pathway_edges),\n",
    "        \"Overlapping Genes\": len(overlapping_genes)\n",
    "    })\n",
    "\n",
    "    # Print results for verification\n",
    "    print(f\"üìå Dosage: {dosage} ‚Üí Genes in Features: {len(genes_in_features)}, Genes in Pathway Edges: {len(genes_in_pathway_edges)}, Overlapping: {len(overlapping_genes)}\")\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "df_overlap = pd.DataFrame(overlap_results)\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nüîπ **Final Overlap Analysis for All Dosages** üîπ\")\n",
    "print(df_overlap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ad23ab",
   "metadata": {},
   "source": [
    "### Gene-Gene Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a32c12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define directories\n",
    "gene_feature_dir = \"Graph_Results/Gene_Features\"\n",
    "gene_gene_edge_dir = \"Graph_Results/Gene_Gene_Edges\"\n",
    "gene_pathway_edge_dir = \"Graph_Results/Gene_Pathway_Edges\"\n",
    "\n",
    "# Get all dosage levels\n",
    "dosages = [f.replace(\"Filtered_Gene_Features_\", \"\").replace(\".csv\", \"\") for f in os.listdir(gene_feature_dir) if f.startswith(\"Filtered_Gene_Features\")]\n",
    "\n",
    "# Store validation results\n",
    "validation_results = []\n",
    "\n",
    "# Process each dosage\n",
    "for dosage in dosages:\n",
    "    print(f\"\\nüîπ Validating Dosage: {dosage}...\")\n",
    "\n",
    "    # File paths\n",
    "    gene_file = os.path.join(gene_feature_dir, f\"Filtered_Gene_Features_{dosage}.csv\")\n",
    "    gene_gene_edge_file = os.path.join(gene_gene_edge_dir, f\"Edges_Gene_Gene_{dosage}.csv\")\n",
    "    gene_pathway_edge_file = os.path.join(gene_pathway_edge_dir, f\"Edges_Gene_Pathway_{dosage}.csv\")\n",
    "\n",
    "    # Ensure required files exist\n",
    "    missing_files = [file for file in [gene_file, gene_gene_edge_file, gene_pathway_edge_file] if not os.path.exists(file)]\n",
    "    if missing_files:\n",
    "        print(f\"‚ö†Ô∏è Skipping {dosage} - Missing files: {missing_files}\")\n",
    "        continue\n",
    "\n",
    "    # Load data\n",
    "    df_genes = pd.read_csv(gene_file)\n",
    "    df_gene_gene_edges = pd.read_csv(gene_gene_edge_file)\n",
    "    df_gene_pathway_edges = pd.read_csv(gene_pathway_edge_file)\n",
    "\n",
    "    # Ensure consistent uppercase formatting\n",
    "    genes_in_features = set(df_genes[\"Gene\"].str.upper())\n",
    "    genes_in_gene_gene_edges = set(df_gene_gene_edges[\"Gene1\"].str.upper()).union(set(df_gene_gene_edges[\"Gene2\"].str.upper()))\n",
    "    genes_in_gene_pathway_edges = set(df_gene_pathway_edges[\"Gene\"].str.upper())\n",
    "\n",
    "    # **Corrected Orphan Node Check**\n",
    "    # A gene is considered orphaned only if it has NO connections in both gene-gene and gene-pathway edges.\n",
    "    connected_genes = genes_in_gene_gene_edges | genes_in_gene_pathway_edges\n",
    "    orphaned_genes = genes_in_features - connected_genes\n",
    "\n",
    "    # Store results\n",
    "    validation_results.append({\n",
    "        \"Dosage\": dosage,\n",
    "        \"Total Genes\": len(genes_in_features),\n",
    "        \"Orphaned Genes (No Connections)\": len(orphaned_genes),\n",
    "    })\n",
    "\n",
    "    # Print results\n",
    "    print(f\"üìå Dosage: {dosage} ‚Üí Total Genes: {len(genes_in_features)}, Orphaned: {len(orphaned_genes)}\")\n",
    "\n",
    "print(\"\\n‚úÖ **Orphan Gene Validation Completed!**\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb42ee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define directories\n",
    "gene_feature_dir = \"Graph_Results/Gene_Features\"\n",
    "gene_gene_edge_dir = \"Graph_Results/Gene_Gene_Edges\"\n",
    "\n",
    "# Get all available dosage levels from Gene Features\n",
    "dosages = [\n",
    "    f.replace(\"Filtered_Gene_Features_\", \"\").replace(\".csv\", \"\")\n",
    "    for f in os.listdir(gene_feature_dir)\n",
    "    if f.startswith(\"Filtered_Gene_Features\")\n",
    "]\n",
    "\n",
    "\n",
    "# Process each dosage\n",
    "for dosage in dosages:\n",
    "    gene_file = os.path.join(gene_feature_dir, f\"Filtered_Gene_Features_{dosage}.csv\")\n",
    "    gene_gene_edge_file = os.path.join(gene_gene_edge_dir, f\"Edges_Gene_Gene_{dosage}.csv\")\n",
    "\n",
    "    # Ensure both files exist\n",
    "    if not os.path.exists(gene_file) or not os.path.exists(gene_gene_edge_file):\n",
    "        print(f\"‚ö†Ô∏è Skipping {dosage} - Missing files!\")\n",
    "        continue\n",
    "\n",
    "    # Load gene feature data\n",
    "    df_genes = pd.read_csv(gene_file)\n",
    "    \n",
    "    # Filter genes that **do not have Degree 0**\n",
    "    valid_genes = set(df_genes[df_genes[\"Degree in Pathways\"] > 0][\"Gene\"].str.upper())\n",
    "\n",
    "    # Load gene-gene edges\n",
    "    df_edges = pd.read_csv(gene_gene_edge_file)\n",
    "    \n",
    "    # Find genes that appear in the edges file\n",
    "    genes_in_edges = set(df_edges[\"Gene1\"]).union(set(df_edges[\"Gene2\"]))\n",
    "\n",
    "    # Find the final set of genes that match both conditions\n",
    "    final_genes = valid_genes.intersection(genes_in_edges)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nüìå Dosage: {dosage} ‚Üí Genes in Gene_Features (Degree > 0) & in Gene-Gene Edges: {len(final_genes)}\")\n",
    "    print(sorted(final_genes)[:10])  # Print first 10 for preview\n",
    "\n",
    "print(\"\\n‚úÖ Completed analysis for all dosages!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acea1057",
   "metadata": {},
   "source": [
    "### Validation of the data Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38997d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing Gene-Pathway edge files\n",
    "gene_pathway_edge_dir = \"Graph_Results/Gene_Pathway_Edges\"\n",
    "\n",
    "# Get all dosage levels from the available edge files\n",
    "dosages = [f.replace(\"Edges_Gene_Pathway_\", \"\").replace(\".csv\", \"\") for f in os.listdir(gene_pathway_edge_dir) if f.startswith(\"Edges_Gene_Pathway\")]\n",
    "\n",
    "# Dictionary to store results\n",
    "pathway_gene_counts = {}\n",
    "\n",
    "# Process each dosage\n",
    "for dosage in dosages:\n",
    "    file_path = os.path.join(gene_pathway_edge_dir, f\"Edges_Gene_Pathway_{dosage}.csv\")\n",
    "\n",
    "    # Load the data\n",
    "    df_edges = pd.read_csv(file_path)\n",
    "\n",
    "    # **Fix: Count only unique genes across all pathways**\n",
    "    unique_genes = df_edges[\"Gene\"].nunique()\n",
    "\n",
    "    # Store the result\n",
    "    pathway_gene_counts[dosage] = unique_genes\n",
    "\n",
    "    print(f\"üìå Dosage: {dosage} ‚Üí Total Unique Genes in Pathways: {unique_genes}\")\n",
    "\n",
    "print(\"\\n‚úÖ Completed analysis of unique genes in pathways for all dosages!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7db3611",
   "metadata": {},
   "source": [
    "### Seeing the Orphan Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3083d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define directories\n",
    "gene_feature_dir = \"Graph_Results/Gene_Features\"\n",
    "pathway_feature_dir = \"Graph_Results/Pathway_Features\"\n",
    "gene_pathway_edge_dir = \"Graph_Results/Gene_Pathway_Edge\"\n",
    "gene_gene_edge_dir = \"Graph_Results/Gene_Gene_Edges\"\n",
    "\n",
    "# Storage for orphan analysis results\n",
    "orphan_gene_results = []\n",
    "orphan_pathway_results = []\n",
    "\n",
    "print(\"\\nüîç **Checking for Orphan Genes & Pathways** üîé\\n\")\n",
    "\n",
    "# Get all dosage levels based on gene features\n",
    "dosages = [\n",
    "    f.replace(\"Filtered_Gene_Features_\", \"\").replace(\".csv\", \"\")\n",
    "    for f in os.listdir(gene_feature_dir) if f.startswith(\"Filtered_Gene_Features\")\n",
    "]\n",
    "\n",
    "# Process each dosage\n",
    "for dosage in dosages:\n",
    "    print(f\"\\nüîπ Validating Dosage: {dosage}...\")\n",
    "\n",
    "    # Define file paths\n",
    "    gene_feature_file = os.path.join(gene_feature_dir, f\"Filtered_Gene_Features_{dosage}.csv\")\n",
    "    pathway_feature_file = os.path.join(pathway_feature_dir, f\"Pathway_Features_{dosage}.csv\")\n",
    "    gene_pathway_edge_file = os.path.join(gene_pathway_edge_dir, f\"Edges_Gene_Pathway_{dosage}.csv\")\n",
    "    gene_gene_edge_file = os.path.join(gene_gene_edge_dir, f\"Edges_Gene_Gene_{dosage}.csv\")\n",
    "\n",
    "    # Ensure all files exist before proceeding\n",
    "    missing_files = []\n",
    "    for file in [gene_feature_file, pathway_feature_file, gene_pathway_edge_file, gene_gene_edge_file]:\n",
    "        if not os.path.exists(file):\n",
    "            missing_files.append(file)\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"‚ö†Ô∏è Skipping {dosage} - Missing files: {missing_files}\")\n",
    "        continue\n",
    "\n",
    "    # Load gene-related data\n",
    "    df_genes = pd.read_csv(gene_feature_file)\n",
    "    df_gene_pathway_edges = pd.read_csv(gene_pathway_edge_file)\n",
    "    df_gene_gene_edges = pd.read_csv(gene_gene_edge_file)\n",
    "\n",
    "    # Extract unique genes from datasets\n",
    "    genes_in_features = set(df_genes[\"Gene\"].str.upper())\n",
    "    genes_in_pathway_edges = set(df_gene_pathway_edges[\"Gene\"].str.upper())\n",
    "    genes_in_gene_gene_edges = set(df_gene_gene_edges[\"Gene1\"].str.upper()).union(set(df_gene_gene_edges[\"Gene2\"].str.upper()))\n",
    "\n",
    "    # Compute orphan genes (genes in features but NOT in any edges)\n",
    "    connected_genes = genes_in_pathway_edges.union(genes_in_gene_gene_edges)\n",
    "    orphaned_genes = genes_in_features - connected_genes  # Only genes with NO edges\n",
    "\n",
    "    # Store gene orphan results\n",
    "    orphan_gene_results.append({\n",
    "        \"Dosage\": dosage,\n",
    "        \"Total Genes\": len(genes_in_features),\n",
    "        \"Orphaned Genes (No Edges)\": len(orphaned_genes)\n",
    "    })\n",
    "\n",
    "    print(f\"üìå Total Genes: {len(genes_in_features)} | Orphaned Genes (No Connections): {len(orphaned_genes)}\")\n",
    "\n",
    "    # Load pathway-related data\n",
    "    df_pathway_features = pd.read_csv(pathway_feature_file)\n",
    "    df_gene_pathway_edges = pd.read_csv(gene_pathway_edge_file)\n",
    "\n",
    "    # Extract unique pathways\n",
    "    feature_pathways = set(df_pathway_features[\"Term\"].str.strip().str.upper())\n",
    "    edge_pathways = set(df_gene_pathway_edges[\"Pathway\"].str.strip().str.upper())\n",
    "\n",
    "    # Compute orphan pathways (pathways in features but missing in edges)\n",
    "    orphaned_pathways = feature_pathways - edge_pathways\n",
    "\n",
    "    # Store pathway orphan results\n",
    "    orphan_pathway_results.append({\n",
    "        \"Dosage\": dosage,\n",
    "        \"Total Pathways\": len(feature_pathways),\n",
    "        \"Orphaned Pathways (No Gene Connections)\": len(orphaned_pathways)\n",
    "    })\n",
    "\n",
    "    print(f\"üìå Total Pathways: {len(feature_pathways)} | Orphaned Pathways (No Connections): {len(orphaned_pathways)}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Convert results to DataFrame for final analysis\n",
    "df_orphan_genes = pd.DataFrame(orphan_gene_results)\n",
    "df_orphan_pathways = pd.DataFrame(orphan_pathway_results)\n",
    "\n",
    "# Display final summary\n",
    "print(\"\\nüîπ **Final Orphan Node Analysis (Genes & Pathways)** üîπ\\n\")\n",
    "print(df_orphan_genes)\n",
    "print(\"\\n\")\n",
    "print(df_orphan_pathways)\n",
    "\n",
    "print(\"\\n‚úÖ **Orphan Node & Pathway Validation Completed!** ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f626103",
   "metadata": {},
   "source": [
    "### Remove Orphan Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0337bb8",
   "metadata": {},
   "source": [
    "## Step 4 : Process Cell data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460e1a59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file path\n",
    "scRNA_file = \"merged_scRNA_metadata_normalized.tsv\"\n",
    "\n",
    "# Load the TSV file\n",
    "df_scrna = pd.read_csv(scRNA_file, sep=\"\\t\")  # Use tab separator for TSV files\n",
    "\n",
    "# Display basic information\n",
    "print(\"üìÇ File Loaded Successfully!\")\n",
    "print(\"Data Overview:\")\n",
    "print(df_scrna.head())  # Show first few rows\n",
    "\n",
    "print(\"\\nüìä Column Names:\")\n",
    "print(df_scrna.columns)  # Show all column names\n",
    "\n",
    "print(\"\\nüîç Data Types:\")\n",
    "print(df_scrna.dtypes)  # Show data types of each column\n",
    "\n",
    "print(\"\\nüõ†Ô∏è Summary Statistics:\")\n",
    "print(df_scrna.describe(include=\"all\"))  # Get summary statistics for all columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eadb0b",
   "metadata": {},
   "source": [
    "### Normalizing nUMI and nGene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b30223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "# Load scRNA metadata file\n",
    "scRNA_file = \"merged_scRNA_metadata_normalized.tsv\"\n",
    "df_cells = pd.read_csv(scRNA_file, sep=\"\\t\")\n",
    "\n",
    "# Step 1: Normalize nGene and nUMI using Min-Max Scaling\n",
    "columns_to_normalize = [\"nGene\", \"nUMI\"]\n",
    "scaler = MinMaxScaler()\n",
    "df_cells[columns_to_normalize] = scaler.fit_transform(df_cells[columns_to_normalize])\n",
    "\n",
    "# Step 2: Encode 'State' as numerical values (S1 -> 0, S2 -> 1, ...)\n",
    "label_encoder = LabelEncoder()\n",
    "df_cells[\"State_Encoded\"] = label_encoder.fit_transform(df_cells[\"State\"])\n",
    "\n",
    "# Save the processed cell features\n",
    "processed_output_file = \"Graph_Results/Cell_Features_Normalized.csv\"\n",
    "df_cells.to_csv(processed_output_file, index=False)\n",
    "\n",
    "print(f\"‚úÖ Normalization & Encoding complete! Saved processed cell features to {processed_output_file}\")\n",
    "print(df_cells.head())  # Display sample results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea064142",
   "metadata": {},
   "source": [
    "### Generating the Cell Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605916e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define directories\n",
    "cell_feature_file = \"Graph_Results/Cell_Features_Normalized.csv\"  # Processed cell metadata\n",
    "deg_dir = \"DEG_Results\"  # DEG results folder\n",
    "output_dir = \"Graph_Results/Cell_Features\"  # Output directory for cell features per dosage\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load the normalized cell feature data\n",
    "df_cells = pd.read_csv(cell_feature_file)\n",
    "\n",
    "# Extract core cell features\n",
    "df_cells = df_cells[[\"cell\", \"nGene\", \"nUMI\", \"orig.ident\", \"State\"]]  # Keep \"State\"\n",
    "\n",
    "# üîπ **Create Correct State Encoding Mapping**\n",
    "unique_states = sorted(df_cells[\"State\"].unique())  # Get unique states (C, S1, S2, ..., S5)\n",
    "state_mapping = {state: idx for idx, state in enumerate(unique_states)}  # Assign numeric labels\n",
    "df_cells[\"State_Encoded\"] = df_cells[\"State\"].map(state_mapping)  # Apply encoding\n",
    "\n",
    "# Get all available dosage levels (excluding Control)\n",
    "dosages = sorted([d for d in os.listdir(deg_dir) if d.startswith(\"DEG_\") and d != \"DEG_C\"])\n",
    "\n",
    "print(\"\\nüîç **Processing Cell Features for Each Dosage** üîé\")\n",
    "\n",
    "# Process each dosage separately\n",
    "for dosage in dosages:\n",
    "    dosage_level = dosage.replace(\"DEG_\", \"\")  # Extract dosage (e.g., T1, T2.5, ..., T320)\n",
    "    \n",
    "    # Define DEG file path for this dosage\n",
    "    deg_file = os.path.join(deg_dir, dosage, f\"DEG_{dosage_level}_vs_Control.csv\")\n",
    "    \n",
    "    # Ensure the DEG file exists\n",
    "    if not os.path.exists(deg_file):\n",
    "        print(f\"‚ö†Ô∏è Skipping {dosage_level} - Missing DEG file!\")\n",
    "        continue\n",
    "\n",
    "    # Load significant genes for this dosage\n",
    "    df_deg = pd.read_csv(deg_file)\n",
    "    significant_genes = set(df_deg[\"Gene\"].str.upper())  # Convert to uppercase for consistency\n",
    "\n",
    "    # ‚úÖ **Fix: Use `orig.ident` for filtering cells for the given dosage**\n",
    "    df_filtered_cells = df_cells[df_cells[\"orig.ident\"] == dosage_level].copy()  # Filter by orig.ident\n",
    "\n",
    "    # Retain only the significant genes from DEG\n",
    "    retained_genes = [gene for gene in significant_genes if gene in df_filtered_cells.columns]\n",
    "\n",
    "    # Drop all non-significant gene expression columns\n",
    "    df_filtered_cells = df_filtered_cells[[\"cell\", \"nGene\", \"nUMI\", \"State_Encoded\"] + retained_genes]\n",
    "\n",
    "    # Save the filtered cell feature file\n",
    "    output_file = os.path.join(output_dir, f\"Cell_Features_{dosage_level}.csv\")\n",
    "    df_filtered_cells.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"‚úÖ Processed {dosage_level}: Saved cell node features!\")\n",
    "\n",
    "print(\"\\n‚úÖ **Cell Node Feature Generation Completed!** ‚úÖ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9917b980",
   "metadata": {},
   "source": [
    "## Final Cell- Gene edge generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b8f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define directories\n",
    "deg_dir = \"DEG_Results\"\n",
    "cell_feature_file = \"Graph_Results/Cell_Features_Normalized.csv\"\n",
    "output_dir = \"Graph_Results/Cell_Gene_Edges\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load full normalized cell feature dataset\n",
    "df_cells = pd.read_csv(cell_feature_file)\n",
    "\n",
    "# Identify metadata columns\n",
    "metadata_cols = [\"cell\", \"nGene\", \"nUMI\", \"orig.ident\", \"percent.mito\", \"State\", \"State_Encoded\"]\n",
    "\n",
    "# Get gene expression columns and map to uppercase\n",
    "gene_expression_cols = [col for col in df_cells.columns if col not in metadata_cols]\n",
    "gene_expression_cols_upper = {col.upper(): col for col in gene_expression_cols}\n",
    "\n",
    "# List all dosage folders\n",
    "dosages = [d.replace(\"DEG_\", \"\") for d in os.listdir(deg_dir) if d.startswith(\"DEG_\")]\n",
    "\n",
    "print(\"\\nüîß Generating Cell-Gene Edges Per Dosage (Filtered by DEG & Cells)...\")\n",
    "\n",
    "for dosage in dosages:\n",
    "    deg_file = os.path.join(deg_dir, f\"DEG_{dosage}\", f\"DEG_{dosage}_vs_Control.csv\")\n",
    "\n",
    "    if not os.path.exists(deg_file):\n",
    "        print(f\"‚ö†Ô∏è Skipping {dosage} - DEG file not found.\")\n",
    "        continue\n",
    "\n",
    "    # Load DEG genes for the dosage\n",
    "    df_deg = pd.read_csv(deg_file)\n",
    "    deg_genes = df_deg[\"Gene\"].str.upper().str.strip().tolist()\n",
    "\n",
    "    # Map DEG genes to actual expression columns (case-insensitive)\n",
    "    valid_gene_cols = [gene_expression_cols_upper[gene] for gene in deg_genes if gene in gene_expression_cols_upper]\n",
    "\n",
    "    if not valid_gene_cols:\n",
    "        print(f\"‚ö†Ô∏è Skipping {dosage} - No matching DEG genes in cell features.\")\n",
    "        continue\n",
    "\n",
    "    # üîπ Filter cells corresponding only to current dosage\n",
    "    df_filtered_cells = df_cells[df_cells[\"orig.ident\"] == dosage].copy()\n",
    "\n",
    "    if df_filtered_cells.empty:\n",
    "        print(f\"‚ö†Ô∏è Skipping {dosage} - No cells found for this dosage in orig.ident.\")\n",
    "        continue\n",
    "\n",
    "    # Extract only relevant genes + cell column\n",
    "    df_subset = df_filtered_cells[[\"cell\"] + valid_gene_cols].copy()\n",
    "\n",
    "    # Melt to long format: (cell, gene, expression)\n",
    "    df_edges = df_subset.melt(id_vars=\"cell\", var_name=\"Gene\", value_name=\"Edge Weight\")\n",
    "\n",
    "    # Drop edges with zero expression\n",
    "    df_edges = df_edges[df_edges[\"Edge Weight\"] > 0]\n",
    "\n",
    "    # Save final edge file\n",
    "    output_file = os.path.join(output_dir, f\"Cell_Gene_Edges_{dosage}.csv\")\n",
    "    df_edges.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"‚úÖ {dosage}: Saved {len(df_edges)} edges for {df_filtered_cells.shape[0]} cells and {len(valid_gene_cols)} genes.\")\n",
    "\n",
    "print(\"\\nüéØ All Cell-Gene Edges (filtered) saved to `Graph_Results/Cell_Gene_Edges/` ‚úÖ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc44daa",
   "metadata": {},
   "source": [
    "# Graph Construction with Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7939a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define folders\n",
    "base_dir = \"Graph_Results\"\n",
    "gene_feat_dir = f\"{base_dir}/Gene_Features\"\n",
    "gene_pathway_edge_dir = f\"{base_dir}/Gene_Pathway_Edge\"\n",
    "cell_gene_edge_dir = f\"{base_dir}/Cell_Gene_Edges\"\n",
    "\n",
    "# Output folder (overwrite original)\n",
    "os.makedirs(gene_feat_dir, exist_ok=True)\n",
    "\n",
    "# Get all dosage levels\n",
    "files = [f for f in os.listdir(gene_feat_dir) if f.startswith(\"Filtered_Gene_Features_\")]\n",
    "dosages = [f.replace(\"Filtered_Gene_Features_\", \"\").replace(\".csv\", \"\") for f in files]\n",
    "\n",
    "for dosage in dosages:\n",
    "    try:\n",
    "        gene_feat_file = os.path.join(gene_feat_dir, f\"Filtered_Gene_Features_{dosage}.csv\")\n",
    "        df_gene = pd.read_csv(gene_feat_file)\n",
    "        genes_in_features = set(df_gene[\"Gene\"].str.upper())\n",
    "\n",
    "        # Collect genes from edges\n",
    "        genes_from_edges = set()\n",
    "\n",
    "        # Gene-Pathway\n",
    "        gp_file = os.path.join(gene_pathway_edge_dir, f\"Edges_Gene_Pathway_{dosage}.csv\")\n",
    "        if os.path.exists(gp_file):\n",
    "            df_gp = pd.read_csv(gp_file)\n",
    "            genes_from_edges.update(df_gp[\"Gene\"].str.upper())\n",
    "\n",
    "        # Cell-Gene\n",
    "        cg_file = os.path.join(cell_gene_edge_dir, f\"Cell_Gene_Edges_{dosage}.csv\")\n",
    "        if os.path.exists(cg_file):\n",
    "            df_cg = pd.read_csv(cg_file)\n",
    "            genes_from_edges.update(df_cg[\"Gene\"].str.upper())\n",
    "\n",
    "        # Filter gene features\n",
    "        valid_genes = genes_in_features.intersection(genes_from_edges)\n",
    "        df_filtered = df_gene[df_gene[\"Gene\"].str.upper().isin(valid_genes)]\n",
    "\n",
    "        dropped_count = len(df_gene) - len(df_filtered)\n",
    "        print(f\"‚úÖ {dosage}: Kept {len(df_filtered)} genes, Dropped {dropped_count} genes\")\n",
    "\n",
    "        # Save (overwrite)\n",
    "        df_filtered.to_csv(gene_feat_file, index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed for {dosage}: {e}\")\n",
    "\n",
    "print(\"\\nüéØ Finished cleaning all gene feature files (excluding Gene-Gene edges)!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a1729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "cell_feat_dir = \"Graph_Results/Cell_Features\"\n",
    "cell_gene_edge_dir = \"Graph_Results/Cell_Gene_Edges\"\n",
    "\n",
    "cell_files = [f for f in os.listdir(cell_feat_dir) if f.startswith(\"Cell_Features_\")]\n",
    "dosages = [f.replace(\"Cell_Features_\", \"\").replace(\".csv\", \"\") for f in cell_files]\n",
    "\n",
    "for dosage in dosages:\n",
    "    try:\n",
    "        cell_file = os.path.join(cell_feat_dir, f\"Cell_Features_{dosage}.csv\")\n",
    "        edge_file = os.path.join(cell_gene_edge_dir, f\"Cell_Gene_Edges_{dosage}.csv\")\n",
    "\n",
    "        if not os.path.exists(edge_file):\n",
    "            print(f\"‚ö†Ô∏è Skipping {dosage} ‚Äî no cell-gene edges\")\n",
    "            continue\n",
    "\n",
    "        df_cells = pd.read_csv(cell_file)\n",
    "        df_edges = pd.read_csv(edge_file)\n",
    "\n",
    "        valid_cells = set(df_edges[\"cell\"])\n",
    "        df_filtered = df_cells[df_cells[\"cell\"].isin(valid_cells)]\n",
    "\n",
    "        df_filtered.to_csv(cell_file, index=False)\n",
    "        print(f\"‚úÖ Cleaned cell features for {dosage}: {len(df_filtered)} cells\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed for {dosage}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21f9680",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathway_feat_dir = \"Graph_Results/Pathway_Features\"\n",
    "gene_pathway_edge_dir = \"Graph_Results/Gene_Pathway_Edge\"\n",
    "\n",
    "pathway_files = [f for f in os.listdir(pathway_feat_dir) if f.startswith(\"Pathway_Features_\")]\n",
    "dosages = [f.replace(\"Pathway_Features_\", \"\").replace(\".csv\", \"\") for f in pathway_files]\n",
    "\n",
    "for dosage in dosages:\n",
    "    try:\n",
    "        feat_file = os.path.join(pathway_feat_dir, f\"Pathway_Features_{dosage}.csv\")\n",
    "        edge_file = os.path.join(gene_pathway_edge_dir, f\"Edges_Gene_Pathway_{dosage}.csv\")\n",
    "\n",
    "        if not os.path.exists(edge_file):\n",
    "            print(f\"‚ö†Ô∏è Skipping {dosage} ‚Äî no gene-pathway edges\")\n",
    "            continue\n",
    "\n",
    "        df_feat = pd.read_csv(feat_file)\n",
    "        df_edge = pd.read_csv(edge_file)\n",
    "\n",
    "        valid_pathways = set(df_edge[\"Pathway\"].str.upper().str.strip())\n",
    "        df_feat[\"Term\"] = df_feat[\"Term\"].str.upper().str.strip()\n",
    "        df_filtered = df_feat[df_feat[\"Term\"].isin(valid_pathways)]\n",
    "\n",
    "        df_filtered.to_csv(feat_file, index=False)\n",
    "        print(f\"‚úÖ Cleaned pathway features for {dosage}: {len(df_filtered)} pathways\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed for {dosage}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3008e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gene_df.dtypes)\n",
    "print(cell_df.dtypes)\n",
    "print(path_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b214d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "\n",
    "# Directories\n",
    "base_dir = \"Graph_Results\"\n",
    "gene_feat_dir = os.path.join(base_dir, \"Gene_Features\")\n",
    "cell_feat_dir = os.path.join(base_dir, \"Cell_Features\")\n",
    "pathway_feat_dir = os.path.join(base_dir, \"Pathway_Features\")\n",
    "gene_pathway_edge_dir = os.path.join(base_dir, \"Gene_Pathway_Edge\")\n",
    "cell_gene_edge_dir = os.path.join(base_dir, \"Cell_Gene_Edges\")\n",
    "\n",
    "graph_output_dir = os.path.join(base_dir, \"HeteroGraphs\")\n",
    "mapping_output_dir = os.path.join(base_dir, \"Graph_Mappings\")\n",
    "os.makedirs(graph_output_dir, exist_ok=True)\n",
    "os.makedirs(mapping_output_dir, exist_ok=True)\n",
    "\n",
    "# List dosages\n",
    "dosages = [f.replace(\"Filtered_Gene_Features_\", \"\").replace(\".csv\", \"\")\n",
    "           for f in os.listdir(gene_feat_dir) if f.endswith(\".csv\") and \"checkpoint\" not in f]\n",
    "\n",
    "for dosage in tqdm(dosages, desc=\"üîÑ Building Graphs\"):\n",
    "    try:\n",
    "        data = HeteroData()\n",
    "\n",
    "        # Load features\n",
    "        gene_df = pd.read_csv(os.path.join(gene_feat_dir, f\"Filtered_Gene_Features_{dosage}.csv\"))\n",
    "        cell_df = pd.read_csv(os.path.join(cell_feat_dir, f\"Cell_Features_{dosage}.csv\"))\n",
    "        path_df = pd.read_csv(os.path.join(pathway_feat_dir, f\"Pathway_Features_{dosage}.csv\"))\n",
    "\n",
    "        # Normalize pathway names\n",
    "        path_df[\"Term\"] = path_df[\"Term\"].str.lower().str.strip()\n",
    "\n",
    "        # Load edges\n",
    "        gene_path_df = pd.read_csv(os.path.join(gene_pathway_edge_dir, f\"Edges_Gene_Pathway_{dosage}.csv\"))\n",
    "        cell_gene_df = pd.read_csv(os.path.join(cell_gene_edge_dir, f\"Cell_Gene_Edges_{dosage}.csv\"))\n",
    "\n",
    "        # Index maps\n",
    "        gene_idx = {g: i for i, g in enumerate(gene_df[\"Gene\"].str.upper())}\n",
    "        cell_idx = {c: i for i, c in enumerate(cell_df[\"cell\"])}\n",
    "        path_idx = {p: i for i, p in enumerate(path_df[\"Term\"])}\n",
    "\n",
    "        # One-hot encode state column (5 states)\n",
    "        num_states = 5\n",
    "        one_hot = pd.get_dummies(cell_df[\"State_Encoded\"], prefix='State', prefix_sep='_')\n",
    "        for s in range(1, num_states + 1):\n",
    "            col = f\"State_{s}\"\n",
    "            if col not in one_hot.columns:\n",
    "                one_hot[col] = 0\n",
    "        one_hot = one_hot.sort_index(axis=1)\n",
    "        cell_features = pd.concat([cell_df[[\"nGene\", \"nUMI\"]], one_hot], axis=1)\n",
    "\n",
    "        # ‚úÖ Strict numeric conversions + DEBUG CHECK\n",
    "        gene_feat_cols = [\"Log2FC\", \"Degree in Pathways\"]\n",
    "        gene_features = gene_df[gene_feat_cols].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        print(f\"\\n[DEBUG] {dosage} gene features dtypes:\\n\", gene_features.dtypes)\n",
    "\n",
    "        cell_numeric_cols = [\"nGene\", \"nUMI\"]\n",
    "        cell_one_hot_cols = [col for col in cell_features.columns if col.startswith(\"State_\")]\n",
    "        cell_features_cleaned = cell_features[cell_numeric_cols + cell_one_hot_cols].apply(pd.to_numeric, errors='coerce').fillna(0).astype(float)\n",
    "        print(f\"[DEBUG] {dosage} cell features dtypes (AFTER float cast):\\n\", cell_features_cleaned.dtypes)\n",
    "\n",
    "        pathway_feat_cols = [\"Combined Score\", \"Pathway Size\"]\n",
    "        pathway_features = path_df[pathway_feat_cols].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        print(f\"[DEBUG] {dosage} pathway features dtypes:\\n\", pathway_features.dtypes)\n",
    "\n",
    "        # ‚úÖ Convert to tensors\n",
    "        data[\"gene\"].x = torch.tensor(gene_features.astype(float).values, dtype=torch.float)\n",
    "        data[\"cell\"].x = torch.tensor(cell_features_cleaned.values, dtype=torch.float)\n",
    "        data[\"pathway\"].x = torch.tensor(pathway_features.astype(float).values, dtype=torch.float)\n",
    "\n",
    "        edge_summary = {}\n",
    "\n",
    "        # Gene-Pathway edges\n",
    "        gp_src, gp_tgt, gp_weight = [], [], []\n",
    "        gene_path_df[\"Pathway\"] = gene_path_df[\"Pathway\"].str.lower().str.strip()\n",
    "        for _, row in gene_path_df.iterrows():\n",
    "            g, p = row[\"Gene\"].upper(), row[\"Pathway\"]\n",
    "            if g in gene_idx and p in path_idx:\n",
    "                gp_src.append(gene_idx[g])\n",
    "                gp_tgt.append(path_idx[p])\n",
    "                gp_weight.append([float(row[\"Edge Weight\"])])\n",
    "        if gp_src:\n",
    "            data[\"gene\", \"involved_in\", \"pathway\"].edge_index = torch.tensor([gp_src, gp_tgt], dtype=torch.long)\n",
    "            data[\"gene\", \"involved_in\", \"pathway\"].edge_attr = torch.tensor(gp_weight, dtype=torch.float)\n",
    "            edge_summary[\"gene‚Üíinvolved_in‚Üípathway\"] = len(gp_src)\n",
    "\n",
    "        # Cell-Gene edges\n",
    "        cg_src, cg_tgt, cg_weight = [], [], []\n",
    "        for _, row in cell_gene_df.iterrows():\n",
    "            c, g = row[\"cell\"], row[\"Gene\"].upper()\n",
    "            if c in cell_idx and g in gene_idx:\n",
    "                cg_src.append(cell_idx[c])\n",
    "                cg_tgt.append(gene_idx[g])\n",
    "                cg_weight.append([float(row[\"Edge Weight\"])])\n",
    "        if cg_src:\n",
    "            data[\"cell\", \"expresses\", \"gene\"].edge_index = torch.tensor([cg_src, cg_tgt], dtype=torch.long)\n",
    "            data[\"cell\", \"expresses\", \"gene\"].edge_attr = torch.tensor(cg_weight, dtype=torch.float)\n",
    "            edge_summary[\"cell‚Üíexpresses‚Üígene\"] = len(cg_src)\n",
    "\n",
    "        # Save graph\n",
    "        torch.save(data, os.path.join(graph_output_dir, f\"HeteroGraph_{dosage}.pt\"))\n",
    "\n",
    "        # Analyze graph summary\n",
    "        G = nx.Graph()\n",
    "        for ntype in data.node_types:\n",
    "            G.add_nodes_from([f\"{ntype}_{i}\" for i in range(data[ntype].num_nodes)], type=ntype)\n",
    "\n",
    "        for (src, rel, dst), edge_index in data.edge_index_dict.items():\n",
    "            for i in range(edge_index.size(1)):\n",
    "                u = f\"{src}_{edge_index[0, i].item()}\"\n",
    "                v = f\"{dst}_{edge_index[1, i].item()}\"\n",
    "                G.add_edge(u, v, type=rel)\n",
    "\n",
    "        isolated_nodes = list(nx.isolates(G))\n",
    "        isolated_by_type = dict(Counter([n.split(\"_\")[0] for n in isolated_nodes]))\n",
    "\n",
    "        # Save mapping JSON\n",
    "        with open(os.path.join(mapping_output_dir, f\"Graph_Mapping_{dosage}.json\"), \"w\") as f:\n",
    "            json.dump({\n",
    "                \"dosage\": dosage,\n",
    "                \"gene_to_index\": gene_idx,\n",
    "                \"cell_to_index\": cell_idx,\n",
    "                \"pathway_to_index\": path_idx,\n",
    "                \"node_counts\": {k: v.num_nodes for k, v in data.items() if isinstance(v, HeteroData)},\n",
    "                \"edge_counts\": edge_summary,\n",
    "                \"isolated_node_count\": len(isolated_nodes),\n",
    "                \"isolated_node_by_type\": isolated_by_type\n",
    "            }, f, indent=2)\n",
    "\n",
    "        print(f\"‚úÖ Graph & JSON saved for {dosage}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed for {dosage}: {e}\")\n",
    "\n",
    "print(\"\\nüéØ All graphs and mapping JSONs generated!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54f9135",
   "metadata": {},
   "source": [
    "## Inspecting the Graphs created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a563f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "from torch_geometric.data import HeteroData\n",
    "from collections import Counter\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "# === Setup ===\n",
    "base_dir = \"Graph_Results\"\n",
    "graph_dir = os.path.join(base_dir, \"HeteroGraphs\")\n",
    "map_dir = os.path.join(base_dir, \"Graph_Mappings\")\n",
    "\n",
    "# === Inspect Each Graph ===\n",
    "graph_files = sorted([f for f in os.listdir(graph_dir) if f.endswith(\".pt\")])\n",
    "\n",
    "for file in graph_files:\n",
    "    dosage = file.replace(\"HeteroGraph_\", \"\").replace(\".pt\", \"\")\n",
    "    print(f\"\\nüîç Inspecting Graph for Dosage: {dosage}\")\n",
    "\n",
    "    graph_path = os.path.join(graph_dir, file)\n",
    "    mapping_path = os.path.join(map_dir, f\"Graph_Mapping_{dosage}.json\")\n",
    "\n",
    "    # Load graph and mapping\n",
    "    data = torch.load(graph_path)\n",
    "    with open(mapping_path, \"r\") as f:\n",
    "        mapping = json.load(f)\n",
    "\n",
    "    # === Node Summary ===\n",
    "    print(\"üß¨ Node Types & Counts:\")\n",
    "    for ntype in data.node_types:\n",
    "        print(f\"  - {ntype}: {data[ntype].num_nodes} nodes | Features: {tuple(data[ntype].x.shape)}\")\n",
    "\n",
    "        # Feature stats\n",
    "        x = data[ntype].x\n",
    "        print(f\"    üìä Feature Mean: {x.mean(dim=0)}\")\n",
    "        print(f\"    üìä Feature Std:  {x.std(dim=0)}\")\n",
    "        print(f\"    üìä Feature Min:  {x.min(dim=0)[0]}\")\n",
    "        print(f\"    üìä Feature Max:  {x.max(dim=0)[0]}\")\n",
    "\n",
    "    # === Edge Summary ===\n",
    "    print(\"\\nüîó Edge Types & Counts:\")\n",
    "    for rel in data.edge_types:\n",
    "        eidx = data[rel].edge_index\n",
    "        print(f\"  - {rel[0]} ‚Üí {rel[1]} ‚Üí {rel[2]}: {eidx.shape[1]} edges\")\n",
    "\n",
    "        # Edge attribute stats\n",
    "        if data[rel].edge_attr is not None:\n",
    "            eattr = data[rel].edge_attr\n",
    "            print(f\"    üìä Edge Weight Mean: {eattr.mean(dim=0)}\")\n",
    "            print(f\"    üìä Edge Weight Std:  {eattr.std(dim=0)}\")\n",
    "            print(f\"    üìä Edge Weight Min:  {eattr.min(dim=0)[0]}\")\n",
    "            print(f\"    üìä Edge Weight Max:  {eattr.max(dim=0)[0]}\")\n",
    "\n",
    "    # === Isolated Node Summary ===\n",
    "    print(\"\\n‚ùó Isolated Node Summary:\")\n",
    "    print(f\"  - Total Isolated Nodes: {mapping['isolated_node_count']}\")\n",
    "    for t, count in mapping['isolated_node_by_type'].items():\n",
    "        print(f\"    ‚Ä¢ {t}: {count}\")\n",
    "\n",
    "    # === Degree Distribution & Density ===\n",
    "    nx_graph = to_networkx(data, to_undirected=False)\n",
    "    degrees = dict(nx_graph.degree())\n",
    "    degree_counts = Counter(degrees.values())\n",
    "\n",
    "    print(\"\\nüìà Node Degree Distribution (Top 10):\")\n",
    "    for deg, count in degree_counts.most_common(10):\n",
    "        print(f\"    Degree {deg}: {count} nodes\")\n",
    "\n",
    "    num_nodes = nx_graph.number_of_nodes()\n",
    "    num_edges = nx_graph.number_of_edges()\n",
    "    possible_edges = num_nodes * (num_nodes - 1)\n",
    "    density = num_edges / possible_edges if possible_edges > 0 else 0\n",
    "    print(f\"\\n‚öô Graph Size: {num_nodes} nodes, {num_edges} edges\")\n",
    "    print(f\"‚öô Approx. Graph Density: {density:.6f}\")\n",
    "\n",
    "    # === Mapping Consistency Check ===\n",
    "    gene_map_count = len(mapping.get('gene_to_index', {}))\n",
    "    cell_map_count = len(mapping.get('cell_to_index', {}))\n",
    "    pathway_map_count = len(mapping.get('pathway_to_index', {}))\n",
    "\n",
    "    if gene_map_count != data['gene'].num_nodes:\n",
    "        print(f\"‚ö† Gene index mismatch: {gene_map_count} vs {data['gene'].num_nodes}\")\n",
    "    if cell_map_count != data['cell'].num_nodes:\n",
    "        print(f\"‚ö† Cell index mismatch: {cell_map_count} vs {data['cell'].num_nodes}\")\n",
    "    if pathway_map_count != data['pathway'].num_nodes:\n",
    "        print(f\"‚ö† Pathway index mismatch: {pathway_map_count} vs {data['pathway'].num_nodes}\")\n",
    "\n",
    "print(\"\\nüéØ All graphs inspected!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150ed787",
   "metadata": {},
   "source": [
    "### Features Scaling of the Graphs constructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ce7235",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Directory where graphs are stored\n",
    "graph_dir = 'Graph_Results/HeteroGraphs'\n",
    "\n",
    "# Get all graph files\n",
    "graph_files = sorted([f for f in os.listdir(graph_dir) if f.endswith('.pt')])\n",
    "\n",
    "print(f\"üîç Inspecting {len(graph_files)} graphs...\\n\")\n",
    "\n",
    "for file in graph_files:\n",
    "    graph_path = os.path.join(graph_dir, file)\n",
    "    data = torch.load(graph_path)\n",
    "\n",
    "    print(f\"üìÇ Graph: {file}\")\n",
    "\n",
    "    for node_type in data.node_types:\n",
    "        if hasattr(data[node_type], 'x') and data[node_type].x is not None:\n",
    "            x = data[node_type].x\n",
    "            if torch.is_floating_point(x):\n",
    "                arr = x.cpu().numpy()\n",
    "                mean = np.mean(arr, axis=0)\n",
    "                std = np.std(arr, axis=0)\n",
    "                min_val = np.min(arr, axis=0)\n",
    "                max_val = np.max(arr, axis=0)\n",
    "\n",
    "                print(f\"  üß¨ Node Type: {node_type}\")\n",
    "                print(f\"    ‚Üí Mean: {np.round(mean, 4)}\")\n",
    "                print(f\"    ‚Üí Std:  {np.round(std, 4)}\")\n",
    "                print(f\"    ‚Üí Min:  {np.round(min_val, 4)}\")\n",
    "                print(f\"    ‚Üí Max:  {np.round(max_val, 4)}\\n\")\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è Node type {node_type} has non-numeric features, skipping.\\n\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Node type {node_type} has no 'x' features, skipping.\\n\")\n",
    "\n",
    "print(\"\\n‚úÖ All graphs inspected!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96ba222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# === Directories ===\n",
    "input_dir = 'Graph_Results/HeteroGraphs'\n",
    "output_dir = 'Graph_Results/HeteroGraphs_ScaledFinal'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "graph_files = [f for f in os.listdir(input_dir) if f.endswith('.pt')]\n",
    "\n",
    "# === Collect all features across graphs ===\n",
    "node_type_features = {}\n",
    "edge_type_features = {}\n",
    "node_max_dims = {}\n",
    "edge_max_dims = {}\n",
    "\n",
    "print(\"üîç Collecting features across all graphs...\\n\")\n",
    "\n",
    "for file in graph_files:\n",
    "    data = torch.load(os.path.join(input_dir, file))\n",
    "\n",
    "    for node_type in data.node_types:\n",
    "        arr = data[node_type].x.cpu().numpy()\n",
    "        node_type_features.setdefault(node_type, []).append(arr)\n",
    "        node_max_dims[node_type] = max(node_max_dims.get(node_type, 0), arr.shape[1])\n",
    "\n",
    "    for edge_type in data.edge_types:\n",
    "        arr = data[edge_type].edge_attr.cpu().numpy()\n",
    "        edge_type_features.setdefault(edge_type, []).append(arr)\n",
    "        edge_max_dims[edge_type] = max(edge_max_dims.get(edge_type, 0), arr.shape[1])\n",
    "\n",
    "# Pad to max dimensions\n",
    "combined_node_features = {}\n",
    "for k, v in node_type_features.items():\n",
    "    max_cols = node_max_dims[k]\n",
    "    padded = [np.pad(arr, ((0, 0), (0, max_cols - arr.shape[1])), mode='constant') for arr in v]\n",
    "    combined_node_features[k] = np.vstack(padded)\n",
    "\n",
    "combined_edge_features = {}\n",
    "for k, v in edge_type_features.items():\n",
    "    max_cols = edge_max_dims[k]\n",
    "    padded = [np.pad(arr, ((0, 0), (0, max_cols - arr.shape[1])), mode='constant') for arr in v]\n",
    "    combined_edge_features[k] = np.vstack(padded)\n",
    "\n",
    "# === Define log-transform rules ===\n",
    "def apply_log_transform(arr, label, onehot_skip=0):\n",
    "    transformed = arr.copy()\n",
    "    # Skip last one-hot columns if specified\n",
    "    continuous_part = transformed[:, :-onehot_skip] if onehot_skip > 0 else transformed\n",
    "\n",
    "    # Apply log1p to all continuous columns if pathway, involved_in, or expresses\n",
    "    if 'pathway' in label or 'involved_in' in label or 'expresses' in label:\n",
    "        continuous_part = np.log1p(continuous_part)\n",
    "    elif 'gene' in label and continuous_part.shape[1] > 1:\n",
    "        continuous_part[:, 1] = np.log1p(continuous_part[:, 1])  # Degree in pathways\n",
    "\n",
    "    if onehot_skip > 0:\n",
    "        transformed[:, :-onehot_skip] = continuous_part\n",
    "    else:\n",
    "        transformed = continuous_part\n",
    "\n",
    "    return transformed\n",
    "\n",
    "# === Fit shared scalers ===\n",
    "node_scalers = {}\n",
    "edge_scalers = {}\n",
    "\n",
    "for node_type, arr in combined_node_features.items():\n",
    "    if node_type == 'cell':\n",
    "        onehot_skip = 5  # last 5 columns are one-hot state\n",
    "        transformed = apply_log_transform(arr, node_type, onehot_skip)\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(transformed[:, :-onehot_skip])\n",
    "        node_scalers[node_type] = (scaler, onehot_skip)\n",
    "    else:\n",
    "        transformed = apply_log_transform(arr, node_type)\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(transformed)\n",
    "        node_scalers[node_type] = (scaler, 0)\n",
    "    print(f\"‚úÖ Fitted scaler for node type: {node_type}\")\n",
    "\n",
    "for edge_type, arr in combined_edge_features.items():\n",
    "    if 'co_expr' in edge_type:\n",
    "        print(f\"‚ö† Skipping scaler fit for co-expression edge type {edge_type} (will be removed)\")\n",
    "        continue\n",
    "    transformed = apply_log_transform(arr, edge_type)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(transformed)\n",
    "    edge_scalers[edge_type] = scaler\n",
    "    print(f\"‚úÖ Fitted scaler for edge type: {edge_type}\")\n",
    "\n",
    "# === Apply scalers and save ===\n",
    "for file in graph_files:\n",
    "    data = torch.load(os.path.join(input_dir, file))\n",
    "    print(f\"\\nüîÑ Processing graph: {file}\")\n",
    "\n",
    "    for node_type in data.node_types:\n",
    "        arr = data[node_type].x.cpu().numpy()\n",
    "        if arr.shape[1] < node_max_dims[node_type]:\n",
    "            arr = np.pad(arr, ((0, 0), (0, node_max_dims[node_type] - arr.shape[1])), mode='constant')\n",
    "\n",
    "        scaler, onehot_skip = node_scalers[node_type]\n",
    "        transformed = apply_log_transform(arr, node_type, onehot_skip)\n",
    "        scaled_continuous = scaler.transform(transformed[:, :-onehot_skip]) if onehot_skip > 0 else scaler.transform(transformed)\n",
    "        if onehot_skip > 0:\n",
    "            scaled_arr = np.hstack([scaled_continuous, arr[:, -onehot_skip:]])  # keep one-hot as-is\n",
    "        else:\n",
    "            scaled_arr = scaled_continuous\n",
    "\n",
    "        data[node_type].x = torch.tensor(scaled_arr, dtype=torch.float32)\n",
    "        print(f\"‚úÖ Scaled node features for {node_type}\")\n",
    "\n",
    "    for edge_type in list(data.edge_types):\n",
    "        if 'co_expr' in edge_type:\n",
    "            print(f\"üóë Removing co-expression edge type: {edge_type}\")\n",
    "            del data[edge_type]\n",
    "            continue\n",
    "\n",
    "        arr = data[edge_type].edge_attr.cpu().numpy()\n",
    "        if arr.shape[1] < edge_max_dims[edge_type]:\n",
    "            arr = np.pad(arr, ((0, 0), (0, edge_max_dims[edge_type] - arr.shape[1])), mode='constant')\n",
    "\n",
    "        transformed = apply_log_transform(arr, edge_type)\n",
    "        scaler = edge_scalers[edge_type]\n",
    "        scaled_arr = scaler.transform(transformed)\n",
    "\n",
    "        data[edge_type].edge_attr = torch.tensor(scaled_arr, dtype=torch.float32)\n",
    "        print(f\"‚úÖ Scaled edge features for {edge_type}\")\n",
    "\n",
    "    # === Remove isolated gene nodes ===\n",
    "    if 'gene' in data.node_types:\n",
    "        total_genes = data['gene'].num_nodes\n",
    "        gene_mask = torch.zeros(total_genes, dtype=torch.bool)\n",
    "\n",
    "        for edge_type in data.edge_types:\n",
    "            src_type, _, tgt_type = edge_type\n",
    "            edge_idx = data[edge_type].edge_index\n",
    "            if src_type == 'gene':\n",
    "                gene_mask[edge_idx[0]] = True\n",
    "            if tgt_type == 'gene':\n",
    "                gene_mask[edge_idx[1]] = True\n",
    "\n",
    "        if gene_mask.sum() < total_genes:\n",
    "            data['gene'].x = data['gene'].x[gene_mask]\n",
    "            print(f\"üßπ Removed isolated gene nodes: kept {gene_mask.sum().item()} of {total_genes}\")\n",
    "\n",
    "    output_path = os.path.join(output_dir, file)\n",
    "    torch.save(data, output_path)\n",
    "    print(f\"üíæ Saved cleaned and scaled graph to {output_path}\")\n",
    "\n",
    "print(\"\\nüéØ All graphs processed: features scaled, co-expression removed, isolated gene nodes cleaned!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da617eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Directory where final scaled graphs are saved\n",
    "scaled_dir = 'Graph_Results/HeteroGraphs_ScaledFinal'\n",
    "plot_dir = 'Graph_Results/Scaling_Plots_Final'\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "graph_files = [f for f in os.listdir(scaled_dir) if f.endswith('.pt')]\n",
    "\n",
    "# Collect all features\n",
    "node_type_features = {}\n",
    "edge_type_features = {}\n",
    "\n",
    "print(\"üîç Collecting scaled features for plotting...\\n\")\n",
    "\n",
    "for file in graph_files:\n",
    "    data = torch.load(os.path.join(scaled_dir, file))\n",
    "\n",
    "    for node_type in data.node_types:\n",
    "        arr = data[node_type].x.cpu().numpy()\n",
    "        node_type_features.setdefault(node_type, []).append(arr)\n",
    "\n",
    "    for edge_type in data.edge_types:\n",
    "        arr = data[edge_type].edge_attr.cpu().numpy()\n",
    "        edge_type_features.setdefault(edge_type, []).append(arr)\n",
    "\n",
    "combined_node_features = {k: np.vstack(v) for k, v in node_type_features.items()}\n",
    "combined_edge_features = {k: np.vstack(v) for k, v in edge_type_features.items()}\n",
    "\n",
    "# Plot function\n",
    "def plot_feature_distributions(data, label, out_dir):\n",
    "    num_features = data.shape[1]\n",
    "    for i in range(num_features):\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.hist(data[:, i], bins=50, alpha=0.7, color='green', density=True)\n",
    "        plt.title(f'{label} - Feature {i} (Scaled)')\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Density')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(out_dir, f'{label.replace(\" \", \"_\")}_Feature_{i}.png'))\n",
    "        plt.close()\n",
    "\n",
    "# Plot node features\n",
    "print(\"üìä Plotting node feature distributions...\")\n",
    "for node_type, arr in combined_node_features.items():\n",
    "    plot_feature_distributions(arr, f'NodeType_{node_type}', plot_dir)\n",
    "\n",
    "# Plot edge features\n",
    "print(\"üìä Plotting edge feature distributions...\")\n",
    "for edge_type, arr in combined_edge_features.items():\n",
    "    plot_feature_distributions(arr, f'EdgeType_{edge_type}', plot_dir)\n",
    "\n",
    "print(\"\\nüéØ All scaled feature distribution plots saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50839dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# Paths\n",
    "graph_path = 'Graph_Results/HeteroGraphs_ScaledFinal/HeteroGraph_T10.pt'\n",
    "mapping_path = 'Graph_Results/Graph_Mappings/Graph_Mapping_T10.json'\n",
    "\n",
    "# Load data\n",
    "data = torch.load(graph_path)\n",
    "with open(mapping_path, 'r') as f:\n",
    "    mapping = json.load(f)\n",
    "\n",
    "# Invert mappings\n",
    "gene_to_index = mapping['gene_to_index']\n",
    "cell_to_index = mapping['cell_to_index']\n",
    "pathway_to_index = mapping['pathway_to_index']\n",
    "index_to_gene = {v: k for k, v in gene_to_index.items()}\n",
    "index_to_cell = {v: k for k, v in cell_to_index.items()}\n",
    "index_to_pathway = {v: k for k, v in pathway_to_index.items()}\n",
    "\n",
    "# Function to display edge info\n",
    "def inspect_edge(edge_type, edge_idx=0):\n",
    "    edge_index = data[edge_type].edge_index\n",
    "    edge_attr = data[edge_type].edge_attr if 'edge_attr' in data[edge_type] else None\n",
    "\n",
    "    src_idx = edge_index[0, edge_idx].item()\n",
    "    tgt_idx = edge_index[1, edge_idx].item()\n",
    "    weight = edge_attr[edge_idx].item() if edge_attr is not None else 'No weight'\n",
    "\n",
    "    src_type, _, tgt_type = edge_type\n",
    "\n",
    "    # Resolve names\n",
    "    if src_type == 'gene':\n",
    "        src_name = index_to_gene.get(src_idx, 'UNKNOWN')\n",
    "    elif src_type == 'cell':\n",
    "        src_name = index_to_cell.get(src_idx, 'UNKNOWN')\n",
    "    elif src_type == 'pathway':\n",
    "        src_name = index_to_pathway.get(src_idx, 'UNKNOWN')\n",
    "    else:\n",
    "        src_name = f'Node {src_idx}'\n",
    "\n",
    "    if tgt_type == 'gene':\n",
    "        tgt_name = index_to_gene.get(tgt_idx, 'UNKNOWN')\n",
    "    elif tgt_type == 'cell':\n",
    "        tgt_name = index_to_cell.get(tgt_idx, 'UNKNOWN')\n",
    "    elif tgt_type == 'pathway':\n",
    "        tgt_name = index_to_pathway.get(tgt_idx, 'UNKNOWN')\n",
    "    else:\n",
    "        tgt_name = f'Node {tgt_idx}'\n",
    "\n",
    "    print(f\"\\nüîó Edge type: {edge_type}\")\n",
    "    print(f\"   Source index: {src_idx} ‚Üí {src_name}\")\n",
    "    print(f\"   Target index: {tgt_idx} ‚Üí {tgt_name}\")\n",
    "    print(f\"   Edge weight: {weight}\")\n",
    "\n",
    "# Inspect a few example edges\n",
    "for etype in data.edge_types:\n",
    "    inspect_edge(etype, edge_idx=0)  # first edge\n",
    "    if data[etype].edge_index.size(1) > 1:\n",
    "        inspect_edge(etype, edge_idx=1)  # second edge\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "pyg_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
