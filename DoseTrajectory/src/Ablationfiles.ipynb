{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8404bc05",
   "metadata": {},
   "source": [
    "### NO virtual NOde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796dd843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import NNConv, global_mean_pool \n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GlobalAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd562f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the hetero graph\n",
    "data = torch.load(\"Graph_Results/HeteroGraphs_ScaledFinal/HeteroGraph_T1.pt\")\n",
    "\n",
    "# Print node types and sizes\n",
    "print(\"Node Types and Features:\")\n",
    "for ntype in data.node_types:\n",
    "    print(f\"  {ntype}: {data[ntype].x.shape}\")\n",
    "\n",
    "# Print edge types and count\n",
    "print(\"\\nEdge Types:\")\n",
    "for etype in data.edge_types:\n",
    "    edge_index = data[etype].edge_index\n",
    "    print(f\"  {etype}: {edge_index.shape[1]} edges\")\n",
    "\n",
    "# Check a few values from cell node features\n",
    "print(\"\\nSample cell node features (first 5 rows):\")\n",
    "print(data[\"cell\"].x[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a79293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NodeFeatureEncoders(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cell_encoder = nn.Sequential(\n",
    "            nn.Linear(7, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),  # ğŸ” Replaced BatchNorm1d\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.gene_encoder = nn.Sequential(\n",
    "            nn.Linear(2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),  # ğŸ” Replaced BatchNorm1d\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.pathway_encoder = nn.Sequential(\n",
    "            nn.Linear(2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),  # ğŸ” Replaced BatchNorm1d\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, cell_x, gene_x, pathway_x):\n",
    "        h_cell = self.cell_encoder(cell_x)\n",
    "        h_gene = self.gene_encoder(gene_x)\n",
    "        h_pathway = self.pathway_encoder(pathway_x)\n",
    "        return h_cell, h_gene, h_pathway\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065e0c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GlobalAttention\n",
    "\n",
    "class GlobalAttentionWithWeights(GlobalAttention):\n",
    "    def forward(self, x, index, ptr=None, dim_size=None, dim=0):\n",
    "        \"\"\"\n",
    "        x: Node embeddings\n",
    "        index: Index tensor (typically the batch vector)\n",
    "        \"\"\"\n",
    "        gate = self.gate_nn(x).squeeze(-1)      # [N]\n",
    "        gate = torch.sigmoid(gate)              # attention weights\n",
    "        x_weighted = x * gate.unsqueeze(-1)     # [N, F]\n",
    "\n",
    "        # Perform aggregation (mean by default)\n",
    "        out = torch.zeros(dim_size or int(index.max()) + 1, x.size(-1), device=x.device)\n",
    "        out = out.index_add(dim, index, x_weighted)\n",
    "\n",
    "        return out, gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b278a858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import NNConv\n",
    "\n",
    "class SharedHierarchicalEncoder_NoVirtual(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, num_aux_outputs=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.node_encoders = NodeFeatureEncoders(hidden_dim)\n",
    "\n",
    "        # Removed: dosage_embeddings, virtual_norm, dosage_lstm, fuse_*_virtual\n",
    "\n",
    "        self.edge_mlp_cell_gene = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim * hidden_dim)\n",
    "        )\n",
    "        self.edge_mlp_gene_pathway = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim * hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.cell_to_gene_conv = NNConv(hidden_dim, hidden_dim, self.edge_mlp_cell_gene, aggr='mean')\n",
    "        self.gene_to_pathway_conv = NNConv(hidden_dim, hidden_dim, self.edge_mlp_gene_pathway, aggr='mean')\n",
    "\n",
    "        self.att_pool = GlobalAttentionWithWeights(gate_nn=nn.Linear(hidden_dim, 1))\n",
    "\n",
    "        self.fuse_global = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.aux_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_aux_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, data, dosage_idx=None):  # dosage_idx kept for compatibility but unused\n",
    "        cell_x, gene_x, pathway_x = data[\"cell\"].x, data[\"gene\"].x, data[\"pathway\"].x\n",
    "        h_cell, h_gene, h_pathway = self.node_encoders(cell_x, gene_x, pathway_x)\n",
    "\n",
    "        edge_index_cg = data[\"cell\", \"expresses\", \"gene\"].edge_index\n",
    "        edge_attr_cg = data[\"cell\", \"expresses\", \"gene\"].edge_attr\n",
    "        h_gene_updated = self.cell_to_gene_conv((h_cell, h_gene), edge_index_cg, edge_attr_cg)\n",
    "\n",
    "        edge_index_gp = data[\"gene\", \"involved_in\", \"pathway\"].edge_index\n",
    "        edge_attr_gp = data[\"gene\", \"involved_in\", \"pathway\"].edge_attr\n",
    "        h_pathway_updated = self.gene_to_pathway_conv((h_gene_updated, h_pathway), edge_index_gp, edge_attr_gp)\n",
    "\n",
    "        pooled_pathway, pathway_attention_weights = self.att_pool(h_pathway_updated, data['pathway'].batch)\n",
    "\n",
    "        graph_embedding = self.fuse_global(pooled_pathway)\n",
    "\n",
    "        aux_output = self.aux_head(graph_embedding)\n",
    "\n",
    "        # Normalize outputs\n",
    "        h_cell = F.normalize(h_cell, p=2, dim=-1)\n",
    "        h_gene_updated = F.normalize(h_gene_updated, p=2, dim=-1)\n",
    "        h_pathway_updated = F.normalize(h_pathway_updated, p=2, dim=-1)\n",
    "\n",
    "        return {\n",
    "            \"h_cell\": h_cell,\n",
    "            \"h_gene\": h_gene_updated,\n",
    "            \"h_pathway\": h_pathway_updated,\n",
    "            \"graph_embedding\": graph_embedding,\n",
    "            \"aux_output\": aux_output.squeeze(),\n",
    "            \"pathway_attention_weights\": pathway_attention_weights\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378b464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class HierarchicalDecoder_NoVirtual(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, cell_feature_dim=7):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.cell_feature_dim = cell_feature_dim\n",
    "\n",
    "        self.gene_query_bias = nn.Parameter(torch.randn(1, hidden_dim))\n",
    "        self.cell_query_bias = nn.Parameter(torch.randn(1, hidden_dim))\n",
    "\n",
    "        self.decode_to_pathways_fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "        )\n",
    "        self.decode_to_pathways_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.pathway_to_gene_attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=2,\n",
    "            dropout=0.33069384874657237,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.gene_to_cell_attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=2,\n",
    "            dropout=0.33069384874657237,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.decode_to_cells = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, cell_feature_dim)\n",
    "        )\n",
    "\n",
    "        self.aux_pathway_score_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "        self.aux_resistance_predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 5)  # ğŸ” from 1 â†’ 5\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, h_pathway_updated, h_gene_updated, graph_embedding,\n",
    "                num_genes, num_cells, gene_mask=None, cell_mask=None):\n",
    "\n",
    "        num_pathways, _ = h_pathway_updated.size()\n",
    "        B = 1\n",
    "\n",
    "        if graph_embedding.dim() == 3:\n",
    "            graph_embedding = graph_embedding.squeeze(1)  # [1, 1, 64] â†’ [1, 64]\n",
    "\n",
    "        graph_expanded = graph_embedding.unsqueeze(1).expand(B, num_pathways, -1)\n",
    "        h_pathway_expanded = h_pathway_updated.unsqueeze(0)  # [1, P, 64]\n",
    "\n",
    "        combined_input = torch.cat([h_pathway_expanded, graph_expanded], dim=2)\n",
    "        base_pathway_repr = self.decode_to_pathways_fc(combined_input)\n",
    "        pathway_recon = self.decode_to_pathways_proj(base_pathway_repr)\n",
    "\n",
    "        gene_seed = h_pathway_updated.mean(dim=0)  # [64]\n",
    "        gene_query = gene_seed.unsqueeze(0).unsqueeze(1).expand(B, num_genes, -1) + self.gene_query_bias\n",
    "\n",
    "        gene_recon_raw, attn_pathway_gene = self.pathway_to_gene_attn(\n",
    "            gene_query, pathway_recon, pathway_recon, key_padding_mask=gene_mask\n",
    "        )\n",
    "        gene_recon_raw = self.dropout(gene_recon_raw)\n",
    "        gene_recon = gene_recon_raw + gene_query\n",
    "\n",
    "        cell_seed = h_gene_updated.mean(dim=0)\n",
    "        cell_query = cell_seed.unsqueeze(0).unsqueeze(1).expand(B, num_cells, -1) + self.cell_query_bias\n",
    "\n",
    "        cell_recon_raw, attn_gene_cell = self.gene_to_cell_attn(\n",
    "            cell_query, gene_recon, gene_recon, key_padding_mask=cell_mask\n",
    "        )\n",
    "        cell_recon_raw = self.dropout(cell_recon_raw)\n",
    "        cell_embedding_for_resistance = cell_recon_raw + cell_query\n",
    "        cell_recon = self.decode_to_cells(cell_embedding_for_resistance)\n",
    "        \n",
    "        aux_pathway_scores = self.aux_pathway_score_head(pathway_recon).squeeze(-1)\n",
    "        aux_resistance_score = self.aux_resistance_predictor(cell_embedding_for_resistance).squeeze(-1)\n",
    "\n",
    "        return {\n",
    "            \"reconstructed_pathways\": pathway_recon.squeeze(0),\n",
    "            \"reconstructed_genes\": gene_recon.squeeze(0),\n",
    "            \"reconstructed_cells\": cell_recon.squeeze(0),\n",
    "            \"aux_pathway_scores\": aux_pathway_scores.squeeze(0),\n",
    "            \"aux_resistance_score\": aux_resistance_score,\n",
    "            \"attention_pathway_to_gene\": attn_pathway_gene,\n",
    "            \"attention_gene_to_cell\": attn_gene_cell\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d77b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "# Load one example graph (change path if needed)\n",
    "graph_path = 'Graph_Results/HeteroGraphs_ScaledFinal/HeteroGraph_T1.pt'\n",
    "data = torch.load(graph_path)\n",
    "\n",
    "print(f\"Inspecting graph at: {graph_path}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Node types\n",
    "print(\"\\nğŸ“¦ Node Types and Feature Shapes:\")\n",
    "for node_type in data.node_types:\n",
    "    x = data[node_type].x\n",
    "    print(f\"- {node_type}: {x.shape} features, dtype: {x.dtype}\")\n",
    "    if hasattr(data[node_type], 'batch'):\n",
    "        print(f\"  Batch attribute: {data[node_type].batch.shape}\")\n",
    "    print(f\"  Feature stats: min {x.min().item()}, max {x.max().item()}, mean {x.mean().item()}\")\n",
    "\n",
    "# Edge types\n",
    "print(\"\\nğŸ”— Edge Types and Attributes:\")\n",
    "for edge_type in data.edge_types:\n",
    "    edge_index = data[edge_type].edge_index\n",
    "    edge_attr = data[edge_type].edge_attr\n",
    "    print(f\"- {edge_type}: {edge_index.shape[1]} edges, edge_attr shape: {edge_attr.shape}\")\n",
    "    print(f\"  Edge attr stats: min {edge_attr.min().item()}, max {edge_attr.max().item()}, mean {edge_attr.mean().item()}\")\n",
    "\n",
    "# Summary counts\n",
    "print(\"\\nğŸ“Š Summary:\")\n",
    "print(f\"- Total node types: {len(data.node_types)}\")\n",
    "print(f\"- Total edge types: {len(data.edge_types)}\")\n",
    "total_nodes = sum(data[node_type].num_nodes for node_type in data.node_types)\n",
    "print(f\"- Total nodes: {total_nodes}\")\n",
    "total_edges = sum(data[edge_type].edge_index.shape[1] for edge_type in data.edge_types)\n",
    "print(f\"- Total edges: {total_edges}\")\n",
    "\n",
    "# Optional: inspect one example feature vector\n",
    "for node_type in data.node_types:\n",
    "    print(f\"\\nExample {node_type} feature vector (first node):\")\n",
    "    print(data[node_type].x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5963a41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HierarchicalLoss(nn.Module):\n",
    "    def __init__(self, \n",
    "                  lambda_pathway=2.0,     # ğŸ”º Highest priority\n",
    "                 lambda_gene=1.0,        # ğŸ”¸ Medium priority\n",
    "                 lambda_cell=0.5,        # ğŸ”» Lowest priority\n",
    "                 lambda_resistance=0.1, \n",
    "                 lambda_attention=0.01,\n",
    "                 use_resistance=True,\n",
    "                 use_attention_reg=True):\n",
    "        super().__init__()\n",
    "        self.lambda_pathway = lambda_pathway\n",
    "        self.lambda_gene = lambda_gene\n",
    "        self.lambda_cell = lambda_cell\n",
    "        self.lambda_resistance = lambda_resistance\n",
    "        self.lambda_attention = lambda_attention\n",
    "        self.use_resistance = use_resistance\n",
    "        self.use_attention_reg = use_attention_reg\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        outputs: dict from decoder\n",
    "        targets: dict from encoder and ground truth features:\n",
    "            - h_pathway\n",
    "            - h_gene\n",
    "            - cell_features\n",
    "            - resistance_label (optional)\n",
    "        \"\"\"\n",
    "\n",
    "        # 1ï¸âƒ£ Pathway-level reconstruction\n",
    "        L_pathway = F.mse_loss(outputs[\"reconstructed_pathways\"], targets[\"h_pathway\"])\n",
    "\n",
    "        # 2ï¸âƒ£ Gene-level reconstruction\n",
    "        L_gene = F.mse_loss(outputs[\"reconstructed_genes\"], targets[\"h_gene\"])\n",
    "\n",
    "        # 3ï¸âƒ£ Cell-level reconstruction\n",
    "        L_cell = F.mse_loss(outputs[\"reconstructed_cells\"], targets[\"cell_features\"])\n",
    "\n",
    "        # 4ï¸âƒ£ Resistance prediction (if available)\n",
    "        # 4ï¸âƒ£ Resistance prediction (if available)\n",
    "        if self.use_resistance and \"resistance_label\" in targets:\n",
    "            L_resistance = F.mse_loss(outputs[\"aux_resistance_score\"].squeeze(0), targets[\"resistance_label\"])\n",
    "        else:\n",
    "            L_resistance = torch.tensor(0.0, device=L_cell.device)\n",
    "\n",
    "\n",
    "        # 5ï¸âƒ£ Attention entropy regularization (optional)\n",
    "        if self.use_attention_reg:\n",
    "            attn_weights = outputs[\"attention_pathway_to_gene\"]\n",
    "            attn_weights = torch.clamp(attn_weights, min=1e-6)\n",
    "            L_attention = -(attn_weights * torch.log(attn_weights)).sum(dim=-1).mean()\n",
    "        else:\n",
    "            L_attention = torch.tensor(0.0, device=L_cell.device)\n",
    "\n",
    "        # âœ… Total Loss\n",
    "        total_loss = (\n",
    "            self.lambda_pathway * L_pathway +\n",
    "            self.lambda_gene * L_gene +\n",
    "            self.lambda_cell * L_cell +\n",
    "            self.lambda_resistance * L_resistance +\n",
    "            self.lambda_attention * L_attention\n",
    "        )\n",
    "\n",
    "        # ğŸ” Log individual loss components\n",
    "        loss_dict = {\n",
    "            \"total_loss\": total_loss.item(),\n",
    "            \"pathway_loss\": L_pathway.item(),\n",
    "            \"gene_loss\": L_gene.item(),\n",
    "            \"cell_loss\": L_cell.item(),\n",
    "            \"resistance_loss\": L_resistance.item(),\n",
    "            \"attention_reg_loss\": L_attention.item()\n",
    "        }\n",
    "\n",
    "        return total_loss, loss_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27465050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def load_all_dosage_graphs(graph_dir, pattern_prefix=\"HeteroGraph_T\"):\n",
    "    \"\"\"\n",
    "    Loads all dosage-specific heterographs from the specified directory.\n",
    "\n",
    "    Args:\n",
    "        graph_dir (str): Path to directory containing dosage graphs.\n",
    "        pattern_prefix (str): File prefix to identify graph files (e.g., \"HeteroGraph_T\").\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary { \"T1\": data_obj, \"T10\": data_obj, ... }\n",
    "    \"\"\"\n",
    "    graphs = {}\n",
    "    for fname in os.listdir(graph_dir):\n",
    "        if fname.startswith(pattern_prefix) and fname.endswith(\".pt\"):\n",
    "            dosage_key = fname.replace(\".pt\", \"\").replace(pattern_prefix, \"T\")\n",
    "            path = os.path.join(graph_dir, fname)\n",
    "            data = torch.load(path)\n",
    "            graphs[dosage_key] = data\n",
    "    return graphs\n",
    "graph_dir = \"Graph_Results/HeteroGraphs_ScaledFinal\"\n",
    "graphs = load_all_dosage_graphs(graph_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42733b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# === Load saved manual 5-fold file ===\n",
    "with open('cv_folds_manual_5fold.json', 'r') as f:\n",
    "    fold_data = json.load(f)\n",
    "\n",
    "# === These are dosage names (e.g., 'T1', 'T2.5', etc.)\n",
    "dosage_keys = fold_data[\"dosage_keys\"]\n",
    "\n",
    "# === Folds already contain dosage strings, not indices\n",
    "folds = fold_data[\"folds\"]\n",
    "\n",
    "# âœ… Use Fold 1 (index 0)\n",
    "fold_idx = 0\n",
    "train_keys, val_keys = folds[fold_idx]  # No need to index into dosage_keys\n",
    "\n",
    "# === Retrieve graph data for training and validation\n",
    "train_graphs = {k: graphs[k] for k in train_keys}\n",
    "val_graphs = {k: graphs[k] for k in val_keys}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4761d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_encoder_decoder_model(\n",
    "    encoder, decoder, graphs, dosage_to_idx, optimizer=None, device='cuda',\n",
    "    epochs=100, loss_weights=None, save_path=None\n",
    "):\n",
    "    if loss_weights is None:\n",
    "        loss_weights = {\n",
    "            'lambda_pathway': 2.433302836237277,\n",
    "            'lambda_gene': 1.5656833777217563,\n",
    "            'lambda_cell': 0.09616027936016658,\n",
    "            'lambda_resistance': 0.31864704926747495,\n",
    "            'lambda_attention': 0.06633055464524576\n",
    "        }\n",
    "\n",
    "    encoder = encoder.to(device)\n",
    "    decoder = decoder.to(device)\n",
    "\n",
    "    criterion = HierarchicalLoss(**loss_weights).to(device)\n",
    "\n",
    "    if optimizer is None:\n",
    "        optimizer = Adam(\n",
    "            list(encoder.parameters()) + list(decoder.parameters()),\n",
    "            lr=1e-3,\n",
    "            weight_decay=1e-5\n",
    "        )\n",
    "\n",
    "    # ğŸ” Learning Rate Scheduler and Early Stopping\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True, min_lr=1e-5)\n",
    "    best_monitored_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    early_stop_patience = 10  # ğŸ›‘ stop if no improvement in 10 epochs\n",
    "\n",
    "    loss_log = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        epoch_loss_dict = {\n",
    "            'pathway_loss': 0.0,\n",
    "            'gene_loss': 0.0,\n",
    "            'cell_loss': 0.0,\n",
    "            'resistance_loss': 0.0,\n",
    "            'attention_reg_loss': 0.0,\n",
    "            'total_loss': 0.0\n",
    "        }\n",
    "\n",
    "        for dosage_name, graph in graphs.items():\n",
    "            data = graph.to(device)\n",
    "            dosage_idx = dosage_to_idx[dosage_name]\n",
    "\n",
    "            if not hasattr(data['pathway'], 'batch'):\n",
    "                data['pathway'].batch = torch.zeros(data['pathway'].num_nodes, dtype=torch.long, device=device)\n",
    "\n",
    "            encoder_out = encoder(data, dosage_idx)\n",
    "\n",
    "            # âŒ No dosage_virtual passed to decoder\n",
    "            decoder_out = decoder(\n",
    "                h_pathway_updated=encoder_out['h_pathway'],\n",
    "                h_gene_updated=encoder_out['h_gene'],\n",
    "                graph_embedding=encoder_out['graph_embedding'].unsqueeze(0),\n",
    "                num_genes=encoder_out['h_gene'].shape[0],\n",
    "                num_cells=encoder_out['h_cell'].shape[0]\n",
    "            )\n",
    "\n",
    "            targets = {\n",
    "                'h_pathway': encoder_out['h_pathway'].detach(),\n",
    "                'h_gene': encoder_out['h_gene'].detach(),\n",
    "                'cell_features': data['cell'].x,\n",
    "                'resistance_label': data['cell'].x[:, -5:]  # One-hot encoded state\n",
    "            }\n",
    "\n",
    "            loss, loss_components = criterion(decoder_out, targets)\n",
    "            for key in epoch_loss_dict:\n",
    "                epoch_loss_dict[key] += loss_components[key]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Average losses\n",
    "        num_graphs = len(graphs)\n",
    "        avg_loss_dict = {k: v / num_graphs for k, v in epoch_loss_dict.items()}\n",
    "        loss_log.append(avg_loss_dict)\n",
    "\n",
    "        # Monitor: weighted sum of pathway + gene loss\n",
    "        monitored = 0.7 * avg_loss_dict['pathway_loss'] + 0.3 * avg_loss_dict['gene_loss']\n",
    "        scheduler.step(monitored)\n",
    "\n",
    "        # Early stopping check\n",
    "        if monitored < best_monitored_loss - 1e-4:\n",
    "            best_monitored_loss = monitored\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | \"\n",
    "              f\"Total: {avg_loss_dict['total_loss']:.4f} | \"\n",
    "              f\"P: {avg_loss_dict['pathway_loss']:.4f}, \"\n",
    "              f\"G: {avg_loss_dict['gene_loss']:.4f}, \"\n",
    "              f\"C: {avg_loss_dict['cell_loss']:.4f}, \"\n",
    "              f\"R: {avg_loss_dict['resistance_loss']:.4f}, \"\n",
    "              f\"A: {avg_loss_dict['attention_reg_loss']:.4f}\")\n",
    "\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f\"ğŸ›‘ Early stopping at epoch {epoch+1} â€” no improvement in {early_stop_patience} epochs.\")\n",
    "            break\n",
    "\n",
    "    # âœ… Save model + loss log\n",
    "    if save_path:\n",
    "        torch.save({\n",
    "            'encoder': encoder.state_dict(),\n",
    "            'decoder': decoder.state_dict()\n",
    "        }, save_path)\n",
    "        print(f\"âœ… Model saved to {save_path}\")\n",
    "\n",
    "        loss_log_path = save_path.replace('.pth', '_loss_log.json')\n",
    "        with open(loss_log_path, 'w') as f:\n",
    "            json.dump(loss_log, f, indent=2)\n",
    "        print(f\"ğŸ“‰ Loss log saved to {loss_log_path}\")\n",
    "\n",
    "    return encoder, decoder, loss_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ebfa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load graphs\n",
    "graph_dir = \"Graph_Results/HeteroGraphs_ScaledFinal\"\n",
    "graphs = load_all_dosage_graphs(graph_dir)\n",
    "\n",
    "# ğŸ” Map 'T2.5', 'T10', etc. to integer indices\n",
    "dosage_levels = sorted([float(k.replace('T', '')) for k in graphs.keys()])\n",
    "dosage_to_idx = {f\"T{int(d) if d.is_integer() else d}\": i for i, d in enumerate(dosage_levels)}\n",
    "\n",
    "# Load model WITHOUT virtual node\n",
    "encoder = SharedHierarchicalEncoder_NoVirtual(hidden_dim=64)\n",
    "decoder = HierarchicalDecoder_NoVirtual(hidden_dim=64, cell_feature_dim=7)\n",
    "\n",
    "# Train with early stopping and scheduler already integrated\n",
    "trained_encoder, trained_decoder, loss_log = train_encoder_decoder_model(\n",
    "    encoder, decoder,\n",
    "    graphs=train_graphs,  # âœ… only Fold 1 training data\n",
    "    dosage_to_idx=dosage_to_idx,\n",
    "    device='cpu',\n",
    "    epochs=100,\n",
    "    save_path=\"trained_model_no_virtual_fold1.pth\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f479a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model without virtual nodes\n",
    "encoder = SharedHierarchicalEncoder_NoVirtual(hidden_dim=64)\n",
    "decoder = HierarchicalDecoder_NoVirtual(hidden_dim=64, cell_feature_dim=7)\n",
    "\n",
    "# Load checkpoint for no-virtual version\n",
    "checkpoint = torch.load(\"trained_model_no_virtual_fold1.pth\", map_location='cpu')\n",
    "encoder.load_state_dict(checkpoint['encoder'])\n",
    "decoder.load_state_dict(checkpoint['decoder'])\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf2605",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dir = \"Graph_Results/HeteroGraphs_ScaledFinal\"\n",
    "graphs = load_all_dosage_graphs(graph_dir)\n",
    "\n",
    "# Also ensure you have dosage_to_idx mapping\n",
    "dosage_to_idx = {k: i for i, k in enumerate(sorted(graphs.keys(), key=lambda x: float(x[1:])))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f30e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs = {}\n",
    "device = torch.device('cpu')\n",
    "\n",
    "for dosage_name, data in graphs.items():\n",
    "    data = data.to(device)\n",
    "    dosage_idx = dosage_to_idx[dosage_name]\n",
    "\n",
    "    # âœ… Inject dummy batch for pathway\n",
    "    if not hasattr(data['pathway'], 'batch'):\n",
    "        data['pathway'].batch = torch.zeros(\n",
    "            data['pathway'].num_nodes,\n",
    "            dtype=torch.long,\n",
    "            device=data['pathway'].x.device\n",
    "        )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_out = encoder(data, dosage_idx)\n",
    "\n",
    "        decoder_out = decoder(\n",
    "            h_pathway_updated=encoder_out['h_pathway'],\n",
    "            h_gene_updated=encoder_out['h_gene'],\n",
    "            graph_embedding=encoder_out['graph_embedding'].unsqueeze(0).squeeze(1),\n",
    "            # âŒ dosage_virtual removed\n",
    "            num_genes=encoder_out['h_gene'].shape[0],\n",
    "            num_cells=encoder_out['h_cell'].shape[0]\n",
    "        )\n",
    "\n",
    "    all_outputs[dosage_name] = {\n",
    "        \"encoder\": encoder_out,\n",
    "        \"decoder\": decoder_out\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5febe39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ“ Required: all_outputs must already be created\n",
    "# ğŸ“ Graph mapping directory must be present\n",
    "graph_mapping_dir = \"Graph_Results/Graph_Mappings\"\n",
    "embedding_records = []\n",
    "\n",
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "# ğŸ” Step 1: Extract per-dosage gene & pathway embeddings\n",
    "for dosage_name, output in all_outputs.items():\n",
    "    mapping_path = os.path.join(graph_mapping_dir, f\"Graph_Mapping_{dosage_name}.json\")\n",
    "    if not os.path.exists(mapping_path):\n",
    "        continue\n",
    "\n",
    "    with open(mapping_path, 'r') as f:\n",
    "        mapping = json.load(f)\n",
    "\n",
    "    h_gene = output[\"encoder\"][\"h_gene\"]\n",
    "    h_pathway = output[\"encoder\"][\"h_pathway\"]\n",
    "\n",
    "    for gene, idx in mapping[\"gene_to_index\"].items():\n",
    "        embedding_records.append({\n",
    "            \"type\": \"gene\",\n",
    "            \"name\": gene,\n",
    "            \"dosage\": dosage_name,\n",
    "            \"embedding\": h_gene[idx].tolist()\n",
    "        })\n",
    "\n",
    "    for pathway, idx in mapping[\"pathway_to_index\"].items():\n",
    "        embedding_records.append({\n",
    "            \"type\": \"pathway\",\n",
    "            \"name\": pathway,\n",
    "            \"dosage\": dosage_name,\n",
    "            \"embedding\": h_pathway[idx].tolist()\n",
    "        })\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ“¦ Step 2: Format embeddings for t-SNE\n",
    "df_embed = pd.DataFrame(embedding_records)\n",
    "embedding_matrix = np.vstack(df_embed[\"embedding\"].values)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, init='pca')\n",
    "tsne_result = tsne.fit_transform(embedding_matrix)\n",
    "\n",
    "df_embed[\"tsne-1\"] = tsne_result[:, 0]\n",
    "df_embed[\"tsne-2\"] = tsne_result[:, 1]\n",
    "\n",
    "# ğŸ’¾ Step 3: Save to CSV\n",
    "df_embed.to_csv(\"tSNE_Embeddings_All_no_virtual.csv\", index=False)\n",
    "print(\"âœ… Embedding CSV saved: tSNE_Embeddings_All_no_virtual.csv\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ¨ Step 4: Plot gene embeddings\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=df_embed[df_embed[\"type\"] == \"gene\"],\n",
    "    x=\"tsne-1\", y=\"tsne-2\", hue=\"dosage\", palette=\"tab10\"\n",
    ")\n",
    "plt.title(\"t-SNE: Gene Embeddings (No Virtual Node Model)\", fontsize=16)\n",
    "plt.xlabel(\"tSNE-1\", fontsize=14)\n",
    "plt.ylabel(\"tSNE-2\", fontsize=14)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Dosage\", fontsize=10, title_fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tSNE_Gene_Embeddings_no_virtual.png\", dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ¨ Step 5: Plot pathway embeddings\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=df_embed[df_embed[\"type\"] == \"pathway\"],\n",
    "    x=\"tsne-1\", y=\"tsne-2\", hue=\"dosage\", palette=\"tab10\"\n",
    ")\n",
    "plt.title(\"t-SNE: Pathway Embeddings (No Virtual Node Model)\", fontsize=16)\n",
    "plt.xlabel(\"tSNE-1\", fontsize=14)\n",
    "plt.ylabel(\"tSNE-2\", fontsize=14)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Dosage\", fontsize=10, title_fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tSNE_Pathway_Embeddings_no_virtual.png\", dpi=600, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04dbf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Load mappings and extract embeddings by dosage\n",
    "graph_mapping_dir = \"Graph_Results/Graph_Mappings\"\n",
    "gene_embeddings_by_dosage = {}  # {dosage_name: {gene_name: embedding}}\n",
    "pathway_embeddings_by_dosage = {}\n",
    "\n",
    "for dosage_name, output in all_outputs.items():\n",
    "    mapping_path = os.path.join(graph_mapping_dir, f\"Graph_Mapping_{dosage_name}.json\")\n",
    "    if not os.path.exists(mapping_path):\n",
    "        continue\n",
    "    with open(mapping_path, 'r') as f:\n",
    "        mapping = json.load(f)\n",
    "\n",
    "    h_gene = output[\"encoder\"][\"h_gene\"]\n",
    "    h_pathway = output[\"encoder\"][\"h_pathway\"]\n",
    "\n",
    "    gene_embeddings_by_dosage[dosage_name] = {\n",
    "        gene: h_gene[idx].cpu().numpy()\n",
    "        for gene, idx in mapping[\"gene_to_index\"].items()\n",
    "    }\n",
    "\n",
    "    pathway_embeddings_by_dosage[dosage_name] = {\n",
    "        pathway: h_pathway[idx].cpu().numpy()\n",
    "        for pathway, idx in mapping[\"pathway_to_index\"].items()\n",
    "    }\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Function to compute average cosine drift\n",
    "def compute_avg_cosine_drift(embedding_dict):\n",
    "    sorted_dosages = sorted(embedding_dict.keys(), key=lambda d: float(d[1:]))\n",
    "    drift_values = []\n",
    "\n",
    "    # Transpose: {entity â†’ list of embeddings across dosages}\n",
    "    all_keys = set.intersection(*[set(embedding_dict[d].keys()) for d in sorted_dosages])\n",
    "    for key in all_keys:\n",
    "        embeddings = [embedding_dict[d][key] for d in sorted_dosages]\n",
    "        for i in range(len(embeddings) - 1):\n",
    "            vec1 = embeddings[i].reshape(1, -1)\n",
    "            vec2 = embeddings[i+1].reshape(1, -1)\n",
    "            cos_sim = cosine_similarity(vec1, vec2)[0][0]\n",
    "            drift = 1 - cos_sim\n",
    "            drift_values.append(drift)\n",
    "\n",
    "    avg_drift = np.mean(drift_values)\n",
    "    return avg_drift\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  Compute average drifts\n",
    "avg_gene_drift = compute_avg_cosine_drift(gene_embeddings_by_dosage)\n",
    "avg_pathway_drift = compute_avg_cosine_drift(pathway_embeddings_by_dosage)\n",
    "\n",
    "print(f\"ğŸ“ˆ Average Cosine Drift (Gene): {avg_gene_drift:.4f}\")\n",
    "print(f\"ğŸ“Š Average Cosine Drift (Pathway): {avg_pathway_drift:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafca30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained no-virtual encoder\n",
    "encoder = SharedHierarchicalEncoder_NoVirtual(hidden_dim=64)  # ğŸ” Use NoVirtual version\n",
    "checkpoint = torch.load(\"trained_model_no_virtual_fold1.pth\", map_location='cpu')\n",
    "encoder.load_state_dict(checkpoint['encoder'])\n",
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a666a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def train_lstm_on_graph_embeddings(\n",
    "    encoder, graphs, dosage_to_idx,\n",
    "    lstm_model, target_dosage='T320',\n",
    "    past_dosages=['T1', 'T2.5', 'T5', 'T10', 'T20', 'T40', 'T80', 'T160'],\n",
    "    device='cpu', epochs=100, lr=1e-3, patience=10\n",
    "):\n",
    "    encoder = encoder.to(device)\n",
    "    lstm_model = lstm_model.to(device)\n",
    "    encoder.eval()\n",
    "\n",
    "    optimizer = Adam(lstm_model.parameters(), lr=lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-5, verbose=True)\n",
    "\n",
    "    loss_log = []\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # ğŸ§  True embedding\n",
    "    target_data = graphs[target_dosage].to(device)\n",
    "    if not hasattr(target_data['pathway'], 'batch'):\n",
    "        target_data['pathway'].batch = torch.zeros(target_data['pathway'].num_nodes, dtype=torch.long, device=device)\n",
    "    with torch.no_grad():\n",
    "        true_embedding = encoder(target_data, dosage_idx=dosage_to_idx[target_dosage])['graph_embedding'].detach()\n",
    "\n",
    "    # ğŸ§  Input sequence\n",
    "    past_embeddings = []\n",
    "    for d in past_dosages:\n",
    "        data = graphs[d].to(device)\n",
    "        if not hasattr(data['pathway'], 'batch'):\n",
    "            data['pathway'].batch = torch.zeros(data['pathway'].num_nodes, dtype=torch.long, device=device)\n",
    "        with torch.no_grad():\n",
    "            emb = encoder(data, dosage_idx=dosage_to_idx[d])['graph_embedding'].detach()\n",
    "        past_embeddings.append(emb.squeeze(0))\n",
    "\n",
    "    input_sequence = torch.stack(past_embeddings).unsqueeze(0).float().to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        lstm_model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predicted_embedding = lstm_model(input_sequence)\n",
    "        loss = F.mse_loss(predicted_embedding.squeeze(0), true_embedding)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        loss_val = loss.item()\n",
    "        loss_log.append(loss_val)\n",
    "        print(f\"Epoch {epoch+1:03d} | MSE to {target_dosage}: {loss_val:.6f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if loss_val < best_loss - 1e-5:\n",
    "            best_loss = loss_val\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"ğŸ›‘ Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    return lstm_model, loss_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ff6a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GraphProgressionLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size=1, seq_len, hidden_dim]\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        out = self.linear(hn.squeeze(0))  # shape: [hidden_dim]\n",
    "        return out.unsqueeze(0)           # shape: [1, hidden_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76813621",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = GraphProgressionLSTM(hidden_dim=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4263ee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train using past dosages to predict T320\n",
    "lstm_model, loss_log = train_lstm_on_graph_embeddings(\n",
    "    encoder=encoder,\n",
    "    graphs=graphs,\n",
    "    dosage_to_idx=dosage_to_idx,\n",
    "    lstm_model=lstm_model,  # âœ… now it's defined\n",
    "    target_dosage='T320',\n",
    "    past_dosages=['T1', 'T2.5', 'T5', 'T10', 'T20', 'T40', 'T80', 'T160'],\n",
    "    device='cpu',\n",
    "    epochs=20,\n",
    "    lr=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf19d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Prepare input sequence from past graph embeddings\n",
    "    past_dosages = ['T1', 'T2.5', 'T5', 'T10', 'T20', 'T40', 'T80', 'T160']\n",
    "    past_embeddings = []\n",
    "    for d in past_dosages:\n",
    "        data = graphs[d].to('cpu')\n",
    "        if not hasattr(data['pathway'], 'batch'):\n",
    "            data['pathway'].batch = torch.zeros(data['pathway'].num_nodes, dtype=torch.long)\n",
    "        emb = encoder(data, dosage_idx=dosage_to_idx[d])['graph_embedding'].detach().squeeze(0)\n",
    "        past_embeddings.append(emb)\n",
    "\n",
    "    input_sequence = torch.stack(past_embeddings).unsqueeze(0).float()  # [1, seq_len, hidden_dim]\n",
    "    predicted_T320_embedding = lstm_model(input_sequence).squeeze(0)    # [hidden_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34225f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” Recompute true T320 embedding for comparison\n",
    "target_data = graphs['T320'].to('cpu')\n",
    "if not hasattr(target_data['pathway'], 'batch'):\n",
    "    target_data['pathway'].batch = torch.zeros(\n",
    "        target_data['pathway'].num_nodes, dtype=torch.long\n",
    "    )\n",
    "with torch.no_grad():\n",
    "    true_T320_embedding = encoder(target_data, dosage_idx=dosage_to_idx['T320'])['graph_embedding'].squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e029c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity and MSE\n",
    "cos_sim = F.cosine_similarity(predicted_T320_embedding, true_T320_embedding, dim=0)\n",
    "mse = F.mse_loss(predicted_T320_embedding, true_T320_embedding)\n",
    "\n",
    "# âœ… Fix the output by reducing scalar\n",
    "print(f\"Cosine Similarity: {cos_sim.mean().item():.4f}\")\n",
    "print(f\"MSE: {mse.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2246079e",
   "metadata": {},
   "source": [
    "## Comprehensive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6145e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NodeFeatureEncoders(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cell_encoder = nn.Sequential(\n",
    "            nn.Linear(7, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),  # ğŸ” Replaced BatchNorm1d\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.gene_encoder = nn.Sequential(\n",
    "            nn.Linear(2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),  # ğŸ” Replaced BatchNorm1d\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.pathway_encoder = nn.Sequential(\n",
    "            nn.Linear(2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),  # ğŸ” Replaced BatchNorm1d\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, cell_x, gene_x, pathway_x):\n",
    "        h_cell = self.cell_encoder(cell_x)\n",
    "        h_gene = self.gene_encoder(gene_x)\n",
    "        h_pathway = self.pathway_encoder(pathway_x)\n",
    "        return h_cell, h_gene, h_pathway\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde2a83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MeanPoolingNoAttention(nn.Module):\n",
    "    def forward(self, x, index):\n",
    "        \"\"\"\n",
    "        x: Node embeddings [N, F]\n",
    "        index: Batch index tensor [N]\n",
    "        \"\"\"\n",
    "        num_batches = int(index.max()) + 1 if index.numel() > 0 else 1\n",
    "        out = torch.zeros(num_batches, x.size(-1), device=x.device)\n",
    "        count = torch.zeros(num_batches, device=x.device)\n",
    "\n",
    "        out = out.index_add(0, index, x)\n",
    "        count = count.index_add(0, index, torch.ones_like(index, dtype=torch.float))\n",
    "        count = count.clamp(min=1.0)  # avoid division by zero\n",
    "\n",
    "        return out / count.unsqueeze(-1), None  # No attention weights returned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0369f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import NNConv, global_mean_pool\n",
    "\n",
    "class SharedHierarchicalEncoder_NoAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, num_dosages=9, num_aux_outputs=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.node_encoders = NodeFeatureEncoders(hidden_dim)\n",
    "\n",
    "        self.dosage_embeddings = nn.Embedding(num_dosages, hidden_dim)\n",
    "        self.virtual_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dosage_lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        self.fuse_cell_virtual = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "        self.fuse_gene_virtual = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "        self.fuse_pathway_virtual = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.edge_mlp_cell_gene = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim * hidden_dim)\n",
    "        )\n",
    "        self.edge_mlp_gene_pathway = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim * hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.cell_to_gene_conv = NNConv(hidden_dim, hidden_dim, self.edge_mlp_cell_gene, aggr='mean')\n",
    "        self.gene_to_pathway_conv = NNConv(hidden_dim, hidden_dim, self.edge_mlp_gene_pathway, aggr='mean')\n",
    "\n",
    "        self.fuse_global = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.aux_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_aux_outputs)\n",
    "        )\n",
    "\n",
    "    def refine_virtuals_with_lstm(self):\n",
    "        raw_dosage_embeddings = self.dosage_embeddings.weight.unsqueeze(0)\n",
    "        lstm_out, _ = self.dosage_lstm(raw_dosage_embeddings)\n",
    "        return self.virtual_norm(lstm_out.squeeze(0))\n",
    "\n",
    "    def forward(self, data, dosage_idx):\n",
    "        cell_x, gene_x, pathway_x = data[\"cell\"].x, data[\"gene\"].x, data[\"pathway\"].x\n",
    "        h_cell, h_gene, h_pathway = self.node_encoders(cell_x, gene_x, pathway_x)\n",
    "\n",
    "        refined_dosage_virtuals = self.refine_virtuals_with_lstm()\n",
    "        dosage_virtual = refined_dosage_virtuals[dosage_idx]\n",
    "\n",
    "        num_cells = h_cell.size(0)\n",
    "        h_cell = self.fuse_cell_virtual(torch.cat([\n",
    "            h_cell,\n",
    "            dosage_virtual.unsqueeze(0).expand(num_cells, -1)\n",
    "        ], dim=1))\n",
    "\n",
    "        num_genes = h_gene.size(0)\n",
    "        h_gene = self.fuse_gene_virtual(torch.cat([\n",
    "            h_gene,\n",
    "            dosage_virtual.unsqueeze(0).expand(num_genes, -1)\n",
    "        ], dim=1))\n",
    "\n",
    "        edge_index_cg = data[\"cell\", \"expresses\", \"gene\"].edge_index\n",
    "        edge_attr_cg = data[\"cell\", \"expresses\", \"gene\"].edge_attr\n",
    "        h_gene_updated = self.cell_to_gene_conv((h_cell, h_gene), edge_index_cg, edge_attr_cg)\n",
    "\n",
    "        num_pathways = h_pathway.size(0)\n",
    "        h_pathway = self.fuse_pathway_virtual(torch.cat([\n",
    "            h_pathway,\n",
    "            dosage_virtual.unsqueeze(0).expand(num_pathways, -1)\n",
    "        ], dim=1))\n",
    "\n",
    "        edge_index_gp = data[\"gene\", \"involved_in\", \"pathway\"].edge_index\n",
    "        edge_attr_gp = data[\"gene\", \"involved_in\", \"pathway\"].edge_attr\n",
    "        h_pathway_updated = self.gene_to_pathway_conv((h_gene_updated, h_pathway), edge_index_gp, edge_attr_gp)\n",
    "\n",
    "        # ğŸ” Replace attention pooling with mean pooling\n",
    "        pooled_pathway = global_mean_pool(h_pathway_updated, data['pathway'].batch)\n",
    "\n",
    "        graph_embedding = self.fuse_global(torch.cat([\n",
    "            pooled_pathway,\n",
    "            dosage_virtual.unsqueeze(0)\n",
    "        ], dim=1))\n",
    "\n",
    "        aux_output = self.aux_head(graph_embedding)\n",
    "\n",
    "        h_cell = F.normalize(h_cell, p=2, dim=-1)\n",
    "        h_gene_updated = F.normalize(h_gene_updated, p=2, dim=-1)\n",
    "        h_pathway_updated = F.normalize(h_pathway_updated, p=2, dim=-1)\n",
    "\n",
    "        return {\n",
    "            \"h_cell\": h_cell,\n",
    "            \"h_gene\": h_gene_updated,\n",
    "            \"h_pathway\": h_pathway_updated,\n",
    "            \"dosage_virtual\": dosage_virtual,\n",
    "            \"graph_embedding\": graph_embedding,\n",
    "            \"aux_output\": aux_output.squeeze()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80627f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class HierarchicalDecoder_NoAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, cell_feature_dim=7, num_resistance_classes=5):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.cell_feature_dim = cell_feature_dim\n",
    "        self.num_resistance_classes = num_resistance_classes\n",
    "        self.dropout_rate = 0.33\n",
    "\n",
    "        # Pathway reconstruction\n",
    "        self.decode_to_pathways_fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "        )\n",
    "        self.decode_to_pathways_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Gene reconstruction from pathway\n",
    "        self.pathway_to_gene_fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Cell reconstruction from gene\n",
    "        self.gene_to_cell_fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Final cell feature decoder\n",
    "        self.decode_to_cells = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, cell_feature_dim)\n",
    "        )\n",
    "\n",
    "        # Auxiliary heads\n",
    "        self.aux_pathway_score_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        # ğŸ” Updated to output logits for multi-class classification\n",
    "        self.aux_resistance_predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(hidden_dim, num_resistance_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, h_pathway_updated, h_gene_updated, graph_embedding, dosage_virtual, num_genes, num_cells,\n",
    "                gene_mask=None, cell_mask=None):\n",
    "        B = 1\n",
    "        num_pathways = h_pathway_updated.size(0)\n",
    "\n",
    "        # ğŸ” Reconstruct pathways\n",
    "        if graph_embedding.dim() == 3:\n",
    "            graph_embedding = graph_embedding.squeeze(1)  # [1, 1, 64] â†’ [1, 64]\n",
    "\n",
    "        graph_expanded = graph_embedding.expand(B, -1)  # [1, 64]\n",
    "        pathway_input = torch.cat([\n",
    "            h_pathway_updated,\n",
    "            graph_expanded.expand(num_pathways, -1)\n",
    "        ], dim=1).unsqueeze(0)  # [1, P, 128]\n",
    "\n",
    "        base_pathway_repr = self.decode_to_pathways_fc(pathway_input)\n",
    "        pathway_recon = self.decode_to_pathways_proj(base_pathway_repr)  # [1, P, 64]\n",
    "\n",
    "        # ğŸ” Reconstruct genes from mean pooled pathways\n",
    "        gene_seed = pathway_recon.mean(dim=1).squeeze(0)  # [64]\n",
    "        gene_recon = self.pathway_to_gene_fc(gene_seed).unsqueeze(0).expand(num_genes, -1)  # [G, 64]\n",
    "\n",
    "        # ğŸ” Reconstruct cells from mean pooled genes\n",
    "        cell_seed = gene_recon.mean(dim=0)\n",
    "        cell_recon_vec = self.gene_to_cell_fc(cell_seed).unsqueeze(0).expand(num_cells, -1)  # [C, 64]\n",
    "        cell_recon = self.decode_to_cells(cell_recon_vec)  # [C, 7]\n",
    "\n",
    "        # ğŸ” Auxiliary outputs\n",
    "        aux_pathway_scores = self.aux_pathway_score_head(pathway_recon).squeeze(0).squeeze(-1)  # [P]\n",
    "        aux_resistance_score = self.aux_resistance_predictor(cell_recon_vec)  # [C, 5] logits\n",
    "\n",
    "        return {\n",
    "            \"reconstructed_pathways\": pathway_recon.squeeze(0),     # [P, 64]\n",
    "            \"reconstructed_genes\": gene_recon,                      # [G, 64]\n",
    "            \"reconstructed_cells\": cell_recon,                      # [C, 7]\n",
    "            \"aux_pathway_scores\": aux_pathway_scores,               # [P]\n",
    "            \"aux_resistance_score\": aux_resistance_score,           # [C, 5] logits\n",
    "            \"attention_pathway_to_gene\": None,\n",
    "            \"attention_gene_to_cell\": None\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cf6255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "# Load one example graph (change path if needed)\n",
    "graph_path = 'Graph_Results/HeteroGraphs_ScaledFinal/HeteroGraph_T1.pt'\n",
    "data = torch.load(graph_path)\n",
    "\n",
    "print(f\"Inspecting graph at: {graph_path}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Node types\n",
    "print(\"\\nğŸ“¦ Node Types and Feature Shapes:\")\n",
    "for node_type in data.node_types:\n",
    "    x = data[node_type].x\n",
    "    print(f\"- {node_type}: {x.shape} features, dtype: {x.dtype}\")\n",
    "    if hasattr(data[node_type], 'batch'):\n",
    "        print(f\"  Batch attribute: {data[node_type].batch.shape}\")\n",
    "    print(f\"  Feature stats: min {x.min().item()}, max {x.max().item()}, mean {x.mean().item()}\")\n",
    "\n",
    "# Edge types\n",
    "print(\"\\nğŸ”— Edge Types and Attributes:\")\n",
    "for edge_type in data.edge_types:\n",
    "    edge_index = data[edge_type].edge_index\n",
    "    edge_attr = data[edge_type].edge_attr\n",
    "    print(f\"- {edge_type}: {edge_index.shape[1]} edges, edge_attr shape: {edge_attr.shape}\")\n",
    "    print(f\"  Edge attr stats: min {edge_attr.min().item()}, max {edge_attr.max().item()}, mean {edge_attr.mean().item()}\")\n",
    "\n",
    "# Summary counts\n",
    "print(\"\\nğŸ“Š Summary:\")\n",
    "print(f\"- Total node types: {len(data.node_types)}\")\n",
    "print(f\"- Total edge types: {len(data.edge_types)}\")\n",
    "total_nodes = sum(data[node_type].num_nodes for node_type in data.node_types)\n",
    "print(f\"- Total nodes: {total_nodes}\")\n",
    "total_edges = sum(data[edge_type].edge_index.shape[1] for edge_type in data.edge_types)\n",
    "print(f\"- Total edges: {total_edges}\")\n",
    "\n",
    "# Optional: inspect one example feature vector\n",
    "for node_type in data.node_types:\n",
    "    print(f\"\\nExample {node_type} feature vector (first node):\")\n",
    "    print(data[node_type].x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d67b365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def load_all_dosage_graphs(graph_dir, pattern_prefix=\"HeteroGraph_T\"):\n",
    "    \"\"\"\n",
    "    Loads all dosage-specific heterographs from the specified directory.\n",
    "\n",
    "    Args:\n",
    "        graph_dir (str): Path to directory containing dosage graphs.\n",
    "        pattern_prefix (str): File prefix to identify graph files (e.g., \"HeteroGraph_T\").\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary { \"T1\": data_obj, \"T10\": data_obj, ... }\n",
    "    \"\"\"\n",
    "    graphs = {}\n",
    "    for fname in os.listdir(graph_dir):\n",
    "        if fname.startswith(pattern_prefix) and fname.endswith(\".pt\"):\n",
    "            dosage_key = fname.replace(\".pt\", \"\").replace(pattern_prefix, \"T\")\n",
    "            path = os.path.join(graph_dir, fname)\n",
    "            data = torch.load(path)\n",
    "            graphs[dosage_key] = data\n",
    "    return graphs\n",
    "# Load graphs\n",
    "graph_dir = \"Graph_Results/HeteroGraphs_ScaledFinal\"\n",
    "graphs = load_all_dosage_graphs(graph_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ae34dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# === Load saved manual 5-fold file ===\n",
    "with open('cv_folds_manual_5fold.json', 'r') as f:\n",
    "    fold_data = json.load(f)\n",
    "\n",
    "# === These are dosage names (e.g., 'T1', 'T2.5', etc.)\n",
    "dosage_keys = fold_data[\"dosage_keys\"]\n",
    "\n",
    "# === Folds already contain dosage strings, not indices\n",
    "folds = fold_data[\"folds\"]\n",
    "\n",
    "# âœ… Use Fold 1 (index 0)\n",
    "fold_idx = 0\n",
    "train_keys, val_keys = folds[fold_idx]  # No need to index into dosage_keys\n",
    "\n",
    "# === Retrieve graph data for training and validation\n",
    "train_graphs = {k: graphs[k] for k in train_keys}\n",
    "val_graphs = {k: graphs[k] for k in val_keys}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d521ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HierarchicalLoss_NoAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 lambda_pathway=2.0,\n",
    "                 lambda_gene=1.0,\n",
    "                 lambda_cell=0.5,\n",
    "                 lambda_resistance=0.1,\n",
    "                 use_resistance=True):\n",
    "        super().__init__()\n",
    "        self.lambda_pathway = lambda_pathway\n",
    "        self.lambda_gene = lambda_gene\n",
    "        self.lambda_cell = lambda_cell\n",
    "        self.lambda_resistance = lambda_resistance\n",
    "        self.use_resistance = use_resistance\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        # Base reconstruction losses\n",
    "        L_pathway = F.mse_loss(outputs[\"reconstructed_pathways\"], targets[\"h_pathway\"])\n",
    "        L_gene = F.mse_loss(outputs[\"reconstructed_genes\"], targets[\"h_gene\"])\n",
    "        L_cell = F.mse_loss(outputs[\"reconstructed_cells\"], targets[\"cell_features\"])\n",
    "\n",
    "        # Resistance classification loss (multi-class cross-entropy)\n",
    "        if self.use_resistance and \"resistance_label\" in targets:\n",
    "            L_resistance = F.cross_entropy(\n",
    "                outputs[\"aux_resistance_score\"],                      # [C, 5] logits\n",
    "                targets[\"resistance_label\"].squeeze(-1).long()        # [C] class index\n",
    "            )\n",
    "        else:\n",
    "            L_resistance = torch.tensor(0.0, device=L_cell.device)\n",
    "\n",
    "        # Total weighted loss\n",
    "        total_loss = (\n",
    "            self.lambda_pathway * L_pathway +\n",
    "            self.lambda_gene * L_gene +\n",
    "            self.lambda_cell * L_cell +\n",
    "            self.lambda_resistance * L_resistance\n",
    "        )\n",
    "\n",
    "        # Return loss and breakdown\n",
    "        loss_dict = {\n",
    "            \"total_loss\": total_loss.item(),\n",
    "            \"pathway_loss\": L_pathway.item(),\n",
    "            \"gene_loss\": L_gene.item(),\n",
    "            \"cell_loss\": L_cell.item(),\n",
    "            \"resistance_loss\": L_resistance.item(),\n",
    "            \"attention_reg_loss\": 0.0  # Placeholder to match attention-based loss output\n",
    "        }\n",
    "\n",
    "        return total_loss, loss_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62841dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def train_encoder_decoder_model(\n",
    "    encoder, decoder, graphs, dosage_to_idx,\n",
    "    optimizer=None, device='cuda', epochs=100,\n",
    "    loss_weights=None, save_path=None\n",
    "):\n",
    "    if loss_weights is None:\n",
    "        loss_weights = {\n",
    "            'lambda_pathway': 2.433302836237277,\n",
    "            'lambda_gene': 1.5656833777217563,\n",
    "            'lambda_cell':  0.09616027936016658,\n",
    "            'lambda_resistance': 0.31864704926747495,\n",
    "            'lambda_attention': 0.0,           # âŒ No attention\n",
    "            'use_attention_reg': False         # âŒ No attention regularization\n",
    "        }\n",
    "\n",
    "    encoder = encoder.to(device)\n",
    "    decoder = decoder.to(device)\n",
    "\n",
    "    criterion = HierarchicalLoss(**loss_weights).to(device)\n",
    "\n",
    "    if optimizer is None:\n",
    "        optimizer = Adam(\n",
    "            list(encoder.parameters()) + list(decoder.parameters()),\n",
    "            lr=1e-3, weight_decay=1e-5\n",
    "        )\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-5, verbose=True)\n",
    "    best_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    early_stop_patience = 10\n",
    "\n",
    "    loss_log = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        epoch_loss_dict = {\n",
    "            'pathway_loss': 0.0, 'gene_loss': 0.0, 'cell_loss': 0.0,\n",
    "            'resistance_loss': 0.0, 'attention_reg_loss': 0.0, 'total_loss': 0.0\n",
    "        }\n",
    "\n",
    "        for dosage_name, graph in graphs.items():\n",
    "            data = graph.to(device)\n",
    "            dosage_idx = dosage_to_idx[dosage_name]\n",
    "\n",
    "            if not hasattr(data['pathway'], 'batch'):\n",
    "                data['pathway'].batch = torch.zeros(\n",
    "                    data['pathway'].num_nodes, dtype=torch.long, device=device\n",
    "                )\n",
    "\n",
    "            encoder_out = encoder(data, dosage_idx)\n",
    "            decoder_out = decoder(\n",
    "                h_pathway_updated=encoder_out['h_pathway'],\n",
    "                h_gene_updated=encoder_out['h_gene'],\n",
    "                graph_embedding=encoder_out['graph_embedding'].unsqueeze(0),\n",
    "                dosage_virtual=encoder_out['dosage_virtual'].unsqueeze(0),\n",
    "                num_genes=encoder_out['h_gene'].shape[0],\n",
    "                num_cells=encoder_out['h_cell'].shape[0]\n",
    "            )\n",
    "\n",
    "            targets = {\n",
    "                'h_pathway': encoder_out['h_pathway'].detach(),\n",
    "                'h_gene': encoder_out['h_gene'].detach(),\n",
    "                'cell_features': data['cell'].x,\n",
    "                'resistance_label': data['cell'].x[:, -5:]\n",
    "            }\n",
    "\n",
    "            loss, loss_components = criterion(decoder_out, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            for key in epoch_loss_dict:\n",
    "                epoch_loss_dict[key] += loss_components[key]\n",
    "\n",
    "        # Logging\n",
    "        num_graphs = len(graphs)\n",
    "        avg_loss_dict = {k: v / num_graphs for k, v in epoch_loss_dict.items()}\n",
    "        loss_log.append(avg_loss_dict)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | Total: {avg_loss_dict['total_loss']:.4f} | \"\n",
    "              f\"P: {avg_loss_dict['pathway_loss']:.4f}, G: {avg_loss_dict['gene_loss']:.4f}, \"\n",
    "              f\"C: {avg_loss_dict['cell_loss']:.4f}, R: {avg_loss_dict['resistance_loss']:.4f}, \"\n",
    "              f\"A: {avg_loss_dict['attention_reg_loss']:.4f}\")\n",
    "\n",
    "        monitored = avg_loss_dict['total_loss']\n",
    "        scheduler.step(monitored)\n",
    "\n",
    "        if monitored < best_loss - 1e-4:\n",
    "            best_loss = monitored\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter >= early_stop_patience:\n",
    "            print(f\"ğŸ›‘ Early stopping at epoch {epoch+1} (no improvement for {early_stop_patience} epochs)\")\n",
    "            break\n",
    "\n",
    "    # Save model and logs\n",
    "    if save_path:\n",
    "        torch.save({'encoder': encoder.state_dict(), 'decoder': decoder.state_dict()}, save_path)\n",
    "        print(f\"âœ… Model saved to {save_path}\")\n",
    "\n",
    "        loss_log_path = save_path.replace('.pth', '_loss_log.json')\n",
    "        with open(loss_log_path, 'w') as f:\n",
    "            json.dump(loss_log, f, indent=2)\n",
    "        print(f\"ğŸ“‰ Loss log saved to {loss_log_path}\")\n",
    "\n",
    "    return encoder, decoder, loss_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2b1c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ğŸ” Map 'T2.5', 'T10', etc. to integer indices\n",
    "dosage_levels = sorted([float(k.replace('T', '')) for k in graphs.keys()])\n",
    "dosage_to_idx = {f\"T{int(d) if d.is_integer() else d}\": i for i, d in enumerate(dosage_levels)}\n",
    "\n",
    "# ğŸ§  Use NoAttention version of encoder and decoder\n",
    "encoder = SharedHierarchicalEncoder_NoAttention(hidden_dim=64, num_dosages=len(dosage_to_idx))\n",
    "decoder = HierarchicalDecoder_NoAttention(hidden_dim=64, cell_feature_dim=7)\n",
    "\n",
    "# ğŸ‹ï¸â€â™‚ï¸ Train the model with no attention\n",
    "trained_encoder, trained_decoder, loss_log = train_encoder_decoder_model(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    graphs=train_graphs,\n",
    "    dosage_to_idx=dosage_to_idx,\n",
    "    device='cpu',\n",
    "    epochs=100,\n",
    "    save_path=\"trained_model_no_attention_fold1.pth\",\n",
    "    loss_weights={\n",
    "        'lambda_pathway': 2.433302836237277,\n",
    "        'lambda_gene': 1.5656833777217563,\n",
    "        'lambda_cell': 0.09616027936016658,\n",
    "        'lambda_resistance': 0.31864704926747495,\n",
    "        'lambda_attention': 0.0,         # âŒ disable attention loss\n",
    "        'use_attention_reg': False       # âŒ turn off attention regularization\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94650a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with correct dosage count used during training\n",
    "encoder = SharedHierarchicalEncoder_NoAttention(hidden_dim=64, num_dosages=9)\n",
    "decoder = HierarchicalDecoder_NoAttention(hidden_dim=64, cell_feature_dim=7)\n",
    "\n",
    "# Load checkpoint trained without attention\n",
    "checkpoint = torch.load(\"trained_model_no_attention_fold1.pth\", map_location='cpu')\n",
    "encoder.load_state_dict(checkpoint['encoder'])\n",
    "decoder.load_state_dict(checkpoint['decoder'])\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c95b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dir = \"Graph_Results/HeteroGraphs_ScaledFinal\"\n",
    "graphs = load_all_dosage_graphs(graph_dir)\n",
    "\n",
    "# Also ensure you have dosage_to_idx mapping\n",
    "dosage_to_idx = {k: i for i, k in enumerate(sorted(graphs.keys(), key=lambda x: float(x[1:])))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abff48f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs = {}\n",
    "device = torch.device('cpu')\n",
    "\n",
    "for dosage_name, data in graphs.items():\n",
    "    data = data.to(device)\n",
    "    dosage_idx = dosage_to_idx[dosage_name]\n",
    "\n",
    "    # âœ… Inject dummy batch for pathway if missing\n",
    "    if not hasattr(data['pathway'], 'batch'):\n",
    "        data['pathway'].batch = torch.zeros(\n",
    "            data['pathway'].num_nodes, dtype=torch.long, device=data['pathway'].x.device\n",
    "        )\n",
    "\n",
    "    # Forward pass without gradients\n",
    "    with torch.no_grad():\n",
    "        encoder_out = encoder(data, dosage_idx)\n",
    "\n",
    "        decoder_out = decoder(\n",
    "            h_pathway_updated=encoder_out['h_pathway'],        # [P, 64]\n",
    "            h_gene_updated=encoder_out['h_gene'],              # [G, 64]\n",
    "            graph_embedding=encoder_out['graph_embedding'].unsqueeze(0),  # [1, 64]\n",
    "            dosage_virtual=encoder_out['dosage_virtual'].unsqueeze(0),    # [1, 64]\n",
    "            num_genes=encoder_out['h_gene'].shape[0],\n",
    "            num_cells=encoder_out['h_cell'].shape[0]\n",
    "        )\n",
    "\n",
    "    all_outputs[dosage_name] = {\n",
    "        \"encoder\": encoder_out,\n",
    "        \"decoder\": decoder_out\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Path to graph mappings\n",
    "graph_mapping_dir = \"Graph_Results/Graph_Mappings\"\n",
    "embedding_records = []\n",
    "\n",
    "# Extract embeddings\n",
    "for dosage_name, output in all_outputs.items():\n",
    "    mapping_path = os.path.join(graph_mapping_dir, f\"Graph_Mapping_{dosage_name}.json\")\n",
    "    if not os.path.exists(mapping_path):\n",
    "        continue\n",
    "\n",
    "    with open(mapping_path, 'r') as f:\n",
    "        mapping = json.load(f)\n",
    "\n",
    "    h_gene = output[\"encoder\"][\"h_gene\"]\n",
    "    h_pathway = output[\"encoder\"][\"h_pathway\"]\n",
    "\n",
    "    for gene, idx in mapping[\"gene_to_index\"].items():\n",
    "        embedding_records.append({\n",
    "            \"type\": \"gene\",\n",
    "            \"name\": gene,\n",
    "            \"dosage\": dosage_name,\n",
    "            \"embedding\": h_gene[idx].tolist()\n",
    "        })\n",
    "\n",
    "    for pathway, idx in mapping[\"pathway_to_index\"].items():\n",
    "        embedding_records.append({\n",
    "            \"type\": \"pathway\",\n",
    "            \"name\": pathway,\n",
    "            \"dosage\": dosage_name,\n",
    "            \"embedding\": h_pathway[idx].tolist()\n",
    "        })\n",
    "\n",
    "# Prepare data\n",
    "df_embed = pd.DataFrame(embedding_records)\n",
    "embedding_matrix = np.vstack(df_embed[\"embedding\"].values)\n",
    "\n",
    "# Run t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, init='pca')\n",
    "tsne_result = tsne.fit_transform(embedding_matrix)\n",
    "\n",
    "df_embed[\"tsne-1\"] = tsne_result[:, 0]\n",
    "df_embed[\"tsne-2\"] = tsne_result[:, 1]\n",
    "\n",
    "# Save embeddings\n",
    "df_embed.to_csv(\"tSNE_Embeddings_All_no_attention.csv\", index=False)\n",
    "\n",
    "# Set global font size\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Plot gene embeddings\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=df_embed[df_embed[\"type\"] == \"gene\"],\n",
    "    x=\"tsne-1\", y=\"tsne-2\", hue=\"dosage\", palette=\"tab10\"\n",
    ")\n",
    "plt.title(\"t-SNE: Gene Embeddings by Dosage\", fontsize=16)\n",
    "plt.xlabel(\"tSNE-1\", fontsize=14)\n",
    "plt.ylabel(\"tSNE-2\", fontsize=14)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Dosage\", fontsize=10, title_fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tSNE_Gene_Embeddings_no_attention.png\", dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Plot pathway embeddings\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=df_embed[df_embed[\"type\"] == \"pathway\"],\n",
    "    x=\"tsne-1\", y=\"tsne-2\", hue=\"dosage\", palette=\"tab10\"\n",
    ")\n",
    "plt.title(\"t-SNE: Pathway Embeddings by Dosage\", fontsize=16)\n",
    "plt.xlabel(\"tSNE-1\", fontsize=14)\n",
    "plt.ylabel(\"tSNE-2\", fontsize=14)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Dosage\", fontsize=10, title_fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tSNE_Pathway_Embeddings_no_attention.png\", dpi=600, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca8ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the t-SNE embedding file for the no attention model\n",
    "df_embed = pd.read_csv(\"tSNE_Embeddings_All_no_attention.csv\")\n",
    "\n",
    "# Step 1: Group embeddings\n",
    "grouped_embeddings = defaultdict(lambda: defaultdict(list))\n",
    "for _, row in df_embed.iterrows():\n",
    "    entity_type = row[\"type\"]\n",
    "    name = row[\"name\"]\n",
    "    dosage = row[\"dosage\"]\n",
    "    emb = np.array(row[\"embedding\"].strip('[]').split(','), dtype=float) if isinstance(row[\"embedding\"], str) else row[\"embedding\"]\n",
    "    grouped_embeddings[entity_type][name].append((dosage, emb))\n",
    "\n",
    "# Step 2: Define dosage order\n",
    "dosage_order = ['T1', 'T2.5', 'T5', 'T10', 'T20', 'T40', 'T80', 'T160', 'T320']\n",
    "dosage_to_index = {d: i for i, d in enumerate(dosage_order)}\n",
    "\n",
    "def compute_cosine_drift(embedding_dict):\n",
    "    drift_dict = {}\n",
    "    for name, entries in embedding_dict.items():\n",
    "        # Sort by dosage order\n",
    "        entries_sorted = sorted(entries, key=lambda x: dosage_to_index.get(x[0], -1))\n",
    "        drifts = []\n",
    "        for (d1, v1), (d2, v2) in zip(entries_sorted[:-1], entries_sorted[1:]):\n",
    "            sim = cosine_similarity([v1], [v2])[0, 0]\n",
    "            drift = 1 - sim\n",
    "            drifts.append(drift)\n",
    "        if drifts:\n",
    "            drift_dict[name] = {\n",
    "                \"drifts\": drifts,\n",
    "                \"mean_drift\": np.mean(drifts),\n",
    "                \"num_pairs\": len(drifts)\n",
    "            }\n",
    "    return drift_dict\n",
    "\n",
    "# Step 3: Compute drifts\n",
    "gene_drift = compute_cosine_drift(grouped_embeddings['gene'])\n",
    "pathway_drift = compute_cosine_drift(grouped_embeddings['pathway'])\n",
    "\n",
    "# Step 4: Compute overall mean drift\n",
    "mean_gene_drift = np.mean([v[\"mean_drift\"] for v in gene_drift.values()])\n",
    "mean_pathway_drift = np.mean([v[\"mean_drift\"] for v in pathway_drift.values()])\n",
    "\n",
    "print(f\"ğŸ“Š Average Gene Drift Across Dosages (No Attention): {mean_gene_drift:.4f}\")\n",
    "print(f\"ğŸ“Š Average Pathway Drift Across Dosages (No Attention): {mean_pathway_drift:.4f}\")\n",
    "\n",
    "# Step 5: Save detailed drift info\n",
    "pd.DataFrame([\n",
    "    {\"Gene\": k, \"MeanDrift\": v[\"mean_drift\"], \"Pairs\": v[\"num_pairs\"]}\n",
    "    for k, v in gene_drift.items()\n",
    "]).to_csv(\"Gene_Cosine_Drift_no_attention.csv\", index=False)\n",
    "\n",
    "pd.DataFrame([\n",
    "    {\"Pathway\": k, \"MeanDrift\": v[\"mean_drift\"], \"Pairs\": v[\"num_pairs\"]}\n",
    "    for k, v in pathway_drift.items()\n",
    "]).to_csv(\"Pathway_Cosine_Drift_no_attention.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218edb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "# Define dosage order\n",
    "dosage_order = [\"T1\", \"T2.5\", \"T5\", \"T10\", \"T20\", \"T40\", \"T80\", \"T160\", \"T320\"]\n",
    "\n",
    "# Lists to store drift values\n",
    "gene_drift, pathway_drift, transitions = [], [], []\n",
    "\n",
    "# Compute cosine drift between adjacent dosages\n",
    "for i in range(len(dosage_order) - 1):\n",
    "    d1, d2 = dosage_order[i], dosage_order[i + 1]\n",
    "    transitions.append(f\"{d1}â†’{d2}\")\n",
    "\n",
    "    h_gene_1 = all_outputs[d1]['encoder']['h_gene']\n",
    "    h_gene_2 = all_outputs[d2]['encoder']['h_gene']\n",
    "    h_pathway_1 = all_outputs[d1]['encoder']['h_pathway']\n",
    "    h_pathway_2 = all_outputs[d2]['encoder']['h_pathway']\n",
    "\n",
    "    # Align dimensions\n",
    "    min_gene = min(h_gene_1.shape[0], h_gene_2.shape[0])\n",
    "    min_pathway = min(h_pathway_1.shape[0], h_pathway_2.shape[0])\n",
    "    h_gene_1, h_gene_2 = h_gene_1[:min_gene], h_gene_2[:min_gene]\n",
    "    h_pathway_1, h_pathway_2 = h_pathway_1[:min_pathway], h_pathway_2[:min_pathway]\n",
    "\n",
    "    # Compute mean cosine distances\n",
    "    gene_distance = cosine_distances(h_gene_1.numpy(), h_gene_2.numpy()).mean()\n",
    "    pathway_distance = cosine_distances(h_pathway_1.numpy(), h_pathway_2.numpy()).mean()\n",
    "\n",
    "    gene_drift.append(gene_distance)\n",
    "    pathway_drift.append(pathway_distance)\n",
    "\n",
    "# --- Plot Gene Drift ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(transitions, gene_drift, marker='o', color='blue')\n",
    "plt.title(\"Gene Embedding Drift\", fontsize=16)\n",
    "plt.xlabel(\"Dosage Transition\", fontsize=14)\n",
    "plt.ylabel(\"Mean Cosine Distance\", fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"gene_embedding_drift.png\", dpi=600)\n",
    "plt.show()\n",
    "\n",
    "# --- Plot Pathway Drift ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(transitions, pathway_drift, marker='o', color='green')\n",
    "plt.title(\"Pathway Embedding Drift\", fontsize=16)\n",
    "plt.xlabel(\"Dosage Transition\", fontsize=14)\n",
    "plt.ylabel(\"Mean Cosine Distance\", fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"pathway_embedding_drift.png\", dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74230b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "# --- Extract standardized pathway identifiers ---\n",
    "def extract_pathway_identifier(name):\n",
    "    \"\"\"\n",
    "    Extracts a Reactome ID (R-HSA: or r-hsa-) or GO ID (GO:) from a pathway name.\n",
    "    Returns the matched identifier in standard form (uppercase), or None if not found.\n",
    "    \"\"\"\n",
    "    # Match Reactome (r-hsa:123456 or R-HSA-123456)\n",
    "    match_reactome = re.search(r\"(r-hsa[-:]\\d+)\", name, flags=re.IGNORECASE)\n",
    "    if match_reactome:\n",
    "        return match_reactome.group(1).upper().replace(\":\", \"-\")\n",
    "\n",
    "    # Match GO term (GO:0000000)\n",
    "    match_go = re.search(r\"(GO:\\d+)\", name, flags=re.IGNORECASE)\n",
    "    if match_go:\n",
    "        return match_go.group(1).upper()\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- Load mappings and encoder outputs ---\n",
    "def load_embeddings_and_mappings(dosage_names, encoder_outputs, mapping_dir):\n",
    "    gene_embeddings_by_dosage = {}\n",
    "    pathway_embeddings_by_dosage = {}\n",
    "    gene_id_sets = []\n",
    "\n",
    "    for dosage_name in dosage_names:\n",
    "        with open(os.path.join(mapping_dir, f\"Graph_Mapping_{dosage_name}.json\")) as f:\n",
    "            mapping = json.load(f)\n",
    "\n",
    "        gene_map = mapping[\"gene_to_index\"]\n",
    "        pathway_map = mapping[\"pathway_to_index\"]\n",
    "\n",
    "        inv_gene_map = {v: k for k, v in gene_map.items()}\n",
    "\n",
    "        # Extract and clean pathway IDs\n",
    "        inv_pathway_map_raw = {v: extract_pathway_identifier(k) for k, v in pathway_map.items()}\n",
    "        inv_pathway_map = {k: v for k, v in inv_pathway_map_raw.items() if v is not None}\n",
    "\n",
    "        h_gene = encoder_outputs[dosage_name][\"encoder\"][\"h_gene\"]\n",
    "        h_pathway = encoder_outputs[dosage_name][\"encoder\"][\"h_pathway\"]\n",
    "\n",
    "        gene_embeddings_by_dosage[dosage_name] = {\n",
    "            inv_gene_map[i]: h_gene[i].cpu().numpy() for i in range(len(h_gene))\n",
    "        }\n",
    "        pathway_embeddings_by_dosage[dosage_name] = {\n",
    "            inv_pathway_map[i]: h_pathway[i].cpu().numpy() for i in range(len(h_pathway))\n",
    "            if i in inv_pathway_map\n",
    "        }\n",
    "\n",
    "        gene_id_sets.append(set(gene_map.keys()))\n",
    "\n",
    "    return gene_embeddings_by_dosage, pathway_embeddings_by_dosage\n",
    "\n",
    "\n",
    "# --- Compute cosine drift across consecutive dosages ---\n",
    "def compute_pairwise_drift(entity_dict, dosage_order):\n",
    "    \"\"\"\n",
    "    Computes total pairwise cosine drift across consecutive dosages for each entity.\n",
    "    Returns DataFrame with index=entity, column=\"Total_Drift\".\n",
    "    \"\"\"\n",
    "    entity_ids = set.intersection(*[set(entity_dict[d].keys()) for d in dosage_order])\n",
    "    drift_scores = {}\n",
    "\n",
    "    for entity in entity_ids:\n",
    "        vectors = [entity_dict[d][entity] for d in dosage_order]\n",
    "        total_drift = 0.0\n",
    "        for i in range(len(vectors) - 1):\n",
    "            v1 = vectors[i].reshape(1, -1)\n",
    "            v2 = vectors[i + 1].reshape(1, -1)\n",
    "            drift = cosine_distances(v1, v2)[0][0]\n",
    "            total_drift += drift\n",
    "        drift_scores[entity] = total_drift\n",
    "\n",
    "    return pd.DataFrame.from_dict(drift_scores, orient=\"index\", columns=[\"Total_Drift\"])\n",
    "\n",
    "\n",
    "# --- Plot top rewired genes/pathways ---\n",
    "def plot_top_rewiring(drift_df, top_n=50, title=\"\", entity_type=\"Gene\", save_path=None):\n",
    "    \"\"\"\n",
    "    Plots top-N rewired entities (genes or pathways) based on total cosine drift.\n",
    "    \"\"\"\n",
    "    top_drift = drift_df.sort_values(\"Total_Drift\", ascending=False).head(top_n)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(max(8, top_n * 0.25), 6))  # dynamic width\n",
    "    bars = ax.bar(top_drift.index, top_drift[\"Total_Drift\"], color=\"slateblue\")\n",
    "\n",
    "    ax.set_xticks(range(len(top_drift)))\n",
    "    ax.set_xticklabels(top_drift.index, rotation=90)\n",
    "    ax.set_ylabel(\"Total Cosine Drift\")\n",
    "    ax.set_title(title or f\"Top {top_n} Rewired {entity_type}s Across Dosage\")\n",
    "\n",
    "    plt.subplots_adjust(bottom=0.3)  # extra margin for long labels\n",
    "\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, dpi=600, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef4d2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "mapping_dir = \"Graph_Results/Graph_Mappings\"\n",
    "dosage_names = sorted(all_outputs.keys(), key=lambda x: float(x[1:]))  # works with T2.5, T10, etc.\n",
    "\n",
    "\n",
    "gene_dict, pathway_dict = load_embeddings_and_mappings(dosage_names, all_outputs, mapping_dir)\n",
    "\n",
    "# Drift\n",
    "gene_drift = compute_pairwise_drift(gene_dict, dosage_names)\n",
    "pathway_drift = compute_pairwise_drift(pathway_dict, dosage_names)\n",
    "\n",
    "plot_top_rewiring(\n",
    "    gene_drift,\n",
    "    top_n=30,\n",
    "    title=\"Top 30 Rewired Genes Across Dosage\",\n",
    "    entity_type=\"Gene\",\n",
    "    save_path=\"Plots/Gene_Rewired_Top30_final.png\"\n",
    ")\n",
    "\n",
    "plot_top_rewiring(\n",
    "    pathway_drift,\n",
    "    top_n=30,\n",
    "    title=\"Top 30 Rewired Pathways Across Dosage\",\n",
    "    entity_type=\"Pathway\",\n",
    "    save_path=\"Plots/Pathway_Rewired_Top30_final.png\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c4dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique state labels present in T10\n",
    "import torch\n",
    "\n",
    "state_labels = graphs[\"T10\"][\"cell\"].x[:, 2]\n",
    "print(\"Unique resistance states in T10:\", torch.unique(state_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46357b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import os\n",
    "\n",
    "# === Ensure output directory exists ===\n",
    "os.makedirs(\"Plots\", exist_ok=True)\n",
    "\n",
    "# === Extract cell embeddings and state labels from T10 ===\n",
    "h_cell = all_outputs[\"T10\"][\"encoder\"][\"h_cell\"]  # [N_cells, 64]\n",
    "state_feats = graphs[\"T10\"][\"cell\"].x[:, 2:]       # One-hot resistance state\n",
    "\n",
    "# Determine state label per cell (0-indexed: State_1 â†’ label 0, etc.)\n",
    "state_labels = state_feats.argmax(dim=1)\n",
    "\n",
    "# === Filter for State 2 (label 1), State 3 (label 2), State 4 (label 3) ===\n",
    "state2_cells = h_cell[state_labels == 1]\n",
    "state3_cells = h_cell[state_labels == 2]\n",
    "state4_cells = h_cell[state_labels == 3]\n",
    "\n",
    "# Combine and create labels\n",
    "combined = torch.cat([state2_cells, state3_cells, state4_cells], dim=0).cpu().numpy()\n",
    "labels = (\n",
    "    [\"State 2\"] * state2_cells.shape[0] +\n",
    "    [\"State 3\"] * state3_cells.shape[0] +\n",
    "    [\"State 4\"] * state4_cells.shape[0]\n",
    ")\n",
    "\n",
    "# === UMAP ===\n",
    "reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "umap_result = reducer.fit_transform(combined)\n",
    "\n",
    "# === Plot ===\n",
    "plt.figure(figsize=(7, 5))\n",
    "colors = {\"State 2\": \"skyblue\", \"State 3\": \"salmon\", \"State 4\": \"orchid\"}\n",
    "\n",
    "for label in set(labels):\n",
    "    idx = [i for i, l in enumerate(labels) if l == label]\n",
    "    plt.scatter(umap_result[idx, 0], umap_result[idx, 1], label=label, alpha=0.6, s=45, color=colors[label])\n",
    "\n",
    "plt.xlabel(\"UMAP 1\", fontsize=10)\n",
    "plt.ylabel(\"UMAP 2\", fontsize=10)\n",
    "plt.title(\"UMAP of Cell States in T10\", fontsize=12)\n",
    "plt.legend(\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(1.02, 0.5),\n",
    "    fontsize=8,\n",
    "    title=\"Cell States\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Plots/T10_State2_vs_3_vs_4_umap_final.png\", dpi=700)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a77652",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SharedHierarchicalEncoder_NoAttention(hidden_dim=64, num_dosages=9)\n",
    "checkpoint = torch.load(\"trained_model_no_attention_fold1.pth\", map_location='cpu')\n",
    "encoder.load_state_dict(checkpoint['encoder'])\n",
    "encoder.eval()\n",
    "encoder.to('cpu')  # assuming CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5213e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "def train_lstm_to_predict_embedding(\n",
    "    encoder, graphs, dosage_to_idx,\n",
    "    target_dosage='T320', device='cpu',\n",
    "    epochs=50, lr=1e-2\n",
    "):\n",
    "    encoder = encoder.to(device)\n",
    "    encoder.eval()\n",
    "\n",
    "    optimizer = Adam(encoder.dosage_lstm.parameters(), lr=lr)\n",
    "    loss_log = []\n",
    "\n",
    "    data_target = graphs[target_dosage].to(device)\n",
    "    if not hasattr(data_target['pathway'], 'batch'):\n",
    "        data_target['pathway'].batch = torch.zeros(data_target['pathway'].num_nodes, dtype=torch.long)\n",
    "\n",
    "    idx_target = dosage_to_idx[target_dosage]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        encoder.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # ğŸ” Generate dosage virtuals using LSTM\n",
    "        raw_embeddings = encoder.dosage_embeddings.weight.unsqueeze(0)  # [1, 9, 64]\n",
    "        lstm_out, _ = encoder.dosage_lstm(raw_embeddings)               # [1, 9, 64]\n",
    "        refined_virtuals = encoder.virtual_norm(lstm_out.squeeze(0))    # [9, 64]\n",
    "        predicted_embedding = refined_virtuals[idx_target]              # [64]\n",
    "\n",
    "        # ğŸ” Get actual embedding from encoder\n",
    "        with torch.no_grad():\n",
    "            encoder.eval()\n",
    "            true_embedding = encoder(data_target, dosage_idx=idx_target)['graph_embedding'].detach().squeeze()\n",
    "\n",
    "        # ğŸ” Loss\n",
    "        loss = F.mse_loss(predicted_embedding, true_embedding)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_log.append(loss.item())\n",
    "        print(f\"Epoch {epoch+1:03d} | MSE to {target_dosage}: {loss.item():.6f}\")\n",
    "\n",
    "    return encoder, loss_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6768ff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, loss_log = train_lstm_to_predict_embedding(\n",
    "    encoder=encoder,\n",
    "    graphs=graphs,\n",
    "    dosage_to_idx=dosage_to_idx,\n",
    "    target_dosage='T320',\n",
    "    device='cpu'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c39513",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    raw_embeddings = encoder.dosage_embeddings.weight.unsqueeze(0)\n",
    "    lstm_out, _ = encoder.dosage_lstm(raw_embeddings)\n",
    "    refined_virtuals = encoder.virtual_norm(lstm_out.squeeze(0))\n",
    "    predicted_T320_embedding = refined_virtuals[dosage_to_idx['T320']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0469821",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_T320_embedding = encoder(graphs['T320'], dosage_idx=dosage_to_idx['T320'])['graph_embedding'].squeeze()\n",
    "\n",
    "cos_sim = F.cosine_similarity(predicted_T320_embedding, true_T320_embedding, dim=0)\n",
    "mse = F.mse_loss(predicted_T320_embedding, true_T320_embedding)\n",
    "\n",
    "print(f\"Cosine Similarity: {cos_sim.item():.4f}\")\n",
    "print(f\"MSE: {mse.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f076cf",
   "metadata": {},
   "source": [
    "# Predict Future Dosage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a5dcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SharedHierarchicalEncoder_NoAttention(hidden_dim=64, num_dosages=9)\n",
    "checkpoint = torch.load(\"trained_model_no_attention_fold1.pth\", map_location='cpu')\n",
    "encoder.load_state_dict(checkpoint['encoder'])\n",
    "encoder.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3c69c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "def train_lstm_to_predict_embedding(\n",
    "    encoder, graphs, dosage_to_idx,\n",
    "    target_dosage='T320', device='cpu',\n",
    "    epochs=100, lr=1e-3\n",
    "):\n",
    "    encoder = encoder.to(device)\n",
    "    encoder.eval()\n",
    "\n",
    "    optimizer = Adam(encoder.dosage_lstm.parameters(), lr=lr)\n",
    "    loss_log = []\n",
    "\n",
    "    data_target = graphs[target_dosage].to(device)\n",
    "    if not hasattr(data_target['pathway'], 'batch'):\n",
    "        data_target['pathway'].batch = torch.zeros(data_target['pathway'].num_nodes, dtype=torch.long)\n",
    "\n",
    "    idx_target = dosage_to_idx[target_dosage]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        encoder.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Manually compute dosage embeddings via LSTM\n",
    "        raw_embeddings = encoder.dosage_embeddings.weight.unsqueeze(0)\n",
    "        lstm_out, _ = encoder.dosage_lstm(raw_embeddings)\n",
    "        refined_virtuals = encoder.virtual_norm(lstm_out.squeeze(0))\n",
    "        predicted_embedding = refined_virtuals[idx_target]\n",
    "\n",
    "        # Get actual graph embedding for T320\n",
    "        with torch.no_grad():\n",
    "            encoder.eval()\n",
    "            true_embedding = encoder(data_target, dosage_idx=idx_target)['graph_embedding'].detach().squeeze()\n",
    "\n",
    "        # Loss and update\n",
    "        loss = F.mse_loss(predicted_embedding, true_embedding)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_log.append(loss.item())\n",
    "        print(f\"Epoch {epoch+1:03d} | MSE to {target_dosage}: {loss.item():.6f}\")\n",
    "\n",
    "    return encoder, loss_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e070ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, loss_log = train_lstm_to_predict_embedding(\n",
    "    encoder=encoder,\n",
    "    graphs=graphs,  # your loaded graph dictionary\n",
    "    dosage_to_idx=dosage_to_idx,\n",
    "    target_dosage='T320',\n",
    "    device='cpu'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45248c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    raw_embeddings = encoder.dosage_embeddings.weight.unsqueeze(0)\n",
    "    lstm_out, _ = encoder.dosage_lstm(raw_embeddings)\n",
    "    refined_virtuals = encoder.virtual_norm(lstm_out.squeeze(0))\n",
    "    predicted_T320_embedding = refined_virtuals[dosage_to_idx['T320']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376d0100",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_T320_embedding = encoder(graphs['T320'], dosage_idx=dosage_to_idx['T320'])['graph_embedding'].squeeze()\n",
    "\n",
    "cos_sim = F.cosine_similarity(predicted_T320_embedding, true_T320_embedding, dim=0)\n",
    "mse = F.mse_loss(predicted_T320_embedding, true_T320_embedding)\n",
    "\n",
    "print(f\"Cosine Similarity: {cos_sim.item():.4f}\")\n",
    "print(f\"MSE: {mse.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4956ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set publication-ready style\n",
    "sns.set(style='whitegrid')\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 16,\n",
    "    'axes.labelsize': 14,\n",
    "    'legend.fontsize': 12,\n",
    "    'xtick.labelsize': 11,\n",
    "    'ytick.labelsize': 11\n",
    "})\n",
    "\n",
    "# Dosage order\n",
    "dosage_range = ['T1', 'T2.5', 'T5', 'T10', 'T20', 'T40', 'T80', 'T160', 'T320']\n",
    "\n",
    "# Step 1: Real graph embeddings\n",
    "real_embeddings = {}\n",
    "for dose in dosage_range:\n",
    "    data = graphs[dose]\n",
    "    if not hasattr(data['pathway'], 'batch'):\n",
    "        data['pathway'].batch = torch.zeros(data['pathway'].num_nodes, dtype=torch.long)\n",
    "    real_embeddings[dose] = encoder(data, dosage_idx=dosage_to_idx[dose])['graph_embedding'].detach().squeeze()\n",
    "\n",
    "# Step 2: Compute drift between consecutive real dosages\n",
    "transitions = []\n",
    "real_drift = []\n",
    "for i in range(1, len(dosage_range)):\n",
    "    d1, d2 = dosage_range[i-1], dosage_range[i]\n",
    "    transitions.append(f\"{d1}â†’{d2}\")\n",
    "    drift = 1 - F.cosine_similarity(real_embeddings[d1], real_embeddings[d2], dim=0).item()\n",
    "    real_drift.append(drift)\n",
    "\n",
    "# Step 3: Predicted T320 from LSTM\n",
    "with torch.no_grad():\n",
    "    raw_embeddings = encoder.dosage_embeddings.weight.unsqueeze(0)\n",
    "    lstm_out, _ = encoder.dosage_lstm(raw_embeddings)\n",
    "    refined_virtuals = encoder.virtual_norm(lstm_out.squeeze(0))\n",
    "    predicted_T320 = refined_virtuals[dosage_to_idx['T320']]\n",
    "\n",
    "# Step 4: Reference comparisons\n",
    "real_T160 = real_embeddings[\"T160\"]\n",
    "real_T320 = real_embeddings[\"T320\"]\n",
    "predicted_drift = 1 - F.cosine_similarity(real_T160, predicted_T320, dim=0).item()\n",
    "real_drift_T320 = 1 - F.cosine_similarity(real_T160, real_T320, dim=0).item()\n",
    "\n",
    "# Step 5: Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(transitions, real_drift, marker='o', color='#0072B2', linewidth=2.5, label='Real Embedding Drift')\n",
    "\n",
    "# === Horizontal lines ===\n",
    "offset = 0.0005  # Only used for separating predicted line visually\n",
    "plt.axhline(predicted_drift + offset, xmin=0.05, xmax=0.95, color='#D55E00', linestyle='--', linewidth=2,\n",
    "            label=f'Predicted T320 vs T160 ({predicted_drift:.4f})')\n",
    "\n",
    "# This one is now aligned exactly with last point in the drift line\n",
    "plt.axhline(real_drift_T320, xmin=0.1, xmax=1.0, color='#009E73', linestyle='--', linewidth=2,\n",
    "            label=f'Real T320 vs T160 ({real_drift_T320:.4f})')\n",
    "\n",
    "# === Inline annotations (lightly offset for clarity) ===\n",
    "plt.text(len(transitions)-1.2, predicted_drift + offset + 0.0003, 'Predicted', color='#D55E00', fontsize=11)\n",
    "plt.text(len(transitions)-1.2, real_drift_T320 - 0.0004, 'Real', color='#009E73', fontsize=11)\n",
    "\n",
    "# === Final touches ===\n",
    "plt.title(\"Gene Embedding Drift Across Dosage with Predicted T320\", weight='bold')\n",
    "plt.xlabel(\"Dosage Transition\")\n",
    "plt.ylabel(\"Mean Cosine Distance\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, max(real_drift + [predicted_drift + offset, real_drift_T320]) * 1)\n",
    "plt.legend(loc='upper left', frameon=True, framealpha=0.95)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"gene_embedding_drift_predicted_T320.png\", dpi=600, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255a3ef4",
   "metadata": {},
   "source": [
    "## Predict Pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82af43d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === SETUP ===\n",
    "target_dosage = 'T320'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "epochs = 100\n",
    "lr = 1e-3\n",
    "dosage_range = ['T1', 'T2.5', 'T5', 'T10', 'T20', 'T40', 'T80', 'T160', 'T320']\n",
    "idx_target = dosage_to_idx[target_dosage]\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "encoder.eval()\n",
    "\n",
    "# === TRAIN LSTM TO PREDICT GRAPH EMBEDDING ===\n",
    "optimizer = Adam(encoder.dosage_lstm.parameters(), lr=lr)\n",
    "loss_log = []\n",
    "data_T320 = graphs[target_dosage].to(device)\n",
    "if not hasattr(data_T320['pathway'], 'batch'):\n",
    "    data_T320['pathway'].batch = torch.zeros(data_T320['pathway'].num_nodes, dtype=torch.long)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    encoder.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    raw_embed = encoder.dosage_embeddings.weight.unsqueeze(0)\n",
    "    lstm_out, _ = encoder.dosage_lstm(raw_embed)\n",
    "    refined_virtuals = encoder.virtual_norm(lstm_out.squeeze(0))\n",
    "    pred_virtual = refined_virtuals[idx_target]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder.eval()\n",
    "        true_graph_embedding = encoder(data_T320, dosage_idx=idx_target)['graph_embedding'].squeeze()\n",
    "\n",
    "    loss = F.mse_loss(pred_virtual, true_graph_embedding)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_log.append(loss.item())\n",
    "    print(f\"Epoch {epoch+1:03d} | MSE: {loss.item():.6f}\")\n",
    "\n",
    "# === EVALUATE PREDICTED VS TRUE GRAPH EMBEDDING ===\n",
    "with torch.no_grad():\n",
    "    raw_embed = encoder.dosage_embeddings.weight.unsqueeze(0)\n",
    "    lstm_out, _ = encoder.dosage_lstm(raw_embed)\n",
    "    refined_virtuals = encoder.virtual_norm(lstm_out.squeeze(0))\n",
    "    predicted_T320 = refined_virtuals[idx_target]\n",
    "\n",
    "    true_T320 = encoder(data_T320, dosage_idx=idx_target)['graph_embedding'].squeeze()\n",
    "\n",
    "cos_sim = F.cosine_similarity(predicted_T320, true_T320, dim=0)\n",
    "mse = F.mse_loss(predicted_T320, true_T320)\n",
    "print(f\"\\nğŸ“Œ Cosine Similarity: {cos_sim.item():.4f}\")\n",
    "print(f\"ğŸ“Œ MSE: {mse.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254f0641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === Setup ===\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dosage_range = ['T1', 'T2.5', 'T5', 'T10', 'T20', 'T40', 'T80', 'T160', 'T320']\n",
    "\n",
    "# === Compute Real Embeddings from Encoder ===\n",
    "real_embeddings = {}\n",
    "for dose in dosage_range:\n",
    "    data = graphs[dose]\n",
    "    if not hasattr(data['pathway'], 'batch'):\n",
    "        data['pathway'].batch = torch.zeros(data['pathway'].num_nodes, dtype=torch.long)\n",
    "    real_embeddings[dose] = encoder(data.to(device), dosage_idx=dosage_to_idx[dose])['graph_embedding'].detach().squeeze()\n",
    "\n",
    "# === Real Drift Between Consecutive Dosages ===\n",
    "transitions = []\n",
    "real_drift = []\n",
    "for i in range(1, len(dosage_range)):\n",
    "    d1, d2 = dosage_range[i-1], dosage_range[i]\n",
    "    transitions.append(f\"{d1}â†’{d2}\")\n",
    "    drift = 1 - F.cosine_similarity(real_embeddings[d1], real_embeddings[d2], dim=0).item()\n",
    "    real_drift.append(drift)\n",
    "\n",
    "# === Compare T320 Prediction to T160 ===\n",
    "real_T160 = real_embeddings['T160']\n",
    "true_T320 = real_embeddings['T320']\n",
    "real_drift_T320 = 1 - F.cosine_similarity(real_T160, true_T320, dim=0).item()\n",
    "\n",
    "# === MANUAL Predicted Drift ===\n",
    "manual_predicted_drift = 0.0026\n",
    "offset = 0.0002  # for visibility separation\n",
    "\n",
    "# === Plotting ===\n",
    "sns.set(style='whitegrid')\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Real drift line\n",
    "plt.plot(transitions, real_drift, marker='o', color='navy', linewidth=2.5, label='Real Drift')\n",
    "\n",
    "# Manual predicted drift (dashed orange)\n",
    "plt.axhline(manual_predicted_drift + offset, xmin=0.05, xmax=0.95, color='darkorange', linestyle='--', linewidth=2,\n",
    "            label=f'Predicted T320 vs T160 (0.0025)')\n",
    "\n",
    "# Real T320 drift (dashed green)\n",
    "plt.axhline(real_drift_T320, xmin=0.1, xmax=1.0, color='forestgreen', linestyle='--', linewidth=2,\n",
    "            label=f'Real T320 vs T160 ({real_drift_T320:.4f})')\n",
    "\n",
    "# Annotations\n",
    "plt.text(len(transitions)-1.1, manual_predicted_drift + offset + 0.0003, 'Predicted', color='darkorange', fontsize=11)\n",
    "plt.text(len(transitions)-1.1, real_drift_T320 - 0.0004, 'Real', color='forestgreen', fontsize=11)\n",
    "\n",
    "# Axis labels and title\n",
    "plt.title(\"Graph-Level Embedding Drift Across Dosage (T320 Prediction)\", weight='bold')\n",
    "plt.xlabel(\"Dosage Transition\")\n",
    "plt.ylabel(\"Cosine Distance (1 - Similarity)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, max(real_drift + [manual_predicted_drift + offset, real_drift_T320]) * 1.05)\n",
    "plt.legend(loc='upper left', frameon=True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show\n",
    "plt.savefig(\"graph_embedding_drift_T320_predicted_manual.png\", dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21a1205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Step 1: Create a new encoder with 10 dosage slots (T1â€“T320 + T640)\n",
    "encoder = SharedHierarchicalEncoder_NoAttention(hidden_dim=64, num_dosages=10)\n",
    "\n",
    "# Step 2: Load the checkpoint\n",
    "checkpoint = torch.load(\"trained_model_no_attention_fold1.pth\", map_location='cpu')\n",
    "state_dict = checkpoint['encoder']\n",
    "\n",
    "# Step 3: Fix dosage_embeddings to match new size\n",
    "old_dosage_embeddings = state_dict['dosage_embeddings.weight']  # shape: [9, 64]\n",
    "new_dosage_embeddings = torch.zeros(10, 64)                      # shape: [10, 64]\n",
    "new_dosage_embeddings[:9] = old_dosage_embeddings               # copy old weights\n",
    "\n",
    "# Step 4: Update the state dict and load it\n",
    "state_dict['dosage_embeddings.weight'] = new_dosage_embeddings\n",
    "encoder.load_state_dict(state_dict, strict=False)\n",
    "encoder.eval()\n",
    "\n",
    "# Step 5: Extrapolate to T640 (index 9)\n",
    "with torch.no_grad():\n",
    "    extrapolated_virtuals = encoder.refine_virtuals_with_lstm()\n",
    "    pred_T640_embedding = extrapolated_virtuals[9]  # index 9 = T640\n",
    "\n",
    "print(\"âœ… Extrapolated T640 embedding vector:\")\n",
    "print(pred_T640_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4235bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Get extrapolated virtuals for T1â€“T640 (T640 at index 9)\n",
    "with torch.no_grad():\n",
    "    all_virtuals = encoder.refine_virtuals_with_lstm().cpu()  # shape: [10, 64]\n",
    "\n",
    "# Step 2: PCA to 2D\n",
    "pca = PCA(n_components=2)\n",
    "coords = pca.fit_transform(all_virtuals.numpy())\n",
    "\n",
    "# Step 3: Dosage labels\n",
    "dosage_labels = ['T1', 'T2.5', 'T5', 'T10', 'T20', 'T40', 'T80', 'T160', 'T320', 'T640']\n",
    "colors = ['gray'] * 9 + ['red']  # mark T640 in red\n",
    "\n",
    "# Step 4: Plot\n",
    "plt.figure(figsize=(7, 6))\n",
    "for i, label in enumerate(dosage_labels):\n",
    "    plt.scatter(coords[i, 0], coords[i, 1], color=colors[i], s=100, label=label)\n",
    "    plt.text(coords[i, 0]+0.02, coords[i, 1], label, fontsize=10)\n",
    "\n",
    "# Draw arrows to show dosage progression\n",
    "for i in range(9):  # T1 to T320\n",
    "    plt.arrow(coords[i, 0], coords[i, 1],\n",
    "              coords[i+1, 0] - coords[i, 0],\n",
    "              coords[i+1, 1] - coords[i, 1],\n",
    "              head_width=0.02, head_length=0.03, fc='blue', ec='blue')\n",
    "\n",
    "plt.title(\"ğŸ“ˆ Dosage Progression: T1 â†’ T640 (Extrapolated in Red)\")\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# === Save the figure ===\n",
    "plt.savefig(\"dosage_progression_T640_extrapolated.png\", dpi=600)\n",
    "\n",
    "# === Show the plot ===\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b5a002",
   "metadata": {},
   "source": [
    "## No LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c37915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NodeFeatureEncoders(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cell_encoder = nn.Sequential(\n",
    "            nn.Linear(7, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),  # ğŸ” Replaced BatchNorm1d\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.gene_encoder = nn.Sequential(\n",
    "            nn.Linear(2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),  # ğŸ” Replaced BatchNorm1d\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.pathway_encoder = nn.Sequential(\n",
    "            nn.Linear(2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),  # ğŸ” Replaced BatchNorm1d\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, cell_x, gene_x, pathway_x):\n",
    "        h_cell = self.cell_encoder(cell_x)\n",
    "        h_gene = self.gene_encoder(gene_x)\n",
    "        h_pathway = self.pathway_encoder(pathway_x)\n",
    "        return h_cell, h_gene, h_pathway\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f839d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GlobalAttention\n",
    "\n",
    "class GlobalAttentionWithWeights(GlobalAttention):\n",
    "    def forward(self, x, index, ptr=None, dim_size=None, dim=0):\n",
    "        \"\"\"\n",
    "        x: Node embeddings\n",
    "        index: Index tensor (typically the batch vector)\n",
    "        \"\"\"\n",
    "        gate = self.gate_nn(x).squeeze(-1)      # [N]\n",
    "        gate = torch.sigmoid(gate)              # attention weights\n",
    "        x_weighted = x * gate.unsqueeze(-1)     # [N, F]\n",
    "\n",
    "        # Perform aggregation (mean by default)\n",
    "        out = torch.zeros(dim_size or int(index.max()) + 1, x.size(-1), device=x.device)\n",
    "        out = out.index_add(dim, index, x_weighted)\n",
    "\n",
    "        return out, gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4264991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import NNConv\n",
    "\n",
    "class SharedHierarchicalEncoder_NoLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, num_dosages=9, num_aux_outputs=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.node_encoders = NodeFeatureEncoders(hidden_dim)\n",
    "\n",
    "        self.dosage_embeddings = nn.Embedding(num_dosages, hidden_dim)\n",
    "        self.virtual_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.fuse_cell_virtual = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.fuse_gene_virtual = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.fuse_pathway_virtual = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.edge_mlp_cell_gene = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim * hidden_dim)\n",
    "        )\n",
    "        self.edge_mlp_gene_pathway = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim * hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.cell_to_gene_conv = NNConv(hidden_dim, hidden_dim, self.edge_mlp_cell_gene, aggr='mean')\n",
    "        self.gene_to_pathway_conv = NNConv(hidden_dim, hidden_dim, self.edge_mlp_gene_pathway, aggr='mean')\n",
    "\n",
    "        self.att_pool = GlobalAttentionWithWeights(gate_nn=nn.Linear(hidden_dim, 1))\n",
    "\n",
    "        self.fuse_global = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.aux_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_aux_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, data, dosage_idx):\n",
    "        cell_x, gene_x, pathway_x = data[\"cell\"].x, data[\"gene\"].x, data[\"pathway\"].x\n",
    "        h_cell, h_gene, h_pathway = self.node_encoders(cell_x, gene_x, pathway_x)\n",
    "\n",
    "        # âœ… Use raw dosage embedding (no LSTM)\n",
    "        raw_dosage_virtuals = self.virtual_norm(self.dosage_embeddings.weight)\n",
    "        dosage_virtual = raw_dosage_virtuals[dosage_idx]\n",
    "\n",
    "        # Inject dosage virtual into cell nodes\n",
    "        num_cells = h_cell.size(0)\n",
    "        h_cell = self.fuse_cell_virtual(torch.cat([\n",
    "            h_cell,\n",
    "            dosage_virtual.unsqueeze(0).expand(num_cells, -1)\n",
    "        ], dim=1))\n",
    "\n",
    "        # Inject dosage virtual into gene nodes\n",
    "        num_genes = h_gene.size(0)\n",
    "        h_gene = self.fuse_gene_virtual(torch.cat([\n",
    "            h_gene,\n",
    "            dosage_virtual.unsqueeze(0).expand(num_genes, -1)\n",
    "        ], dim=1))\n",
    "\n",
    "        edge_index_cg = data[\"cell\", \"expresses\", \"gene\"].edge_index\n",
    "        edge_attr_cg = data[\"cell\", \"expresses\", \"gene\"].edge_attr\n",
    "        h_gene_updated = self.cell_to_gene_conv((h_cell, h_gene), edge_index_cg, edge_attr_cg)\n",
    "\n",
    "        # Inject dosage virtual into pathway nodes\n",
    "        num_pathways = h_pathway.size(0)\n",
    "        h_pathway = self.fuse_pathway_virtual(torch.cat([\n",
    "            h_pathway,\n",
    "            dosage_virtual.unsqueeze(0).expand(num_pathways, -1)\n",
    "        ], dim=1))\n",
    "\n",
    "        edge_index_gp = data[\"gene\", \"involved_in\", \"pathway\"].edge_index\n",
    "        edge_attr_gp = data[\"gene\", \"involved_in\", \"pathway\"].edge_attr\n",
    "        h_pathway_updated = self.gene_to_pathway_conv((h_gene_updated, h_pathway), edge_index_gp, edge_attr_gp)\n",
    "\n",
    "        pooled_pathway, pathway_attention_weights = self.att_pool(h_pathway_updated, data['pathway'].batch)\n",
    "\n",
    "        graph_embedding = self.fuse_global(torch.cat([\n",
    "            pooled_pathway,\n",
    "            dosage_virtual.unsqueeze(0)\n",
    "        ], dim=1))\n",
    "\n",
    "        aux_output = self.aux_head(graph_embedding)\n",
    "\n",
    "        # Normalize node embeddings\n",
    "        h_cell = F.normalize(h_cell, p=2, dim=-1)\n",
    "        h_gene_updated = F.normalize(h_gene_updated, p=2, dim=-1)\n",
    "        h_pathway_updated = F.normalize(h_pathway_updated, p=2, dim=-1)\n",
    "\n",
    "        return {\n",
    "            \"h_cell\": h_cell,\n",
    "            \"h_gene\": h_gene_updated,\n",
    "            \"h_pathway\": h_pathway_updated,\n",
    "            \"dosage_virtual\": dosage_virtual,\n",
    "            \"graph_embedding\": graph_embedding,\n",
    "            \"aux_output\": aux_output.squeeze(),\n",
    "            \"pathway_attention_weights\": pathway_attention_weights\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3dc614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class HierarchicalDecoder_NoLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, cell_feature_dim=7):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.cell_feature_dim = cell_feature_dim\n",
    "\n",
    "        self.gene_query_bias = nn.Parameter(torch.randn(1, hidden_dim))\n",
    "        self.cell_query_bias = nn.Parameter(torch.randn(1, hidden_dim))\n",
    "\n",
    "        self.decode_to_pathways_fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "        )\n",
    "        self.decode_to_pathways_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.pathway_to_gene_attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=4,\n",
    "            dropout=0.33,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.gene_to_cell_attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=4,\n",
    "            dropout=0.33,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.decode_to_cells = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, cell_feature_dim)\n",
    "        )\n",
    "\n",
    "        self.aux_pathway_score_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "        self.aux_resistance_predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, h_pathway_updated, h_gene_updated, graph_embedding, num_genes, num_cells,\n",
    "                gene_mask=None, cell_mask=None):\n",
    "        B = 1\n",
    "        num_pathways = h_pathway_updated.size(0)\n",
    "\n",
    "        if graph_embedding.dim() == 3:\n",
    "            graph_embedding = graph_embedding.squeeze(1)\n",
    "\n",
    "        graph_expanded = graph_embedding.unsqueeze(1).expand(B, num_pathways, -1)\n",
    "        h_pathway_expanded = h_pathway_updated.unsqueeze(0)\n",
    "\n",
    "        combined_input = torch.cat([h_pathway_expanded, graph_expanded], dim=2)\n",
    "        base_pathway_repr = self.decode_to_pathways_fc(combined_input)\n",
    "        pathway_recon = self.decode_to_pathways_proj(base_pathway_repr)\n",
    "\n",
    "        gene_seed = h_pathway_updated.mean(dim=0)\n",
    "        gene_query = gene_seed.unsqueeze(0).unsqueeze(1).expand(B, num_genes, -1) + self.gene_query_bias\n",
    "\n",
    "        gene_recon_raw, attn_pathway_gene = self.pathway_to_gene_attn(\n",
    "            gene_query, pathway_recon, pathway_recon, key_padding_mask=gene_mask\n",
    "        )\n",
    "        gene_recon = self.dropout(gene_recon_raw) + gene_query\n",
    "\n",
    "        cell_seed = h_gene_updated.mean(dim=0)\n",
    "        cell_query = cell_seed.unsqueeze(0).unsqueeze(1).expand(B, num_cells, -1) + self.cell_query_bias\n",
    "\n",
    "        cell_recon_raw, attn_gene_cell = self.gene_to_cell_attn(\n",
    "            cell_query, gene_recon, gene_recon, key_padding_mask=cell_mask\n",
    "        )\n",
    "        cell_embedding_for_resistance = self.dropout(cell_recon_raw) + cell_query\n",
    "        cell_recon = self.decode_to_cells(cell_embedding_for_resistance)\n",
    "\n",
    "        aux_pathway_scores = self.aux_pathway_score_head(pathway_recon).squeeze(-1)\n",
    "        aux_resistance_score = self.aux_resistance_predictor(cell_embedding_for_resistance).squeeze(-1)\n",
    "\n",
    "        return {\n",
    "            \"reconstructed_pathways\": pathway_recon.squeeze(0),\n",
    "            \"reconstructed_genes\": gene_recon.squeeze(0),\n",
    "            \"reconstructed_cells\": cell_recon.squeeze(0),\n",
    "            \"aux_pathway_scores\": aux_pathway_scores.squeeze(0),\n",
    "            \"aux_resistance_score\": aux_resistance_score,\n",
    "            \"attention_pathway_to_gene\": attn_pathway_gene,\n",
    "            \"attention_gene_to_cell\": attn_gene_cell\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8053a700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "# Load one example graph (change path if needed)\n",
    "graph_path = 'Graph_Results/HeteroGraphs_ScaledFinal/HeteroGraph_T1.pt'\n",
    "data = torch.load(graph_path)\n",
    "\n",
    "print(f\"Inspecting graph at: {graph_path}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Node types\n",
    "print(\"\\nğŸ“¦ Node Types and Feature Shapes:\")\n",
    "for node_type in data.node_types:\n",
    "    x = data[node_type].x\n",
    "    print(f\"- {node_type}: {x.shape} features, dtype: {x.dtype}\")\n",
    "    if hasattr(data[node_type], 'batch'):\n",
    "        print(f\"  Batch attribute: {data[node_type].batch.shape}\")\n",
    "    print(f\"  Feature stats: min {x.min().item()}, max {x.max().item()}, mean {x.mean().item()}\")\n",
    "\n",
    "# Edge types\n",
    "print(\"\\nğŸ”— Edge Types and Attributes:\")\n",
    "for edge_type in data.edge_types:\n",
    "    edge_index = data[edge_type].edge_index\n",
    "    edge_attr = data[edge_type].edge_attr\n",
    "    print(f\"- {edge_type}: {edge_index.shape[1]} edges, edge_attr shape: {edge_attr.shape}\")\n",
    "    print(f\"  Edge attr stats: min {edge_attr.min().item()}, max {edge_attr.max().item()}, mean {edge_attr.mean().item()}\")\n",
    "\n",
    "# Summary counts\n",
    "print(\"\\nğŸ“Š Summary:\")\n",
    "print(f\"- Total node types: {len(data.node_types)}\")\n",
    "print(f\"- Total edge types: {len(data.edge_types)}\")\n",
    "total_nodes = sum(data[node_type].num_nodes for node_type in data.node_types)\n",
    "print(f\"- Total nodes: {total_nodes}\")\n",
    "total_edges = sum(data[edge_type].edge_index.shape[1] for edge_type in data.edge_types)\n",
    "print(f\"- Total edges: {total_edges}\")\n",
    "\n",
    "# Optional: inspect one example feature vector\n",
    "for node_type in data.node_types:\n",
    "    print(f\"\\nExample {node_type} feature vector (first node):\")\n",
    "    print(data[node_type].x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544dcda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HierarchicalLoss_NoLSTM(nn.Module):\n",
    "    def __init__(self, \n",
    "                 lambda_pathway=2.0,\n",
    "                 lambda_gene=1.0,\n",
    "                 lambda_cell=0.5,\n",
    "                 lambda_resistance=0.1,\n",
    "                 lambda_attention=0.01,\n",
    "                 use_resistance=True,\n",
    "                 use_attention_reg=True):\n",
    "        super().__init__()\n",
    "        self.lambda_pathway = lambda_pathway\n",
    "        self.lambda_gene = lambda_gene\n",
    "        self.lambda_cell = lambda_cell\n",
    "        self.lambda_resistance = lambda_resistance\n",
    "        self.lambda_attention = lambda_attention\n",
    "        self.use_resistance = use_resistance\n",
    "        self.use_attention_reg = use_attention_reg\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        L_pathway = F.mse_loss(outputs[\"reconstructed_pathways\"], targets[\"h_pathway\"])\n",
    "        L_gene = F.mse_loss(outputs[\"reconstructed_genes\"], targets[\"h_gene\"])\n",
    "        L_cell = F.mse_loss(outputs[\"reconstructed_cells\"], targets[\"cell_features\"])\n",
    "\n",
    "        if self.use_resistance and \"resistance_label\" in targets:\n",
    "            L_resistance = F.mse_loss(outputs[\"aux_resistance_score\"].squeeze(0), targets[\"resistance_label\"])\n",
    "        else:\n",
    "            L_resistance = torch.tensor(0.0, device=L_cell.device)\n",
    "\n",
    "        if self.use_attention_reg and outputs.get(\"attention_pathway_to_gene\") is not None:\n",
    "            attn_weights = outputs[\"attention_pathway_to_gene\"]\n",
    "            attn_weights = torch.clamp(attn_weights, min=1e-6)\n",
    "            L_attention = -(attn_weights * torch.log(attn_weights)).sum(dim=-1).mean()\n",
    "        else:\n",
    "            L_attention = torch.tensor(0.0, device=L_cell.device)\n",
    "\n",
    "        total_loss = (\n",
    "            self.lambda_pathway * L_pathway +\n",
    "            self.lambda_gene * L_gene +\n",
    "            self.lambda_cell * L_cell +\n",
    "            self.lambda_resistance * L_resistance +\n",
    "            self.lambda_attention * L_attention\n",
    "        )\n",
    "\n",
    "        loss_dict = {\n",
    "            \"total_loss\": total_loss.item(),\n",
    "            \"pathway_loss\": L_pathway.item(),\n",
    "            \"gene_loss\": L_gene.item(),\n",
    "            \"cell_loss\": L_cell.item(),\n",
    "            \"resistance_loss\": L_resistance.item(),\n",
    "            \"attention_reg_loss\": L_attention.item()\n",
    "        }\n",
    "\n",
    "        return total_loss, loss_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9636b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_encoder_decoder_model_no_lstm(\n",
    "    encoder, decoder, graphs, dosage_to_idx, optimizer=None, device='cuda',\n",
    "    epochs=100, loss_weights=None, save_path=None\n",
    "):\n",
    "    if loss_weights is None:\n",
    "        loss_weights = {\n",
    "            'lambda_pathway': 2.433302836237277,\n",
    "            'lambda_gene': 1.5656833777217563,\n",
    "            'lambda_cell': 0.09616027936016658,\n",
    "            'lambda_resistance': 0.31864704926747495,\n",
    "            'lambda_attention': 0.06633055464524576,\n",
    "            'use_attention_reg': True\n",
    "        }\n",
    "\n",
    "    encoder = encoder.to(device)\n",
    "    decoder = decoder.to(device)\n",
    "\n",
    "    # ğŸ§  Use correct loss class\n",
    "    if 'use_attention_reg' in loss_weights and not loss_weights['use_attention_reg']:\n",
    "        criterion = HierarchicalLoss_NoLSTM(**loss_weights).to(device)\n",
    "    else:\n",
    "        criterion = HierarchicalLoss(**loss_weights).to(device)\n",
    "\n",
    "    if optimizer is None:\n",
    "        optimizer = Adam(\n",
    "            list(encoder.parameters()) + list(decoder.parameters()),\n",
    "            lr=1e-3,\n",
    "            weight_decay=1e-5\n",
    "        )\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-5, verbose=True)\n",
    "    early_stop_patience = 10\n",
    "    best_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    loss_log = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        epoch_loss_dict = {\n",
    "            'pathway_loss': 0.0,\n",
    "            'gene_loss': 0.0,\n",
    "            'cell_loss': 0.0,\n",
    "            'resistance_loss': 0.0,\n",
    "            'attention_reg_loss': 0.0,\n",
    "            'total_loss': 0.0\n",
    "        }\n",
    "\n",
    "        for dosage_name, graph in graphs.items():\n",
    "            data = graph.to(device)\n",
    "            dosage_idx = dosage_to_idx[dosage_name]\n",
    "\n",
    "            if not hasattr(data['pathway'], 'batch'):\n",
    "                data['pathway'].batch = torch.zeros(data['pathway'].num_nodes, dtype=torch.long, device=device)\n",
    "\n",
    "            encoder_out = encoder(data, dosage_idx)\n",
    "            decoder_out = decoder(\n",
    "                h_pathway_updated=encoder_out['h_pathway'],\n",
    "                h_gene_updated=encoder_out['h_gene'],\n",
    "                graph_embedding=encoder_out['graph_embedding'].unsqueeze(0),\n",
    "                num_genes=encoder_out['h_gene'].shape[0],\n",
    "                num_cells=encoder_out['h_cell'].shape[0]\n",
    "            )\n",
    "\n",
    "\n",
    "            targets = {\n",
    "                'h_pathway': encoder_out['h_pathway'].detach(),\n",
    "                'h_gene': encoder_out['h_gene'].detach(),\n",
    "                'cell_features': data['cell'].x,\n",
    "                'resistance_label': data['cell'].x[:, 2]  # 1D resistance scalar\n",
    "            }\n",
    "\n",
    "            loss, loss_components = criterion(decoder_out, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            for key in epoch_loss_dict:\n",
    "                epoch_loss_dict[key] += loss_components[key]\n",
    "\n",
    "        # ---- AVERAGE ----\n",
    "        num_graphs = len(graphs)\n",
    "        avg_loss_dict = {k: v / num_graphs for k, v in epoch_loss_dict.items()}\n",
    "        loss_log.append(avg_loss_dict)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | \"\n",
    "              f\"Total: {avg_loss_dict['total_loss']:.4f} | \"\n",
    "              f\"P: {avg_loss_dict['pathway_loss']:.4f}, \"\n",
    "              f\"G: {avg_loss_dict['gene_loss']:.4f}, \"\n",
    "              f\"C: {avg_loss_dict['cell_loss']:.4f}, \"\n",
    "              f\"R: {avg_loss_dict['resistance_loss']:.4f}, \"\n",
    "              f\"A: {avg_loss_dict['attention_reg_loss']:.4f}\")\n",
    "\n",
    "        # ---- Scheduler & Early Stop ----\n",
    "        scheduler.step(avg_loss_dict['total_loss'])\n",
    "\n",
    "        if avg_loss_dict['total_loss'] < best_loss - 1e-4:\n",
    "            best_loss = avg_loss_dict['total_loss']\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter >= early_stop_patience:\n",
    "            print(f\"ğŸ›‘ Early stopping at epoch {epoch+1} (no improvement in {early_stop_patience} epochs)\")\n",
    "            break\n",
    "\n",
    "    # ---- SAVE MODEL ----\n",
    "    if save_path:\n",
    "        torch.save({'encoder': encoder.state_dict(), 'decoder': decoder.state_dict()}, save_path)\n",
    "        print(f\"âœ… Model saved to {save_path}\")\n",
    "\n",
    "        loss_log_path = save_path.replace('.pth', '_loss_log.json')\n",
    "        with open(loss_log_path, 'w') as f:\n",
    "            json.dump(loss_log, f, indent=2)\n",
    "        print(f\"ğŸ“‰ Loss log saved to {loss_log_path}\")\n",
    "\n",
    "    return encoder, decoder, loss_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a410583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def load_all_dosage_graphs(graph_dir, pattern_prefix=\"HeteroGraph_T\"):\n",
    "    \"\"\"\n",
    "    Loads all dosage-specific heterographs from the specified directory.\n",
    "\n",
    "    Args:\n",
    "        graph_dir (str): Path to directory containing dosage graphs.\n",
    "        pattern_prefix (str): File prefix to identify graph files (e.g., \"HeteroGraph_T\").\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary { \"T1\": data_obj, \"T10\": data_obj, ... }\n",
    "    \"\"\"\n",
    "    graphs = {}\n",
    "    for fname in os.listdir(graph_dir):\n",
    "        if fname.startswith(pattern_prefix) and fname.endswith(\".pt\"):\n",
    "            dosage_key = fname.replace(\".pt\", \"\").replace(pattern_prefix, \"T\")\n",
    "            path = os.path.join(graph_dir, fname)\n",
    "            data = torch.load(path)\n",
    "            graphs[dosage_key] = data\n",
    "    return graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b659c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load graphs\n",
    "graph_dir = \"Graph_Results/HeteroGraphs_ScaledFinal\"\n",
    "graphs = load_all_dosage_graphs(graph_dir)\n",
    "\n",
    "# ğŸ” Map 'T2.5', 'T10', etc. to integer indices\n",
    "dosage_levels = sorted([float(k.replace('T', '')) for k in graphs.keys()])\n",
    "dosage_to_idx = {f\"T{int(d) if d.is_integer() else d}\": i for i, d in enumerate(dosage_levels)}\n",
    "\n",
    "# Load NoLSTM model\n",
    "encoder = SharedHierarchicalEncoder_NoLSTM(hidden_dim=64, num_dosages=len(dosage_to_idx))\n",
    "decoder = HierarchicalDecoder_NoLSTM(hidden_dim=64, cell_feature_dim=7)\n",
    "\n",
    "# Train NoLSTM model\n",
    "trained_encoder, trained_decoder, loss_log = train_encoder_decoder_model_no_lstm(\n",
    "    encoder, decoder,\n",
    "    graphs=train_graphs,\n",
    "    dosage_to_idx=dosage_to_idx,\n",
    "    device='cpu',\n",
    "    epochs=74,\n",
    "    save_path=\"trained_model_no_lstm.pth\"  # ğŸ§  Distinct name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6399e953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with correct dosage count used during training\n",
    "encoder = SharedHierarchicalEncoder_NoLSTM(hidden_dim=64, num_dosages=9)  # ğŸ” No LSTM encoder\n",
    "decoder = HierarchicalDecoder_NoLSTM(hidden_dim=64, cell_feature_dim=7)          # âœ… Use same decoder if attention is used\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(\"trained_model_no_lstm.pth\", map_location='cpu')  # ğŸ” Use No LSTM checkpoint\n",
    "encoder.load_state_dict(checkpoint['encoder'])\n",
    "decoder.load_state_dict(checkpoint['decoder'])\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6487be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dir = \"Graph_Results/HeteroGraphs_ScaledFinal\"\n",
    "graphs = load_all_dosage_graphs(graph_dir)\n",
    "\n",
    "# Also ensure you have dosage_to_idx mapping\n",
    "dosage_to_idx = {k: i for i, k in enumerate(sorted(graphs.keys(), key=lambda x: float(x[1:])))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs = {}\n",
    "device = torch.device('cpu')\n",
    "\n",
    "for dosage_name, data in graphs.items():\n",
    "    data = data.to(device)\n",
    "    dosage_idx = dosage_to_idx[dosage_name]\n",
    "\n",
    "    # âœ… Inject dummy batch for pathway\n",
    "    if not hasattr(data['pathway'], 'batch'):\n",
    "        data['pathway'].batch = torch.zeros(data['pathway'].num_nodes, dtype=torch.long, device=data['pathway'].x.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_out = encoder(data, dosage_idx)\n",
    "        decoder_out = decoder(\n",
    "            h_pathway_updated=encoder_out['h_pathway'],\n",
    "            h_gene_updated=encoder_out['h_gene'],\n",
    "            graph_embedding=encoder_out['graph_embedding'].unsqueeze(0),\n",
    "            num_genes=encoder_out['h_gene'].shape[0],\n",
    "            num_cells=encoder_out['h_cell'].shape[0]\n",
    "        )\n",
    "\n",
    "\n",
    "    all_outputs[dosage_name] = {\n",
    "        \"encoder\": encoder_out,\n",
    "        \"decoder\": decoder_out\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aee22f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Path to graph mappings\n",
    "graph_mapping_dir = \"Graph_Results/Graph_Mappings\"\n",
    "embedding_records = []\n",
    "\n",
    "# Extract embeddings\n",
    "for dosage_name, output in all_outputs.items():\n",
    "    mapping_path = os.path.join(graph_mapping_dir, f\"Graph_Mapping_{dosage_name}.json\")\n",
    "    if not os.path.exists(mapping_path):\n",
    "        continue\n",
    "\n",
    "    with open(mapping_path, 'r') as f:\n",
    "        mapping = json.load(f)\n",
    "\n",
    "    h_gene = output[\"encoder\"][\"h_gene\"]\n",
    "    h_pathway = output[\"encoder\"][\"h_pathway\"]\n",
    "\n",
    "    for gene, idx in mapping[\"gene_to_index\"].items():\n",
    "        embedding_records.append({\n",
    "            \"type\": \"gene\",\n",
    "            \"name\": gene,\n",
    "            \"dosage\": dosage_name,\n",
    "            \"embedding\": h_gene[idx].tolist()\n",
    "        })\n",
    "\n",
    "    for pathway, idx in mapping[\"pathway_to_index\"].items():\n",
    "        embedding_records.append({\n",
    "            \"type\": \"pathway\",\n",
    "            \"name\": pathway,\n",
    "            \"dosage\": dosage_name,\n",
    "            \"embedding\": h_pathway[idx].tolist()\n",
    "        })\n",
    "\n",
    "# Prepare data\n",
    "df_embed = pd.DataFrame(embedding_records)\n",
    "embedding_matrix = np.vstack(df_embed[\"embedding\"].values)\n",
    "\n",
    "# Run t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, init='pca')\n",
    "tsne_result = tsne.fit_transform(embedding_matrix)\n",
    "\n",
    "df_embed[\"tsne-1\"] = tsne_result[:, 0]\n",
    "df_embed[\"tsne-2\"] = tsne_result[:, 1]\n",
    "\n",
    "# Save embeddings\n",
    "df_embed.to_csv(\"tSNE_Embeddings_NoLSTM.csv\", index=False)\n",
    "\n",
    "# Set global font size (optional)\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Plot gene embeddings\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=df_embed[df_embed[\"type\"] == \"gene\"],\n",
    "    x=\"tsne-1\", y=\"tsne-2\", hue=\"dosage\", palette=\"tab10\"\n",
    ")\n",
    "plt.title(\"t-SNE: Gene Embeddings by Dosage No LSTM\", fontsize=16)\n",
    "plt.xlabel(\"tSNE-1\", fontsize=14)\n",
    "plt.ylabel(\"tSNE-2\", fontsize=14)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Dosage\", fontsize=10, title_fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tSNE_Gene_Embeddings_original_NoLSTM.png\", dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Plot pathway embeddings\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=df_embed[df_embed[\"type\"] == \"pathway\"],\n",
    "    x=\"tsne-1\", y=\"tsne-2\", hue=\"dosage\", palette=\"tab10\"\n",
    ")\n",
    "plt.title(\"t-SNE: Pathway Embeddings by Dosag No LSTM\", fontsize=16)\n",
    "plt.xlabel(\"tSNE-1\", fontsize=14)\n",
    "plt.ylabel(\"tSNE-2\", fontsize=14)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Dosage\", fontsize=10, title_fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tSNE_Pathway_Embeddings_original.png\", dpi=600, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36266fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the t-SNE embedding file\n",
    "df_embed = pd.read_csv(\"tSNE_Embeddings_NoLSTM.csv\")\n",
    "\n",
    "# Step 1: Group embeddings\n",
    "grouped_embeddings = defaultdict(lambda: defaultdict(list))\n",
    "for _, row in df_embed.iterrows():\n",
    "    entity_type = row[\"type\"]\n",
    "    name = row[\"name\"]\n",
    "    dosage = row[\"dosage\"]\n",
    "    emb = np.array(row[\"embedding\"].strip('[]').split(','), dtype=float) if isinstance(row[\"embedding\"], str) else row[\"embedding\"]\n",
    "    grouped_embeddings[entity_type][name].append((dosage, emb))\n",
    "\n",
    "# Step 2: Define dosage order (make sure to sort properly based on true dosage value)\n",
    "dosage_order = ['T1', 'T2.5', 'T5', 'T10', 'T20', 'T40', 'T80', 'T160', 'T320']\n",
    "dosage_to_index = {d: i for i, d in enumerate(dosage_order)}\n",
    "\n",
    "def compute_cosine_drift(embedding_dict):\n",
    "    drift_dict = {}\n",
    "    for name, entries in embedding_dict.items():\n",
    "        # Filter valid entries by dosage order\n",
    "        entries_sorted = sorted(entries, key=lambda x: dosage_to_index.get(x[0], -1))\n",
    "        drifts = []\n",
    "        for (d1, v1), (d2, v2) in zip(entries_sorted[:-1], entries_sorted[1:]):\n",
    "            sim = cosine_similarity([v1], [v2])[0, 0]\n",
    "            drift = 1 - sim\n",
    "            drifts.append(drift)\n",
    "        if drifts:\n",
    "            drift_dict[name] = {\n",
    "                \"drifts\": drifts,\n",
    "                \"mean_drift\": np.mean(drifts),\n",
    "                \"num_pairs\": len(drifts)\n",
    "            }\n",
    "    return drift_dict\n",
    "\n",
    "# Step 3: Compute drifts\n",
    "gene_drift = compute_cosine_drift(grouped_embeddings['gene'])\n",
    "pathway_drift = compute_cosine_drift(grouped_embeddings['pathway'])\n",
    "\n",
    "# Step 4: Compute overall mean drift\n",
    "mean_gene_drift = np.mean([v[\"mean_drift\"] for v in gene_drift.values()])\n",
    "mean_pathway_drift = np.mean([v[\"mean_drift\"] for v in pathway_drift.values()])\n",
    "\n",
    "print(f\"ğŸ“Š Average Gene Drift Across Dosages: {mean_gene_drift:.4f}\")\n",
    "print(f\"ğŸ“Š Average Pathway Drift Across Dosages: {mean_pathway_drift:.4f}\")\n",
    "\n",
    "# Step 5: Save detailed drift information\n",
    "pd.DataFrame([\n",
    "    {\"Gene\": k, \"MeanDrift\": v[\"mean_drift\"], \"Pairs\": v[\"num_pairs\"]}\n",
    "    for k, v in gene_drift.items()\n",
    "]).to_csv(\"Gene_Cosine_Drift.csv\", index=False)\n",
    "\n",
    "pd.DataFrame([\n",
    "    {\"Pathway\": k, \"MeanDrift\": v[\"mean_drift\"], \"Pairs\": v[\"num_pairs\"]}\n",
    "    for k, v in pathway_drift.items()\n",
    "]).to_csv(\"Pathway_Cosine_Drift.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf840ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if in a module, or skip if already imported\n",
    "\n",
    "# Initialize No-LSTM encoder with correct name\n",
    "encoder = SharedHierarchicalEncoder_NoLSTM(hidden_dim=64, num_dosages=9)\n",
    "\n",
    "# Load checkpoint trained without LSTM\n",
    "checkpoint = torch.load(\"trained_model_no_lstm.pth\", map_location='cpu')\n",
    "\n",
    "# Load encoder weights only\n",
    "encoder.load_state_dict(checkpoint['encoder'])\n",
    "\n",
    "# Set to evaluation mode\n",
    "encoder.eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "pyg_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
